{
  "id": "abigailcopiaco/daseedataset",
  "id_no": 1263382,
  "datasetSlugNullable": "daseedataset",
  "ownerUserNullable": "abigailcopiaco",
  "usabilityRatingNullable": 0.625,
  "titleNullable": "DASEE-dataset",
  "subtitleNullable": "Domestic Acoustic Sounds and Events in the Environment of dementia patients",
  "descriptionNullable": "## **Context**\n\nAccess to informative databases is a crucial part of notable research developments. In the field of domestic audio classification there have been significant advances in recent years. Although several audio databases exist, these can be limited in terms of the amount of information they provide, such as the exact location of the sound sources, and the associated noise levels. In this work, we detail our approach on generating an unbiased synthetic domestic audio database, consisting of sound scenes and events, emulated in both quiet and noisy environments. Data is carefully curated such that it reflects issues commonly faced in a dementia patient\u2019s environment, and recreate scenarios that could occur in real-world settings. Similarly, the room impulse response generated is based on a typical one-bedroom apartment at Hebrew SeniorLife Facility. As a result, we present an 11-class database containing excerpts of clean and noisy signals at 5-seconds duration each, uniformly sampled at 16 kHz. Using our baseline model using Continues Wavelet Transform Scalograms and AlexNet, this yielded a weighted F1-score of 86.24%.\n\n## **Content**\n\nThe developed synthetic domestic acoustic scene and events database contains 11 classes, summarized as follows:\n\n| Category | Training Data  | Testing Data |\n| --- | --- | --- |\n| Absence / Silence  | 11286 | 876 |\n| Alarm  | 2765 | 260 |\n| Cat  | 11724 | 1080 |\n| Dog  | 6673 | 792 |\n| Kitchen Activities  | 12291 | 1062 |\n| Scream  | 4308 | 376 |\n| Shatter  | 2877 | 370 |\n| Shaver or toothbrush  | 11231 | 1077 |\n| Slam  | 1565 | 268 |\n| Speech  | 30113 | 2374 |\n| Water  | 6796 | 829 |\n\nFile naming system:\nlocation_soundclass_recordingnumber_node_scale_perfectSNR_backgroundnoise_SNRnoise.wav\nExample:\nbedroom_Alarm_2395_1_white_80_AC_15.wav\nLocation = Bedroom\nSound Class = Alarm\nRecording Number = 2395\nNode = 1\nLoudness is scaled by = White Noise\nPerfect SNR = 80 dB (for loudness scaling)\nBackground Noise = Air Conditioner (AC)\nSNR Noise Level = 15 dB\n\nAll recordings are four-channel, uniformly produced with 5-s duration, and has a sampling frequency of 16 kHz. More information about the experimental setup and data curation can be found in the paper published for this work, cited as follows:\nPaper: \nA. Copiaco, C. Ritz, S. Fasciani, and N. Abdulaziz, \u201cDASEE: A Synthetic Database of Domestic Acoustic Scenes and Events in Dementia Patients\u2019 Environment\u201d, *submitted for review*, 2021.\n\nCodes available in: https://github.com/abigailcopiaco/DASEEdataset.git \n\n## **Acknowledgements**\n\nThe codes provided with the dataset requires the VOICEBOX Toolbox and RIRD Generator code. See details below:\n\nCopyright (C) Mike Brookes 2014\nVersion: $Id: v_addnoise.m 10461 2018-03-29 13:30:51Z dmb $\nVOICEBOX is a MATLAB toolbox for speech processing.\nHome page: http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html\n\nThis code also uses the 'rird_generator.mat' function from the following source:\nhttp://www.commsp.ee.ic.ac.uk/~ssh12/RIRD.htm\nCitation: Room Impulse Response for Directional source generator (RIRDgen) by Sina Hafezi, Alastair H. Moore, and Patrick A. Naylor, 2015\n\nGratitude is also given to the sources of the dry samples used for the development of this dataset:\n1. DESED Synthetic Soundscapes [1]\n2. Kaggle: Audio Cats and Dogs [2]\n3. Open SLR: 64 and 70 [3]\n4. FSDKaggle2019 [4]\n5. FSD50K [5]\n6. SINS Database [6]\n7. Urban Sound 8K [7]\n\nReferences:\n[1]\tTurpault, N.; Serizel, R.; Shah, A.P., and Salamon, J. Sound event detection in domestic environments with weakly labeled data and soundscape synthesis. In Proc. DCASE Workshop, Oct 2019\n[2]\tTakahashi, N.; Gygli, M.; Pfister, B.; and Van Gool, L. Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition, Proc. Interspeech, San Fransisco, 2016.\n[3]\tHe, F. et al. Open-source Multi-speaker Speech Corpora for Building Gujarati, Kannada, Malayalam, Marathi, Tamil and Telugu Speech Synthesis Systems, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), Marseille, France, 2020.\n[4]\tFonseca, E.; Plakal, M.; Font, F.; Ellis, D.P.W.; Serra, X. Audio tagging with noisy labels and minimal supervision. Proceedings of the DCASE 2019 Workshop, NYC, US, 2019.\n[5]\tFonseca, E.; Favory, X.; Pons, J.; Font, F.; and Serra, X. \"FSD50K: an Open Dataset of Human-Labeled Sound Events\", arXiv:2010.00475, 2020.\n[6]\tG. Dekkers et al., \"The SINS database for detection of daily activities in a home environment using an acoustic sensor network,\" DCASE, 2017.\n[7]\tSalamon, J.; Jacoby, C.; and Bello, J.P., A Dataset and Taxonomy for Urban Sound Research, 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.\n",
  "datasetId": 1263382,
  "datasetSlug": "daseedataset",
  "hasDatasetSlug": true,
  "ownerUser": "abigailcopiaco",
  "hasOwnerUser": true,
  "usabilityRating": 0.625,
  "hasUsabilityRating": true,
  "totalViews": 7529,
  "totalVotes": 10,
  "totalDownloads": 528,
  "title": "DASEE-dataset",
  "hasTitle": true,
  "subtitle": "Domestic Acoustic Sounds and Events in the Environment of dementia patients",
  "hasSubtitle": true,
  "description": "## **Context**\n\nAccess to informative databases is a crucial part of notable research developments. In the field of domestic audio classification there have been significant advances in recent years. Although several audio databases exist, these can be limited in terms of the amount of information they provide, such as the exact location of the sound sources, and the associated noise levels. In this work, we detail our approach on generating an unbiased synthetic domestic audio database, consisting of sound scenes and events, emulated in both quiet and noisy environments. Data is carefully curated such that it reflects issues commonly faced in a dementia patient\u2019s environment, and recreate scenarios that could occur in real-world settings. Similarly, the room impulse response generated is based on a typical one-bedroom apartment at Hebrew SeniorLife Facility. As a result, we present an 11-class database containing excerpts of clean and noisy signals at 5-seconds duration each, uniformly sampled at 16 kHz. Using our baseline model using Continues Wavelet Transform Scalograms and AlexNet, this yielded a weighted F1-score of 86.24%.\n\n## **Content**\n\nThe developed synthetic domestic acoustic scene and events database contains 11 classes, summarized as follows:\n\n| Category | Training Data  | Testing Data |\n| --- | --- | --- |\n| Absence / Silence  | 11286 | 876 |\n| Alarm  | 2765 | 260 |\n| Cat  | 11724 | 1080 |\n| Dog  | 6673 | 792 |\n| Kitchen Activities  | 12291 | 1062 |\n| Scream  | 4308 | 376 |\n| Shatter  | 2877 | 370 |\n| Shaver or toothbrush  | 11231 | 1077 |\n| Slam  | 1565 | 268 |\n| Speech  | 30113 | 2374 |\n| Water  | 6796 | 829 |\n\nFile naming system:\nlocation_soundclass_recordingnumber_node_scale_perfectSNR_backgroundnoise_SNRnoise.wav\nExample:\nbedroom_Alarm_2395_1_white_80_AC_15.wav\nLocation = Bedroom\nSound Class = Alarm\nRecording Number = 2395\nNode = 1\nLoudness is scaled by = White Noise\nPerfect SNR = 80 dB (for loudness scaling)\nBackground Noise = Air Conditioner (AC)\nSNR Noise Level = 15 dB\n\nAll recordings are four-channel, uniformly produced with 5-s duration, and has a sampling frequency of 16 kHz. More information about the experimental setup and data curation can be found in the paper published for this work, cited as follows:\nPaper: \nA. Copiaco, C. Ritz, S. Fasciani, and N. Abdulaziz, \u201cDASEE: A Synthetic Database of Domestic Acoustic Scenes and Events in Dementia Patients\u2019 Environment\u201d, *submitted for review*, 2021.\n\nCodes available in: https://github.com/abigailcopiaco/DASEEdataset.git \n\n## **Acknowledgements**\n\nThe codes provided with the dataset requires the VOICEBOX Toolbox and RIRD Generator code. See details below:\n\nCopyright (C) Mike Brookes 2014\nVersion: $Id: v_addnoise.m 10461 2018-03-29 13:30:51Z dmb $\nVOICEBOX is a MATLAB toolbox for speech processing.\nHome page: http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html\n\nThis code also uses the 'rird_generator.mat' function from the following source:\nhttp://www.commsp.ee.ic.ac.uk/~ssh12/RIRD.htm\nCitation: Room Impulse Response for Directional source generator (RIRDgen) by Sina Hafezi, Alastair H. Moore, and Patrick A. Naylor, 2015\n\nGratitude is also given to the sources of the dry samples used for the development of this dataset:\n1. DESED Synthetic Soundscapes [1]\n2. Kaggle: Audio Cats and Dogs [2]\n3. Open SLR: 64 and 70 [3]\n4. FSDKaggle2019 [4]\n5. FSD50K [5]\n6. SINS Database [6]\n7. Urban Sound 8K [7]\n\nReferences:\n[1]\tTurpault, N.; Serizel, R.; Shah, A.P., and Salamon, J. Sound event detection in domestic environments with weakly labeled data and soundscape synthesis. In Proc. DCASE Workshop, Oct 2019\n[2]\tTakahashi, N.; Gygli, M.; Pfister, B.; and Van Gool, L. Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition, Proc. Interspeech, San Fransisco, 2016.\n[3]\tHe, F. et al. Open-source Multi-speaker Speech Corpora for Building Gujarati, Kannada, Malayalam, Marathi, Tamil and Telugu Speech Synthesis Systems, Proceedings of The 12th Language Resources and Evaluation Conference (LREC), Marseille, France, 2020.\n[4]\tFonseca, E.; Plakal, M.; Font, F.; Ellis, D.P.W.; Serra, X. Audio tagging with noisy labels and minimal supervision. Proceedings of the DCASE 2019 Workshop, NYC, US, 2019.\n[5]\tFonseca, E.; Favory, X.; Pons, J.; Font, F.; and Serra, X. \"FSD50K: an Open Dataset of Human-Labeled Sound Events\", arXiv:2010.00475, 2020.\n[6]\tG. Dekkers et al., \"The SINS database for detection of daily activities in a home environment using an acoustic sensor network,\" DCASE, 2017.\n[7]\tSalamon, J.; Jacoby, C.; and Bello, J.P., A Dataset and Taxonomy for Urban Sound Research, 22nd ACM International Conference on Multimedia, Orlando USA, Nov. 2014.\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "health",
    "audio"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-SA-4.0",
      "name": "CC-BY-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}