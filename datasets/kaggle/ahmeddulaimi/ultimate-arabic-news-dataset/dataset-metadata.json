{
  "id": "ahmeddulaimi/ultimate-arabic-news-dataset",
  "id_no": 3514107,
  "datasetSlugNullable": "ultimate-arabic-news-dataset",
  "ownerUserNullable": "ahmeddulaimi",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Ultimate Arabic News Dataset",
  "subtitleNullable": " \u0623\u0643\u0628\u0631 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0639\u0631\u0628\u064a\u0629 \u0625\u062e\u0628\u0627\u0631\u064a\u0629 \u0639\u0644\u0649 \u0627\u0644\u0627\u0637\u0644\u0627\u0642 | The Largest Arabic News Dataset",
  "descriptionNullable": "The Ultimate Arabic News Dataset is a collection of single-label modern Arabic texts that are used in news websites and press articles.\n\nArabic news data was collected by web scraping techniques from many famous news sites such as Al-Arabiya, Al-Youm Al-Sabea (Youm7), Al-Jazeera, the news published on the Google search engine and other various sources.\n\n- The data we collect consists of two Primary files:\n\n**UltimateArabic**: A file containing more than 193,000 original Arabic news texts, without pre-processing. The texts contain words, numbers, and symbols that can be removed using pre-processing to increase accuracy when using the dataset in various Arabic natural language processing tasks such as text classification.\n\n**UltimateArabicPrePros**: It is a file that contains the data mentioned in the first file, but after pre-processing, where the number of data became about 188,000 text documents, where stop words, non-Arabic words, symbols and numbers have been removed so that this file is ready for use directly in the various Arabic natural language processing tasks. Like text classification.\n\n- We have added four different versions of the original data set, from which the appropriate version can be selected for use in text classification techniques.\n\nThe first data set (**Original**) contains the raw data without pre-processing the data in any way, so the number of tokens in the first data set is very high. In the second data set (**Original_without_Stop**) the data was cleaned, such as removing symbols, numbers, and non-Arabic words, as well as stop words, so the number of symbols is greatly reduced. In the third dataset (**Original_with_Stem**) the data was cleaned, and text stemming technique was used to remove all additions and suffixes that might affect the accuracy of the results and to obtain the words roots. In the 4th edition of the dataset (**Original_Without_Stop_Stem**) all preprocessing techniques such as data cleaning, stop word removal and text stemming technique were applied, so we note that the number of tokens in the 4th edition is the lowest among all releases.\n\n- The data is divided into 10 different categories:\nCulture, Diverse, Economy, Sport, Politic, Art, Society, Technology, Medical and Religion.",
  "datasetId": 3514107,
  "datasetSlug": "ultimate-arabic-news-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "ahmeddulaimi",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 400,
  "totalVotes": 0,
  "totalDownloads": 75,
  "title": "Ultimate Arabic News Dataset",
  "hasTitle": true,
  "subtitle": " \u0623\u0643\u0628\u0631 \u0645\u062c\u0645\u0648\u0639\u0629 \u0628\u064a\u0627\u0646\u0627\u062a \u0639\u0631\u0628\u064a\u0629 \u0625\u062e\u0628\u0627\u0631\u064a\u0629 \u0639\u0644\u0649 \u0627\u0644\u0627\u0637\u0644\u0627\u0642 | The Largest Arabic News Dataset",
  "hasSubtitle": true,
  "description": "The Ultimate Arabic News Dataset is a collection of single-label modern Arabic texts that are used in news websites and press articles.\n\nArabic news data was collected by web scraping techniques from many famous news sites such as Al-Arabiya, Al-Youm Al-Sabea (Youm7), Al-Jazeera, the news published on the Google search engine and other various sources.\n\n- The data we collect consists of two Primary files:\n\n**UltimateArabic**: A file containing more than 193,000 original Arabic news texts, without pre-processing. The texts contain words, numbers, and symbols that can be removed using pre-processing to increase accuracy when using the dataset in various Arabic natural language processing tasks such as text classification.\n\n**UltimateArabicPrePros**: It is a file that contains the data mentioned in the first file, but after pre-processing, where the number of data became about 188,000 text documents, where stop words, non-Arabic words, symbols and numbers have been removed so that this file is ready for use directly in the various Arabic natural language processing tasks. Like text classification.\n\n- We have added four different versions of the original data set, from which the appropriate version can be selected for use in text classification techniques.\n\nThe first data set (**Original**) contains the raw data without pre-processing the data in any way, so the number of tokens in the first data set is very high. In the second data set (**Original_without_Stop**) the data was cleaned, such as removing symbols, numbers, and non-Arabic words, as well as stop words, so the number of symbols is greatly reduced. In the third dataset (**Original_with_Stem**) the data was cleaned, and text stemming technique was used to remove all additions and suffixes that might affect the accuracy of the results and to obtain the words roots. In the 4th edition of the dataset (**Original_Without_Stop_Stem**) all preprocessing techniques such as data cleaning, stop word removal and text stemming technique were applied, so we note that the number of tokens in the 4th edition is the lowest among all releases.\n\n- The data is divided into 10 different categories:\nCulture, Diverse, Economy, Sport, Politic, Art, Society, Technology, Medical and Religion.",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "news",
    "multilabel classification",
    "text classification",
    "text pre-processing",
    "arabic"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-SA-4.0",
      "name": "CC-BY-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}