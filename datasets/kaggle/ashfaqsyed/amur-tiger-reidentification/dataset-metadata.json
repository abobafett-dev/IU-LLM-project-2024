{
  "id": "ashfaqsyed/amur-tiger-reidentification",
  "id_no": 3085332,
  "datasetSlugNullable": "amur-tiger-reidentification",
  "ownerUserNullable": "ashfaqsyed",
  "usabilityRatingNullable": 0.7647058823529411,
  "titleNullable": "Amur Tiger ReIdentification",
  "subtitleNullable": "This dataset contains more than 8,000 video clips of 92 individual Amur tigers ",
  "descriptionNullable": "This dataset contains more than 8,000 video clips of 92 individual Amur tigers from 10 zoos in China. Around 9500 bounding boxes are provided along with pose keypoints, and around 3600 of those bounding boxes are associated with an individual tiger ID. This data set was originally published as part of the Re-identification challenge at the ICCV 2019 Workshop on Computer Vision for Wildlife Conservation; suggested train/val/test splits correspond to those used for the competition.\n\nData format\nAll annotation tar files include README.md files with detailed format information; this section provides a high-level summary only.\n\nDetection\nBounding boxes are provided in Pascal VOC format.\n\nPose\nPose annotations are provided in COCO format. Annotations use the COCO \u201ckeypoint\u201d annotation type, with categories like \u201cleft_ear\u201d, \u201cright_ear\u201d, \u201cnose\u201d, etc.\n\nRe-identification\nIdentifications in the \u201ctrain\u201d set are provided as a .csv-formatted list of [ID,filename] pairs; the \u201ctest\u201d set contains only a list of images requiring identification. Pose annotations are provided for both sets.\n\nThe competition for which this dataset was prepared divided re-identification into two tasks, one (\u201cplain re-ID\u201d) where pose and bounding box annotations were available, and one (\u201cwild re-ID\u201d) where annotations were not available.\n\nTracks\nTiger Detection: From images/videos captured by cameras, this task aims to place tight bounding boxes around tigers. As the detection may run on the edge (smart cameras), both the detection accuracy (in terms of AP) and the computing cost are used to measure the quality of the detector.\n\nTiger Pose Detection: From images/videos with detected tiger bounding boxes, this task aims to estimate tiger pose (i.e., keypoint landmarks) for tiger image alignment/normalization, so that pose variations are removed or alleviated in the tiger re-identification step. We will use mean average precision (mAP) and object keypoint similarity (OKS) to evaluate submissions.\n\nTiger Re-ID with Human Alignment (Plain Re-ID): We define a set of queries and a target database of Amur tigers. Both queries and targets in the database are already annotated with bounding boxes and pose information. Tiger re-identification aims to find all the database images containing the same tiger as the query. Both mAP and rank-1 accuracy will be used to evaluate accuracy.\n\nTiger Re-ID in the Wild: This track will evaluate the accuracy of tiger re-identification in wild with a fully automated pipeline. To simulate the real use case, no annotations are provided. Submissions should automatically detect and identify tigers in all images in the test set. Both mAP and rank-1 accuracy will be used to evaluate the accuracy of different models.\n\nFormat Description\nDetection: Data annotaiton in Pascal VOC format. Submission in COCO detection format. Training with the given training set and testing set will be provided in the test stage.\nPose: Both data annotaiton and submission are in COCO format. Training with the given training set and testing set will be provided in the test stage.\nPlain ReID: Dataset contains cropped images with manual annotaetd ID and keypoints. Submission should be a json file in the following format:\n\n[\n\u2003{\"query_id\":0, \u2003 \"ans_ids\":[29,38,10,.......]},\n\u2003{\"query_id\":3, \u2003 \"ans_ids\":[95,18,20,.......]},\n\u2003...\n]\nwhere the \"query_id\" is the id of query image, and each followed array \"ans_ids\" lists re-ID results (image ids) in the confidence descending order.\nSimilar to most existing Re-ID tasks, the plain Re-ID task requires to build models on training-set, and evaluating on the test-set. During testing, each image will be taken as query image, while all the remained images in the test-set as \"gallery\" or \"database\", the query results should be rank-list of images in \"gallery\". The evaluation server will separate the test-set into two cases: single-camera and cross camera (see our arxiv report for more details) to measure performance. The evaluation metrics are mAP and top-k (k=1, 5).\n\nReID in Wild: This task aims to evaluate the performance of Re-ID in a full automatical way. Paritipants require to build tiger detector, tiger pose estimator, and re-ID module based on the provide training-set, and integrate them as a full pipeline to re-identification each detected tiger in a set of wild input images. The test-set is the same as that of the detection task. The re-ID evaluation will use all the detected boxes as \"gallery\", while the other procedure is smilar to the plain re-ID case. Submission should be a json file with the following schema:\n\n{\n\u2003\"bboxs\":[bbox],\n\u2003\"reid_result\":[\n\u2003\u2003{\"query_id\":int,\u2003\"ans_ids\":[int,int,...]}\n\u2003]\n}\nwhere\nbbox{\n\u2003\"bbox_id\": int, #used in reid_result\n\u2003\"image_id\": int,\n\u2003\"pos\": [x,y,w,h] #x,y is the top-left coord, all in pixels.\n}\nwhere the 'reid_result' is almost the same format as in Plain ReID, with only 'id' replaced by 'bbox_id'. To make the evaluation tackable, partcipants require to make the 'bbox_id' unique in their submission file not only in each test image, but also among all images in the test-set.\n\nCitation\nIf you use this dataset, please cite the associated arXiv publication:\nLi, S., Li, J., Lin, W., & Tang, H. (2019). Amur Tiger Re-identification in the Wild. arXiv preprint arXiv:1906.05586.\n\n",
  "datasetId": 3085332,
  "datasetSlug": "amur-tiger-reidentification",
  "hasDatasetSlug": true,
  "ownerUser": "ashfaqsyed",
  "hasOwnerUser": true,
  "usabilityRating": 0.7647058823529411,
  "hasUsabilityRating": true,
  "totalViews": 1028,
  "totalVotes": 5,
  "totalDownloads": 59,
  "title": "Amur Tiger ReIdentification",
  "hasTitle": true,
  "subtitle": "This dataset contains more than 8,000 video clips of 92 individual Amur tigers ",
  "hasSubtitle": true,
  "description": "This dataset contains more than 8,000 video clips of 92 individual Amur tigers from 10 zoos in China. Around 9500 bounding boxes are provided along with pose keypoints, and around 3600 of those bounding boxes are associated with an individual tiger ID. This data set was originally published as part of the Re-identification challenge at the ICCV 2019 Workshop on Computer Vision for Wildlife Conservation; suggested train/val/test splits correspond to those used for the competition.\n\nData format\nAll annotation tar files include README.md files with detailed format information; this section provides a high-level summary only.\n\nDetection\nBounding boxes are provided in Pascal VOC format.\n\nPose\nPose annotations are provided in COCO format. Annotations use the COCO \u201ckeypoint\u201d annotation type, with categories like \u201cleft_ear\u201d, \u201cright_ear\u201d, \u201cnose\u201d, etc.\n\nRe-identification\nIdentifications in the \u201ctrain\u201d set are provided as a .csv-formatted list of [ID,filename] pairs; the \u201ctest\u201d set contains only a list of images requiring identification. Pose annotations are provided for both sets.\n\nThe competition for which this dataset was prepared divided re-identification into two tasks, one (\u201cplain re-ID\u201d) where pose and bounding box annotations were available, and one (\u201cwild re-ID\u201d) where annotations were not available.\n\nTracks\nTiger Detection: From images/videos captured by cameras, this task aims to place tight bounding boxes around tigers. As the detection may run on the edge (smart cameras), both the detection accuracy (in terms of AP) and the computing cost are used to measure the quality of the detector.\n\nTiger Pose Detection: From images/videos with detected tiger bounding boxes, this task aims to estimate tiger pose (i.e., keypoint landmarks) for tiger image alignment/normalization, so that pose variations are removed or alleviated in the tiger re-identification step. We will use mean average precision (mAP) and object keypoint similarity (OKS) to evaluate submissions.\n\nTiger Re-ID with Human Alignment (Plain Re-ID): We define a set of queries and a target database of Amur tigers. Both queries and targets in the database are already annotated with bounding boxes and pose information. Tiger re-identification aims to find all the database images containing the same tiger as the query. Both mAP and rank-1 accuracy will be used to evaluate accuracy.\n\nTiger Re-ID in the Wild: This track will evaluate the accuracy of tiger re-identification in wild with a fully automated pipeline. To simulate the real use case, no annotations are provided. Submissions should automatically detect and identify tigers in all images in the test set. Both mAP and rank-1 accuracy will be used to evaluate the accuracy of different models.\n\nFormat Description\nDetection: Data annotaiton in Pascal VOC format. Submission in COCO detection format. Training with the given training set and testing set will be provided in the test stage.\nPose: Both data annotaiton and submission are in COCO format. Training with the given training set and testing set will be provided in the test stage.\nPlain ReID: Dataset contains cropped images with manual annotaetd ID and keypoints. Submission should be a json file in the following format:\n\n[\n\u2003{\"query_id\":0, \u2003 \"ans_ids\":[29,38,10,.......]},\n\u2003{\"query_id\":3, \u2003 \"ans_ids\":[95,18,20,.......]},\n\u2003...\n]\nwhere the \"query_id\" is the id of query image, and each followed array \"ans_ids\" lists re-ID results (image ids) in the confidence descending order.\nSimilar to most existing Re-ID tasks, the plain Re-ID task requires to build models on training-set, and evaluating on the test-set. During testing, each image will be taken as query image, while all the remained images in the test-set as \"gallery\" or \"database\", the query results should be rank-list of images in \"gallery\". The evaluation server will separate the test-set into two cases: single-camera and cross camera (see our arxiv report for more details) to measure performance. The evaluation metrics are mAP and top-k (k=1, 5).\n\nReID in Wild: This task aims to evaluate the performance of Re-ID in a full automatical way. Paritipants require to build tiger detector, tiger pose estimator, and re-ID module based on the provide training-set, and integrate them as a full pipeline to re-identification each detected tiger in a set of wild input images. The test-set is the same as that of the detection task. The re-ID evaluation will use all the detected boxes as \"gallery\", while the other procedure is smilar to the plain re-ID case. Submission should be a json file with the following schema:\n\n{\n\u2003\"bboxs\":[bbox],\n\u2003\"reid_result\":[\n\u2003\u2003{\"query_id\":int,\u2003\"ans_ids\":[int,int,...]}\n\u2003]\n}\nwhere\nbbox{\n\u2003\"bbox_id\": int, #used in reid_result\n\u2003\"image_id\": int,\n\u2003\"pos\": [x,y,w,h] #x,y is the top-left coord, all in pixels.\n}\nwhere the 'reid_result' is almost the same format as in Plain ReID, with only 'id' replaced by 'bbox_id'. To make the evaluation tackable, partcipants require to make the 'bbox_id' unique in their submission file not only in each test image, but also among all images in the test-set.\n\nCitation\nIf you use this dataset, please cite the associated arXiv publication:\nLi, S., Li, J., Lin, W., & Tang, H. (2019). Amur Tiger Re-identification in the Wild. arXiv preprint arXiv:1906.05586.\n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "africa",
    "animals",
    "computer vision",
    "classification",
    "image"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-NC-SA-4.0",
      "name": "CC-BY-NC-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}