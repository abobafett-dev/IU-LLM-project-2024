{
  "id": "birdy654/scene-classification-images-and-audio",
  "id_no": 497259,
  "datasetSlugNullable": "scene-classification-images-and-audio",
  "ownerUserNullable": "birdy654",
  "usabilityRatingNullable": 0.7058823529411765,
  "titleNullable": "Scene Classification: Images and Audio",
  "subtitleNullable": "Visuals and Sound extracted from Video",
  "descriptionNullable": "## Do images and audio complement one another in scene classification?\n\nThese dataset is made up of images from 8 different environments. 37 video sources have been processed, every 1 second an image is extracted (frame at 0.5s, 1.5s, 2.5s ... and so on) and to accompany that image, the MFCC audio statistics are also extracted from the relevant second of video. \n\nIn this dataset, you will notice some common errors from single classifiers. For example, in the video of London, the image classifier confuses the environment with \"FOREST\" when a lady walks past with flowing hair. Likewise, the audio classifier gets confused by \"RIVER\" when we walk past a large fountain in Las Vegas due to the sounds of flowing water. Both of these errors can be fixed by a multi-modal approach, where fusion allows for the correction of errors. In our study, both of these issues were classified as \"CITY\" since multimodality can provide a solution for single-modal errors due to anomalous data occurring.\n\n\n###Please cite this study if you use the dataset\n**Look and Listen: A Multi-Modal Late Fusion Approach to Scene Classification for Autonomous Machines**\n**Jordan J. Bird, Diego R. Faria, Cristiano Premebida, Aniko Ekart, and George Vogiatzis**\n\n## Context\nIn this challenge, we can learn environments (\"Where am I?\") from either images, audio, or take a multimodal approach to fuse the data.\n\nMulti-modal fusion often requires far fewer computing resources than temporal models, but sometimes at the cost of classification ability. Can a method of fusion overcome this? Let's find out!\n\n## Content\nClass data are given as strings in dataset.csv\n\nEach row of the dataset contains a path to the image, as well as the MFCC data extracted from the second of video that accompany the frame.\n\n##MFCC Extraction\n(copied and pasted from the paper)\nwe extract the the Mel-Frequency Cepstral Coefficients (MFCC) of the audio clips through a set of sliding windows 0.25s in length (ie frame size of 4K sampling points) and an additional set of overlapping windows, thus producing 8 sliding windows, 8 frames/sec. From each audio-frame, we extract 13 MFCC attributes, producing 104 attributes per 1 second clip.\n\nThese are numbered in sequence from MFCC_1\n\n##Two Classes?\nThe original study deals with Class 2 (the actual environment, 8 classes) but we have included Class 1 also. Class 1 is a much easier binary classification problem of \"Outdoors\" and \"Indoors\"",
  "datasetId": 497259,
  "datasetSlug": "scene-classification-images-and-audio",
  "hasDatasetSlug": true,
  "ownerUser": "birdy654",
  "hasOwnerUser": true,
  "usabilityRating": 0.7058823529411765,
  "hasUsabilityRating": true,
  "totalViews": 8979,
  "totalVotes": 19,
  "totalDownloads": 477,
  "title": "Scene Classification: Images and Audio",
  "hasTitle": true,
  "subtitle": "Visuals and Sound extracted from Video",
  "hasSubtitle": true,
  "description": "## Do images and audio complement one another in scene classification?\n\nThese dataset is made up of images from 8 different environments. 37 video sources have been processed, every 1 second an image is extracted (frame at 0.5s, 1.5s, 2.5s ... and so on) and to accompany that image, the MFCC audio statistics are also extracted from the relevant second of video. \n\nIn this dataset, you will notice some common errors from single classifiers. For example, in the video of London, the image classifier confuses the environment with \"FOREST\" when a lady walks past with flowing hair. Likewise, the audio classifier gets confused by \"RIVER\" when we walk past a large fountain in Las Vegas due to the sounds of flowing water. Both of these errors can be fixed by a multi-modal approach, where fusion allows for the correction of errors. In our study, both of these issues were classified as \"CITY\" since multimodality can provide a solution for single-modal errors due to anomalous data occurring.\n\n\n###Please cite this study if you use the dataset\n**Look and Listen: A Multi-Modal Late Fusion Approach to Scene Classification for Autonomous Machines**\n**Jordan J. Bird, Diego R. Faria, Cristiano Premebida, Aniko Ekart, and George Vogiatzis**\n\n## Context\nIn this challenge, we can learn environments (\"Where am I?\") from either images, audio, or take a multimodal approach to fuse the data.\n\nMulti-modal fusion often requires far fewer computing resources than temporal models, but sometimes at the cost of classification ability. Can a method of fusion overcome this? Let's find out!\n\n## Content\nClass data are given as strings in dataset.csv\n\nEach row of the dataset contains a path to the image, as well as the MFCC data extracted from the second of video that accompany the frame.\n\n##MFCC Extraction\n(copied and pasted from the paper)\nwe extract the the Mel-Frequency Cepstral Coefficients (MFCC) of the audio clips through a set of sliding windows 0.25s in length (ie frame size of 4K sampling points) and an additional set of overlapping windows, thus producing 8 sliding windows, 8 frames/sec. From each audio-frame, we extract 13 MFCC attributes, producing 104 attributes per 1 second clip.\n\nThese are numbered in sequence from MFCC_1\n\n##Two Classes?\nThe original study deals with Class 2 (the actual environment, 8 classes) but we have included Class 1 also. Class 1 is a much easier binary classification problem of \"Outdoors\" and \"Indoors\"",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "arts and entertainment",
    "music",
    "environment",
    "education",
    "image",
    "video",
    "multiclass classification",
    "audio"
  ],
  "licenses": [
    {
      "nameNullable": "copyright-authors",
      "name": "copyright-authors",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}