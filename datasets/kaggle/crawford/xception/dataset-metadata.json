{
  "id": "crawford/xception",
  "id_no": 6300,
  "datasetSlugNullable": "xception",
  "ownerUserNullable": "crawford",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "Xception",
  "subtitleNullable": "Xception Pre-trained Model for Keras",
  "descriptionNullable": "# Xception\n\n---\n\n## Xception: Deep Learning with Depthwise Separable Convolutions<br>\nWe present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.\n\n**Author: Fran\u00e7ois Chollet**<br>\n**https://arxiv.org/abs/1610.02357**\n\n---\n\n\n![Xception architecture][1]\n\n\n---\n\n### What is a Pre-trained Model?\nA pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset. \n\n### Why use a Pre-trained Model?\nPre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. \n\n\n  [1]: https://imgur.com/tA2qDIQ.jpg",
  "datasetId": 6300,
  "datasetSlug": "xception",
  "hasDatasetSlug": true,
  "ownerUser": "crawford",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 19638,
  "totalVotes": 32,
  "totalDownloads": 1496,
  "title": "Xception",
  "hasTitle": true,
  "subtitle": "Xception Pre-trained Model for Keras",
  "hasSubtitle": true,
  "description": "# Xception\n\n---\n\n## Xception: Deep Learning with Depthwise Separable Convolutions<br>\nWe present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.\n\n**Author: Fran\u00e7ois Chollet**<br>\n**https://arxiv.org/abs/1610.02357**\n\n---\n\n\n![Xception architecture][1]\n\n\n---\n\n### What is a Pre-trained Model?\nA pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset. \n\n### Why use a Pre-trained Model?\nPre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. \n\n\n  [1]: https://imgur.com/tA2qDIQ.jpg",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "computer science"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}