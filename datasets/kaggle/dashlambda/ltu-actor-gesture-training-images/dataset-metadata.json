{
  "id": "dashlambda/ltu-actor-gesture-training-images",
  "id_no": 1859253,
  "datasetSlugNullable": "ltu-actor-gesture-training-images",
  "ownerUserNullable": "dashlambda",
  "usabilityRatingNullable": 0.5,
  "titleNullable": "LTU ACTor Gesture Training Images",
  "subtitleNullable": "Dataset for Gesture Image Classification",
  "descriptionNullable": "### Context\n\nDataset of gesture images consisting of frames containing author J. Schulte for use in the human-vehicle Leader-Follower Autonomy (LFA) project. In this project, we have enabled a human leader to activate and deactivate the following function of an autonomous vehicle (Lawrence Technological University's ACTor1) using gesture recognition. We demonstrate a reliable and practical application of gesture recognition in a real-world scenario. For more information, please see our paper here: [https://www.mdpi.com/2624-8921/4/1/16](https://www.mdpi.com/2624-8921/4/1/16). Also please see our GitHub repository here: [https://github.com/jschulte-ltu/ACTor\\_Person\\_Following](https://github.com/jschulte-ltu/ACTor_Person_Following).\n\nCropped pictures are generated by using [YOLO](https://pjreddie.com/darknet/yolo/) to locate people in a frame and cut out an image of our target size (192x192 pixels) to feed into a [TensorFlow Posenet](https://tfhub.dev/google/movenet/singlepose/lightning/4) to perform pose estimation. Pose data is then fed into a DNN classifier to return the most likely gesture.\n\n### Content\n\nThe images are split into three folders, each with one gesture. \"Start\" is the starting gesture; a user that gives a \"start\" command will be the pose target, and will be closely followed by the vehicle. \"Halt\" is the stopping gesture; this commands the vehicle to stop tracking the current target. \"None\" is a series of poses that are neither \"stop\" nor \"start\"; the vehicle will not change behaviour if not given a specific command.\n\n### Acknowledgements\n\nCo-authors: Chan-Jin Chung, Nicholas Paul, Mitchel Pleune\nOther Acknowledgements: Joseph Redmon (YOLO creator), Google Engineers (Tensorflow)",
  "datasetId": 1859253,
  "datasetSlug": "ltu-actor-gesture-training-images",
  "hasDatasetSlug": true,
  "ownerUser": "dashlambda",
  "hasOwnerUser": true,
  "usabilityRating": 0.5,
  "hasUsabilityRating": true,
  "totalViews": 1723,
  "totalVotes": 2,
  "totalDownloads": 16,
  "title": "LTU ACTor Gesture Training Images",
  "hasTitle": true,
  "subtitle": "Dataset for Gesture Image Classification",
  "hasSubtitle": true,
  "description": "### Context\n\nDataset of gesture images consisting of frames containing author J. Schulte for use in the human-vehicle Leader-Follower Autonomy (LFA) project. In this project, we have enabled a human leader to activate and deactivate the following function of an autonomous vehicle (Lawrence Technological University's ACTor1) using gesture recognition. We demonstrate a reliable and practical application of gesture recognition in a real-world scenario. For more information, please see our paper here: [https://www.mdpi.com/2624-8921/4/1/16](https://www.mdpi.com/2624-8921/4/1/16). Also please see our GitHub repository here: [https://github.com/jschulte-ltu/ACTor\\_Person\\_Following](https://github.com/jschulte-ltu/ACTor_Person_Following).\n\nCropped pictures are generated by using [YOLO](https://pjreddie.com/darknet/yolo/) to locate people in a frame and cut out an image of our target size (192x192 pixels) to feed into a [TensorFlow Posenet](https://tfhub.dev/google/movenet/singlepose/lightning/4) to perform pose estimation. Pose data is then fed into a DNN classifier to return the most likely gesture.\n\n### Content\n\nThe images are split into three folders, each with one gesture. \"Start\" is the starting gesture; a user that gives a \"start\" command will be the pose target, and will be closely followed by the vehicle. \"Halt\" is the stopping gesture; this commands the vehicle to stop tracking the current target. \"None\" is a series of poses that are neither \"stop\" nor \"start\"; the vehicle will not change behaviour if not given a specific command.\n\n### Acknowledgements\n\nCo-authors: Chan-Jin Chung, Nicholas Paul, Mitchel Pleune\nOther Acknowledgements: Joseph Redmon (YOLO creator), Google Engineers (Tensorflow)",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "movies and tv shows",
    "artificial intelligence",
    "computer vision",
    "deep learning",
    "cnn",
    "image"
  ],
  "licenses": [
    {
      "nameNullable": "unknown",
      "name": "unknown",
      "hasName": true
    }
  ],
  "collaborators": [
    {
      "username": "markkocherovsky",
      "role": "writer"
    }
  ],
  "data": []
}