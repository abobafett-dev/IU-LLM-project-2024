{
  "id": "dschettler8845/the-met-dataset",
  "id_no": 2346910,
  "datasetSlugNullable": "the-met-dataset",
  "ownerUserNullable": "dschettler8845",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "The MET Dataset",
  "subtitleNullable": "A large-scale dataset for Instance-Level Recognition (ILR) in the artwork domain",
  "descriptionNullable": "*The Met Dataset was created by: Nikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne van Noord, Giorgos Tolias.*\n\n***README Sourced from [here](http://cmp.felk.cvut.cz/met/)***\n\n<br>\n\n## **What is the Met Dataset?**\n\n* The Met dataset is a large-scale dataset for **I**nstance-**L**evel **R**ecognition (**ILR**) in the artwork domain.\n  * We rely on the open access collection from the **Met**ropolitan Museum of Art (The **Met**) in New York to form the training set, which consists of about **400k images from more than 224k classes**, with artworks of world-level geographic coverage and chronological periods dating back to the Paleolithic period. \n  * Each museum exhibit corresponds to a unique artwork, and defines its own class. \n  * The training set exhibits a long-tail distribution with more than half of the classes represented by a single image, making it a special case of few-shot learning.\n  * We have established **ground-truth for more than 1,100 images from museum visitors, which form the Met queries**. \n\n## **What is The Goal Of the Dataset?**\n* The goal is to recognize the Met exhibits depicted in the Met queries. \n\n## **Other Information?**\n* There is a distribution shift between these queries and the training images which are created in studio-like conditions. \n* We additionally include a large set of images not related to The Met, which form an Out-Of-Distribution (OOD) query set, the distractor queries. \n  * There is no Met exhibit despicted in distractor queries. \n* The query set is composed by the combination of those two sets.\n\n<br>\n\n## **Dataset Information**\n\nThe images of the dataset and the ground-truth files can be downloaded from the links below. *All images have been resized so that their largest side is 500 pixels.*\n* Met exhibit (train) images (28 GB)\n* mini Met exhibit (train) images (3 GB)\n* Met query images (32 MB)\n* other query images (1 GB)\n* no-art query images (1 GB)\n* ground-truth (.json) files (3 MB)\n\n<br>\n\n## **Embedding Models**\n\n* Models for descriptor extraction have been provided in this kaggle dataset. \n* They can be used directly to extract descriptors with the provided code for descriptor extraction.\n* The models are:\n  * **ResNet18-IN-SRC**\n    * Trained on Met with contrastive loss (Syn+Real-Closest). \n    * Initialization: **ImageNet pre-training**\n  * **ResNet18-SWSL-SRC**\n    * Trained on Met with contrastive loss (Syn+Real-Closest)\n    * Initialization: **SWSL**\n\n<br>\n\n## **Descriptors**\n\n* Descriptors extracted using the models have been provided in this kaggle dataset\n* They can be used directly with the provided code for kNN classification.\n  * **ResNet18-IN**\n    * multi-scale descriptors\n  * **ResNet18-IN-SRC**\n    * multi-scale descriptors\n  * **ResNet18-SWSL-SRC**\n    * multi-scale descriptors\n\n<br>\n\n## **Code**\n\nCode is provided on [github](https://github.com/nikosips/met) to offer support for:\n  * using the dataset\n  * performing the evaluation\n  * reproducing experiments in the NeurIPS 2021 paper\n\n<br>\n\n## **Presentation**\n\n* A video of the presentation of the Met Dataset at the [4th Instance-Level Recognition Workshop of ICCV'21](https://ilr-workshop.github.io/ICCVW2021/) can be found [here](https://www.youtube.com/watch?v=w1vmdB-lB78&t=45s&ab_channel=HanGuangxing).\n\n<br>\n\n## **Related Publication**\n\n```\nThe Met Dataset: Instance-level Recognition for Artworks [ pdf | suppl | bib | poster | video ]\nN.A. Ypsilantis, N. Garcia, G. Han, S. Ibrahimi, N. van Noord, G. Tolias\nAccepted at NeurIPS 2021 Track on Datasets and Benchmarks.\n```\n\n<br>\n\n## **Feedback**\n* Any feedback is very welcome.\n  * Please send it to: ypsilnik@fel.cvut.cz",
  "datasetId": 2346910,
  "datasetSlug": "the-met-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "dschettler8845",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 3227,
  "totalVotes": 18,
  "totalDownloads": 205,
  "title": "The MET Dataset",
  "hasTitle": true,
  "subtitle": "A large-scale dataset for Instance-Level Recognition (ILR) in the artwork domain",
  "hasSubtitle": true,
  "description": "*The Met Dataset was created by: Nikolaos-Antonios Ypsilantis, Noa Garcia, Guangxing Han, Sarah Ibrahimi, Nanne van Noord, Giorgos Tolias.*\n\n***README Sourced from [here](http://cmp.felk.cvut.cz/met/)***\n\n<br>\n\n## **What is the Met Dataset?**\n\n* The Met dataset is a large-scale dataset for **I**nstance-**L**evel **R**ecognition (**ILR**) in the artwork domain.\n  * We rely on the open access collection from the **Met**ropolitan Museum of Art (The **Met**) in New York to form the training set, which consists of about **400k images from more than 224k classes**, with artworks of world-level geographic coverage and chronological periods dating back to the Paleolithic period. \n  * Each museum exhibit corresponds to a unique artwork, and defines its own class. \n  * The training set exhibits a long-tail distribution with more than half of the classes represented by a single image, making it a special case of few-shot learning.\n  * We have established **ground-truth for more than 1,100 images from museum visitors, which form the Met queries**. \n\n## **What is The Goal Of the Dataset?**\n* The goal is to recognize the Met exhibits depicted in the Met queries. \n\n## **Other Information?**\n* There is a distribution shift between these queries and the training images which are created in studio-like conditions. \n* We additionally include a large set of images not related to The Met, which form an Out-Of-Distribution (OOD) query set, the distractor queries. \n  * There is no Met exhibit despicted in distractor queries. \n* The query set is composed by the combination of those two sets.\n\n<br>\n\n## **Dataset Information**\n\nThe images of the dataset and the ground-truth files can be downloaded from the links below. *All images have been resized so that their largest side is 500 pixels.*\n* Met exhibit (train) images (28 GB)\n* mini Met exhibit (train) images (3 GB)\n* Met query images (32 MB)\n* other query images (1 GB)\n* no-art query images (1 GB)\n* ground-truth (.json) files (3 MB)\n\n<br>\n\n## **Embedding Models**\n\n* Models for descriptor extraction have been provided in this kaggle dataset. \n* They can be used directly to extract descriptors with the provided code for descriptor extraction.\n* The models are:\n  * **ResNet18-IN-SRC**\n    * Trained on Met with contrastive loss (Syn+Real-Closest). \n    * Initialization: **ImageNet pre-training**\n  * **ResNet18-SWSL-SRC**\n    * Trained on Met with contrastive loss (Syn+Real-Closest)\n    * Initialization: **SWSL**\n\n<br>\n\n## **Descriptors**\n\n* Descriptors extracted using the models have been provided in this kaggle dataset\n* They can be used directly with the provided code for kNN classification.\n  * **ResNet18-IN**\n    * multi-scale descriptors\n  * **ResNet18-IN-SRC**\n    * multi-scale descriptors\n  * **ResNet18-SWSL-SRC**\n    * multi-scale descriptors\n\n<br>\n\n## **Code**\n\nCode is provided on [github](https://github.com/nikosips/met) to offer support for:\n  * using the dataset\n  * performing the evaluation\n  * reproducing experiments in the NeurIPS 2021 paper\n\n<br>\n\n## **Presentation**\n\n* A video of the presentation of the Met Dataset at the [4th Instance-Level Recognition Workshop of ICCV'21](https://ilr-workshop.github.io/ICCVW2021/) can be found [here](https://www.youtube.com/watch?v=w1vmdB-lB78&t=45s&ab_channel=HanGuangxing).\n\n<br>\n\n## **Related Publication**\n\n```\nThe Met Dataset: Instance-level Recognition for Artworks [ pdf | suppl | bib | poster | video ]\nN.A. Ypsilantis, N. Garcia, G. Han, S. Ibrahimi, N. van Noord, G. Tolias\nAccepted at NeurIPS 2021 Track on Datasets and Benchmarks.\n```\n\n<br>\n\n## **Feedback**\n* Any feedback is very welcome.\n  * Please send it to: ypsilnik@fel.cvut.cz",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "arts and entertainment"
  ],
  "licenses": [
    {
      "nameNullable": "copyright-authors",
      "name": "copyright-authors",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}