{
  "id": "elemento/music-albums-popularity-prediction",
  "id_no": 1603567,
  "datasetSlugNullable": "music-albums-popularity-prediction",
  "ownerUserNullable": "elemento",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Music Albums Popularity Prediction",
  "subtitleNullable": "Practice your ML skills on this Musical Dataset!",
  "descriptionNullable": "# Context\n***Spotify*** provides an ***API*** service that allows us to access data from their archive of millions of songs. Their API gives the users the ability to download all the possible information about ***Albums***, ***Episodes***, ***Playlists***, ***Tracks***, ***Users***, etc. These features include attributes such as a song\u2019s tempo, level of acoustics, how danceable a song is, and many more similar ones. \n\n# Attributes\n- **ID:** A unique identifier for every row\n- **Name:** Name of the album \n- **Release Date:** Release date of the album\n- **Artists:** All the artists of the album\n- **Total_tracks:** Number of total tracks of the album\n- **T_name:** Name of the track\n- **Duration:** Duration of the track (in ms)\n- **Danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements. A value of 0.0 is least danceable and 1.0 is the most danceable.\n- **Energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. \n- **Key:** The key the track is in. Integers map to pitches using standard Pitch\n- **Mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.-\n- **Speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n- **Acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. \n- **Instrumentalness**: Predicts whether a track contains no vocals. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. \n- **Liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.\n- **Valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative (e.g. sad, depressed, angry). \n- **Tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n- **Time Signature:** An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). \n\n# Beneficiaries\n- Music production companies like Universal Music Group, Sony Music Entertainment, T-Series, etc, are some of the probable beneficiaries of this analysis. \n- If they somehow get to know that say *danceability* helps to increase the popularity of the album, then they can think about producing albums with such tracks.\n- Consider another situation in which they got to know that inclusion of tracks of a certain artist can help increase the popularity of an album, and hence, they would approach such artists more.\n- Music applications like Spotify, Prime Music, Deezer, YouTube Music, SoundCloud, etc can also benefit from this kind of analysis. In the current scenario, the `popularity` is calculated on the basis of number of likes, number of playbacks, etc. \n- But if these apps can acquire the information about these attributes of albums from say, music critics, artists themselves, etc, then they can predict the albums which are more likely to be popular in the near future, and hence, can focus on promoting such albums to a greater extent.\n\n\n### Acknowledgements\nData collected via [Spotify Web API](https://developer.spotify.com/documentation/web-api/)\nPhoto by [Clay Banks](https://unsplash.com/@claybanks) on [Unsplash](https://unsplash.com/photos/fEVaiLwWvlU)",
  "datasetId": 1603567,
  "datasetSlug": "music-albums-popularity-prediction",
  "hasDatasetSlug": true,
  "ownerUser": "elemento",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 11486,
  "totalVotes": 17,
  "totalDownloads": 1114,
  "title": "Music Albums Popularity Prediction",
  "hasTitle": true,
  "subtitle": "Practice your ML skills on this Musical Dataset!",
  "hasSubtitle": true,
  "description": "# Context\n***Spotify*** provides an ***API*** service that allows us to access data from their archive of millions of songs. Their API gives the users the ability to download all the possible information about ***Albums***, ***Episodes***, ***Playlists***, ***Tracks***, ***Users***, etc. These features include attributes such as a song\u2019s tempo, level of acoustics, how danceable a song is, and many more similar ones. \n\n# Attributes\n- **ID:** A unique identifier for every row\n- **Name:** Name of the album \n- **Release Date:** Release date of the album\n- **Artists:** All the artists of the album\n- **Total_tracks:** Number of total tracks of the album\n- **T_name:** Name of the track\n- **Duration:** Duration of the track (in ms)\n- **Danceability:** Danceability describes how suitable a track is for dancing based on a combination of musical elements. A value of 0.0 is least danceable and 1.0 is the most danceable.\n- **Energy:** Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. \n- **Key:** The key the track is in. Integers map to pitches using standard Pitch\n- **Mode:** Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.-\n- **Speechiness:** Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n- **Acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. \n- **Instrumentalness**: Predicts whether a track contains no vocals. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. \n- **Liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides a strong likelihood that the track is live.\n- **Valence:** A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive, while tracks with low valence sound more negative (e.g. sad, depressed, angry). \n- **Tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, the tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n- **Time Signature:** An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure). \n\n# Beneficiaries\n- Music production companies like Universal Music Group, Sony Music Entertainment, T-Series, etc, are some of the probable beneficiaries of this analysis. \n- If they somehow get to know that say *danceability* helps to increase the popularity of the album, then they can think about producing albums with such tracks.\n- Consider another situation in which they got to know that inclusion of tracks of a certain artist can help increase the popularity of an album, and hence, they would approach such artists more.\n- Music applications like Spotify, Prime Music, Deezer, YouTube Music, SoundCloud, etc can also benefit from this kind of analysis. In the current scenario, the `popularity` is calculated on the basis of number of likes, number of playbacks, etc. \n- But if these apps can acquire the information about these attributes of albums from say, music critics, artists themselves, etc, then they can predict the albums which are more likely to be popular in the near future, and hence, can focus on promoting such albums to a greater extent.\n\n\n### Acknowledgements\nData collected via [Spotify Web API](https://developer.spotify.com/documentation/web-api/)\nPhoto by [Clay Banks](https://unsplash.com/@claybanks) on [Unsplash](https://unsplash.com/photos/fEVaiLwWvlU)",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "music",
    "tabular",
    "regression"
  ],
  "licenses": [
    {
      "nameNullable": "copyright-authors",
      "name": "copyright-authors",
      "hasName": true
    }
  ],
  "collaborators": [
    {
      "username": "mitishaagarwal",
      "role": "writer"
    }
  ],
  "data": []
}