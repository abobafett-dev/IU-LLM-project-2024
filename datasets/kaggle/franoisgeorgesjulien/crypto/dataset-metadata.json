{
  "id": "franoisgeorgesjulien/crypto",
  "id_no": 3886296,
  "datasetSlugNullable": "crypto",
  "ownerUserNullable": "franoisgeorgesjulien",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Crypto Data Hourly Price since 2017 to 2023-10",
  "subtitleNullable": "1M+ Hourly Price Data Tick for 34 Cryptocurrencies from 2017 to Present, OHLCV",
  "descriptionNullable": "**Find my notebook : [Advanced EDA & Data Wrangling - Crypto Market Data](https://www.kaggle.com/code/franoisgeorgesjulien/advanced-eda-data-wrangling-crypto-market-data)** where I cover the full EDA and advanced data wrangling to get beautiful dataset ready for analysis.\n\n**Find my Deep Reinforcement Learning v1 notebook: [Deep Reinforcement Learning for Trading\n](https://www.kaggle.com/code/franoisgeorgesjulien/deep-reinforcement-learning-for-trading)**\n<br>\n\n**Find my Quant Analysis notebook:[\ud83d\udc8e Quant Analysis & Visualization | BTC V1\n](https://www.kaggle.com/code/franoisgeorgesjulien/quant-analysis-visualization-btc-v1)**\n\n<br>\n\n**Dataset Presentation:**\n\nThis dataset provides a comprehensive collection of hourly price data for 34 major cryptocurrencies, covering a time span from January 2017 to the present day. The dataset includes Open, High, Low, Close, Volume (OHLCV), and the number of trades for each cryptocurrency for each hour (row).\n\nMaking it a valuable resource for cryptocurrency market analysis, research, and trading strategies. Whether you are interested in historical trends or real-time market dynamics, this dataset offers insights into the price movements of a diverse range of cryptocurrencies.\n\nThis is a pure gold mine, for all kind of analysis and predictive models. The granularity of the dataset offers a wide range of possibilities. Have Fun!\n\n**Ready to Use - Cleaned and arranged dataset less than 0.015% of missing data hour:** crypto_data.csv\n\n**First Draft - Before External Sources Merge (to cover missing data points)**: crypto_force.csv\n\n**Original dataset merged from all individual token datasets**: cryptotoken_full.csv\n\n<br>\n\n**crypto_data.csv & cryptotoken_full.csv** highly challenging wrangling situations:\n- fix 'Date' formats and inconsistencies\n- find missing hours and isolate them for each token\n- import external data source containing targeted missing hours and merge dataframes to fill missing rows\n\n*see notebook 'Advanced EDA & Data Wrangling - Crypto Market Data'* to follow along and have a look at the EDA, wrangling and cleaning process.\n\n\n<br>\n\n**Date Range:** From 2017-08-17 04:00:00 to 2023-10-19 23:00:00\n\n**Date Format:** YYYY-MM-DD HH-MM-SS (raw data to be converted to datetime)\n\n**Data Source:** Binance API (some missing rows filled using Kraken & Poloniex market data)\n\n**Crypto Token in the dataset (also available as independent dataset):**\n- 1INCH\n- AAVE\n- ADA (Cardano)\n- ALGO (Algorand)\n- ATOM (Cosmos)\n- AVAX (Avalanche)\n- BAL (Balancer)\n- BCH (Bitcoin Cash)\n- BNB (Binance Coin)\n- BTC (Bitcoin)\n- COMP (Compound)\n- CRV (Curve DAO Token)\n- DENT\n- DOGE (Dogecoin)\n- DOT (Polkadot)\n- DYDX\n- ETC (Ethereum Classic)\n- ETH (Ethereum)\n- FIL (Filecoin)\n- HBAR (Hedera Hashgraph)\n- ICP (Internet Computer)\n- LINK (Chainlink)\n- LTC (Litecoin)\n- MATIC (Polygon)\n- MKR (Maker)\n- RVN (Ravencoin)\n- SHIB (Shiba Inu)\n- SOL (Solana)\n- SUSHI (SushiSwap)\n- TRX (Tron)\n- UNI (Uniswap)\n- VET (VeChain)\n- XLM (Stellar)\n- XMR (Monero)\n\n<br>\n\n**Date column presents some inconsistencies that need to be cleaned before formatting to datetime:**\n- For column 'Symbol' and 'ETCUSDT' = '23-07-27': it is missing all hours (no data, no hourly rows for this day). I fixed it by using the only one row available for that day and duplicated the values for each hour. Can be fixed using this code:\n\n```\nstart_timestamp = pd.Timestamp('2023-07-27 00:00:00')\nend_timestamp = pd.Timestamp('2023-07-27 23:00:00')\n\nhourly_timestamps = pd.date_range(start=start_timestamp, end=end_timestamp, freq='H')\n\nhourly_data = {\n    'Date': hourly_timestamps,\n    'Symbol': 'ETCUSDT',\n    'Open': 18.29,\n    'High': 18.3,\n    'Low': 18.17,\n    'Close': 18.22,\n    'Volume USDT': 127468,\n    'tradecount': 623,\n    'Token': 'ETC'\n}\n\nhourly_df = pd.DataFrame(hourly_data)\ndf = pd.concat([df, hourly_df], ignore_index=True)\n\ndf = df.drop(550341)\n```\n\n- Some rows for 'Date' have extra digits '.000' '.874' etc.. instead of the right format YYYY-MM-DD HH-MM-SS. To clean it you can use the following code:\n\n```\n# Count the occurrences of the pattern '.xxx' in the 'Date' column\ncount_occurrences_before = df['Date'].str.count(r'\\.\\d{3}')\nprint(\"Occurrences before cleaning:\", count_occurrences_before.sum()) \n\n# Remove '.xxx' pattern from the 'Date' column\ndf['Date'] = df['Date'].str.replace(r'\\.\\d{3}', '', regex=True)\n\n# Count the occurrences of the pattern '.xxx' in the 'Date' column after cleaning\ncount_occurrences_after = df['Date'].str.count(r'\\.\\d{3}')\nprint(\"Occurrences after cleaning:\", count_occurrences_after.sum()) \n```\n\n\n**Disclaimer: Any individual or entity choosing to engage in market analysis, develop predictive models, or utilize data for trading purposes must do so at their own discretion and risk. It is important to understand that trading involves potential financial loss, and decisions made in the financial markets carry inherent risks. This dataset is provided for informational and research purposes only, and its use in trading decisions should be made with full awareness of the associated risks. Users are urged to exercise caution, conduct thorough research, and consider seeking advice from qualified financial professionals when engaging in trading activities. The dataset provider assumes no responsibility for trading outcomes. NFA.**",
  "datasetId": 3886296,
  "datasetSlug": "crypto",
  "hasDatasetSlug": true,
  "ownerUser": "franoisgeorgesjulien",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 6071,
  "totalVotes": 37,
  "totalDownloads": 1336,
  "title": "Crypto Data Hourly Price since 2017 to 2023-10",
  "hasTitle": true,
  "subtitle": "1M+ Hourly Price Data Tick for 34 Cryptocurrencies from 2017 to Present, OHLCV",
  "hasSubtitle": true,
  "description": "**Find my notebook : [Advanced EDA & Data Wrangling - Crypto Market Data](https://www.kaggle.com/code/franoisgeorgesjulien/advanced-eda-data-wrangling-crypto-market-data)** where I cover the full EDA and advanced data wrangling to get beautiful dataset ready for analysis.\n\n**Find my Deep Reinforcement Learning v1 notebook: [Deep Reinforcement Learning for Trading\n](https://www.kaggle.com/code/franoisgeorgesjulien/deep-reinforcement-learning-for-trading)**\n<br>\n\n**Find my Quant Analysis notebook:[\ud83d\udc8e Quant Analysis & Visualization | BTC V1\n](https://www.kaggle.com/code/franoisgeorgesjulien/quant-analysis-visualization-btc-v1)**\n\n<br>\n\n**Dataset Presentation:**\n\nThis dataset provides a comprehensive collection of hourly price data for 34 major cryptocurrencies, covering a time span from January 2017 to the present day. The dataset includes Open, High, Low, Close, Volume (OHLCV), and the number of trades for each cryptocurrency for each hour (row).\n\nMaking it a valuable resource for cryptocurrency market analysis, research, and trading strategies. Whether you are interested in historical trends or real-time market dynamics, this dataset offers insights into the price movements of a diverse range of cryptocurrencies.\n\nThis is a pure gold mine, for all kind of analysis and predictive models. The granularity of the dataset offers a wide range of possibilities. Have Fun!\n\n**Ready to Use - Cleaned and arranged dataset less than 0.015% of missing data hour:** crypto_data.csv\n\n**First Draft - Before External Sources Merge (to cover missing data points)**: crypto_force.csv\n\n**Original dataset merged from all individual token datasets**: cryptotoken_full.csv\n\n<br>\n\n**crypto_data.csv & cryptotoken_full.csv** highly challenging wrangling situations:\n- fix 'Date' formats and inconsistencies\n- find missing hours and isolate them for each token\n- import external data source containing targeted missing hours and merge dataframes to fill missing rows\n\n*see notebook 'Advanced EDA & Data Wrangling - Crypto Market Data'* to follow along and have a look at the EDA, wrangling and cleaning process.\n\n\n<br>\n\n**Date Range:** From 2017-08-17 04:00:00 to 2023-10-19 23:00:00\n\n**Date Format:** YYYY-MM-DD HH-MM-SS (raw data to be converted to datetime)\n\n**Data Source:** Binance API (some missing rows filled using Kraken & Poloniex market data)\n\n**Crypto Token in the dataset (also available as independent dataset):**\n- 1INCH\n- AAVE\n- ADA (Cardano)\n- ALGO (Algorand)\n- ATOM (Cosmos)\n- AVAX (Avalanche)\n- BAL (Balancer)\n- BCH (Bitcoin Cash)\n- BNB (Binance Coin)\n- BTC (Bitcoin)\n- COMP (Compound)\n- CRV (Curve DAO Token)\n- DENT\n- DOGE (Dogecoin)\n- DOT (Polkadot)\n- DYDX\n- ETC (Ethereum Classic)\n- ETH (Ethereum)\n- FIL (Filecoin)\n- HBAR (Hedera Hashgraph)\n- ICP (Internet Computer)\n- LINK (Chainlink)\n- LTC (Litecoin)\n- MATIC (Polygon)\n- MKR (Maker)\n- RVN (Ravencoin)\n- SHIB (Shiba Inu)\n- SOL (Solana)\n- SUSHI (SushiSwap)\n- TRX (Tron)\n- UNI (Uniswap)\n- VET (VeChain)\n- XLM (Stellar)\n- XMR (Monero)\n\n<br>\n\n**Date column presents some inconsistencies that need to be cleaned before formatting to datetime:**\n- For column 'Symbol' and 'ETCUSDT' = '23-07-27': it is missing all hours (no data, no hourly rows for this day). I fixed it by using the only one row available for that day and duplicated the values for each hour. Can be fixed using this code:\n\n```\nstart_timestamp = pd.Timestamp('2023-07-27 00:00:00')\nend_timestamp = pd.Timestamp('2023-07-27 23:00:00')\n\nhourly_timestamps = pd.date_range(start=start_timestamp, end=end_timestamp, freq='H')\n\nhourly_data = {\n    'Date': hourly_timestamps,\n    'Symbol': 'ETCUSDT',\n    'Open': 18.29,\n    'High': 18.3,\n    'Low': 18.17,\n    'Close': 18.22,\n    'Volume USDT': 127468,\n    'tradecount': 623,\n    'Token': 'ETC'\n}\n\nhourly_df = pd.DataFrame(hourly_data)\ndf = pd.concat([df, hourly_df], ignore_index=True)\n\ndf = df.drop(550341)\n```\n\n- Some rows for 'Date' have extra digits '.000' '.874' etc.. instead of the right format YYYY-MM-DD HH-MM-SS. To clean it you can use the following code:\n\n```\n# Count the occurrences of the pattern '.xxx' in the 'Date' column\ncount_occurrences_before = df['Date'].str.count(r'\\.\\d{3}')\nprint(\"Occurrences before cleaning:\", count_occurrences_before.sum()) \n\n# Remove '.xxx' pattern from the 'Date' column\ndf['Date'] = df['Date'].str.replace(r'\\.\\d{3}', '', regex=True)\n\n# Count the occurrences of the pattern '.xxx' in the 'Date' column after cleaning\ncount_occurrences_after = df['Date'].str.count(r'\\.\\d{3}')\nprint(\"Occurrences after cleaning:\", count_occurrences_after.sum()) \n```\n\n\n**Disclaimer: Any individual or entity choosing to engage in market analysis, develop predictive models, or utilize data for trading purposes must do so at their own discretion and risk. It is important to understand that trading involves potential financial loss, and decisions made in the financial markets carry inherent risks. This dataset is provided for informational and research purposes only, and its use in trading decisions should be made with full awareness of the associated risks. Users are urged to exercise caution, conduct thorough research, and consider seeking advice from qualified financial professionals when engaging in trading activities. The dataset provider assumes no responsibility for trading outcomes. NFA.**",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "finance",
    "artificial intelligence",
    "time series analysis",
    "deep learning",
    "currencies and foreign exchange"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}