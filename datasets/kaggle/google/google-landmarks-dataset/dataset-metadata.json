{
  "id": "/google-landmarks-dataset",
  "id_no": 24658,
  "datasetSlugNullable": "google-landmarks-dataset",
  "ownerUserNullable": null,
  "usabilityRatingNullable": 0.7647058823529411,
  "titleNullable": "Google-Landmarks Dataset",
  "subtitleNullable": "Label famous (and not-so-famous) landmarks in images",
  "descriptionNullable": "# **Note: The Google Landmarks Dataset v1 is deprecated and no longer available.  Please consider using the [Google Landmarks Dataset v2](https://github.com/cvdfoundation/google-landmark) instead.**\n\nDid you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. This motivated us to release Google-Landmarks, the largest worldwide dataset to date, to foster progress in this problem. \n\nThe dataset is divided into two sets of images, to evaluate two different computer vision tasks: recognition and retrieval. The data was originally described in [1], and published as part of the [Google Landmark Recognition Challenge](https://www.kaggle.com/c/landmark-recognition-challenge) and [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge). \nAdditionally, to spur research in this field, we have open-sourced Deep Local Features (DELF), an attentive local feature descriptor that we believe is especially suited for this kind of task. DELF's code can be found on github via [this link](https://github.com/tensorflow/models/tree/master/research/delf).\n\n**UPDATE**: We have now also made available the Google Landmark Boxes dataset, containing 86 thousand bounding boxes.\n\nIf you make use of the Google Landmarks dataset in your research, please consider citing:\n\n`H. Noh, A. Araujo, J. Sim, T. Weyand, B. Han, \"Large-Scale Image Retrieval with Attentive Deep Local Features\", Proc. ICCV'17`\n\nIf you make use of the Google Landmark Boxes dataset in your research, please consider citing:\n\n`M. Teichmann*, A. Araujo*, M. Zhu and J. Sim, \u201cDetect-to-Retrieve: Efficient Regional Aggregation for Image Search\u201d, Proc. CVPR'19`\n\n### Challenges\n\nThe two challenges associated to this dataset can be found in the following links:\n\n* [Google Landmark Recognition Challenge](https://www.kaggle.com/c/landmark-recognition-challenge)\n* [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge)\n\n### CVPR'18 Workshop\n\nThe [Landmark Recognition Workshop](https://landmarkscvprw18.github.io) at [CVPR 2018](http://cvpr2018.thecvf.com/program/workshops) will discuss recent progress on landmark recognition and image retrieval, taking into account the results of the above-mentioned challenges. Top submissions for the challenges will be invited to give talks at the workshop. \n\n### Content\n\nThe dataset contains URLs of images which are publicly available online (this [Python script](https://www.kaggle.com/tobwey/landmark-recognition-challenge-image-downloader) may be useful to download the images). Note that no image data is released, only URLs. \n\nThe dataset contains test images, training images and index images. The test images are used in both tasks: for the recognition task, a landmark label may be predicted for each test image; for the retrieval task, relevant index images may be retrieved for each test image. The training images are associated to landmark labels, and can be used to train models for the recognition and retrieval challenges (for a visualization of the geographic distribution of training images, see [3]). The index images are used in the retrieval task, composing the set from which images should be retrieved.\n\nNote that the test set for both the recognition and retrieval tasks is the same, to encourage researchers to experiment with both. We also encourage participants to use the training data from the recognition task to train models which could be useful for the retrieval task. Note, however, that there are no landmarks in common between the training/index sets of the two tasks.\n\nThe images listed in the dataset are not directly in our control, so their availability may change over time, and the dataset files may be updated to remove URLs which no longer work.\n\n## Dataset construction\n\nThe training and index sets were constructed by clustering photos with respect to their geolocation and visual similarity using an algorithm similar to the one described in [4]. Matches between training images were established using local feature matching. Note that there may be multiple clusters per landmark, which typically correspond to different views or different parts of the landmark. To avoid bias, no computer vision algorithms were used for ground truth generation. Instead, we established ground truth correspondences between test images and landmarks using human annotators.\n\n### License\n\nThe images listed in this dataset are publicly available on the web, and may have different licenses. Google does not own their copyright.\n\n### References\n\n[1] H. Noh, A. Araujo, J. Sim, T. Weyand, B. Han, \"Large-Scale Image Retrieval with Attentive Deep Local Features\", Proc. ICCV'17\n\n[2] M. Teichmann*, A. Araujo*, M. Zhu and J. Sim, \u201cDetect-to-Retrieve: Efficient Regional Aggregation for Image Search\u201d, Proc. CVPR'19\n\n[3] A. Araujo, T. Weyand, \"Google-Landmarks: A New Dataset and Challenge for Landmark Recognition\", Google Research blog post, available online [here](https://research.googleblog.com/2018/03/google-landmarks-new-dataset-and.html)\n\n[4] Y.-T. Zheng, M. Zhao, Y. Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher T.-S. Chua, H. Neven, \u201cTour the World: Building a Web-Scale Landmark Recognition Engine,\u201d Proc. CVPR\u201909",
  "datasetId": 24658,
  "datasetSlug": "google-landmarks-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "",
  "hasOwnerUser": false,
  "usabilityRating": 0.7647058823529411,
  "hasUsabilityRating": true,
  "totalViews": 200593,
  "totalVotes": 652,
  "totalDownloads": 9926,
  "title": "Google-Landmarks Dataset",
  "hasTitle": true,
  "subtitle": "Label famous (and not-so-famous) landmarks in images",
  "hasSubtitle": true,
  "description": "# **Note: The Google Landmarks Dataset v1 is deprecated and no longer available.  Please consider using the [Google Landmarks Dataset v2](https://github.com/cvdfoundation/google-landmark) instead.**\n\nDid you ever go through your vacation photos and ask yourself: What is the name of this temple I visited in China? Who created this monument I saw in France? Landmark recognition can help! This technology can predict landmark labels directly from image pixels, to help people better understand and organize their photo collections. Today, a great obstacle to landmark recognition research is the lack of large annotated datasets. This motivated us to release Google-Landmarks, the largest worldwide dataset to date, to foster progress in this problem. \n\nThe dataset is divided into two sets of images, to evaluate two different computer vision tasks: recognition and retrieval. The data was originally described in [1], and published as part of the [Google Landmark Recognition Challenge](https://www.kaggle.com/c/landmark-recognition-challenge) and [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge). \nAdditionally, to spur research in this field, we have open-sourced Deep Local Features (DELF), an attentive local feature descriptor that we believe is especially suited for this kind of task. DELF's code can be found on github via [this link](https://github.com/tensorflow/models/tree/master/research/delf).\n\n**UPDATE**: We have now also made available the Google Landmark Boxes dataset, containing 86 thousand bounding boxes.\n\nIf you make use of the Google Landmarks dataset in your research, please consider citing:\n\n`H. Noh, A. Araujo, J. Sim, T. Weyand, B. Han, \"Large-Scale Image Retrieval with Attentive Deep Local Features\", Proc. ICCV'17`\n\nIf you make use of the Google Landmark Boxes dataset in your research, please consider citing:\n\n`M. Teichmann*, A. Araujo*, M. Zhu and J. Sim, \u201cDetect-to-Retrieve: Efficient Regional Aggregation for Image Search\u201d, Proc. CVPR'19`\n\n### Challenges\n\nThe two challenges associated to this dataset can be found in the following links:\n\n* [Google Landmark Recognition Challenge](https://www.kaggle.com/c/landmark-recognition-challenge)\n* [Google Landmark Retrieval Challenge](https://www.kaggle.com/c/landmark-retrieval-challenge)\n\n### CVPR'18 Workshop\n\nThe [Landmark Recognition Workshop](https://landmarkscvprw18.github.io) at [CVPR 2018](http://cvpr2018.thecvf.com/program/workshops) will discuss recent progress on landmark recognition and image retrieval, taking into account the results of the above-mentioned challenges. Top submissions for the challenges will be invited to give talks at the workshop. \n\n### Content\n\nThe dataset contains URLs of images which are publicly available online (this [Python script](https://www.kaggle.com/tobwey/landmark-recognition-challenge-image-downloader) may be useful to download the images). Note that no image data is released, only URLs. \n\nThe dataset contains test images, training images and index images. The test images are used in both tasks: for the recognition task, a landmark label may be predicted for each test image; for the retrieval task, relevant index images may be retrieved for each test image. The training images are associated to landmark labels, and can be used to train models for the recognition and retrieval challenges (for a visualization of the geographic distribution of training images, see [3]). The index images are used in the retrieval task, composing the set from which images should be retrieved.\n\nNote that the test set for both the recognition and retrieval tasks is the same, to encourage researchers to experiment with both. We also encourage participants to use the training data from the recognition task to train models which could be useful for the retrieval task. Note, however, that there are no landmarks in common between the training/index sets of the two tasks.\n\nThe images listed in the dataset are not directly in our control, so their availability may change over time, and the dataset files may be updated to remove URLs which no longer work.\n\n## Dataset construction\n\nThe training and index sets were constructed by clustering photos with respect to their geolocation and visual similarity using an algorithm similar to the one described in [4]. Matches between training images were established using local feature matching. Note that there may be multiple clusters per landmark, which typically correspond to different views or different parts of the landmark. To avoid bias, no computer vision algorithms were used for ground truth generation. Instead, we established ground truth correspondences between test images and landmarks using human annotators.\n\n### License\n\nThe images listed in this dataset are publicly available on the web, and may have different licenses. Google does not own their copyright.\n\n### References\n\n[1] H. Noh, A. Araujo, J. Sim, T. Weyand, B. Han, \"Large-Scale Image Retrieval with Attentive Deep Local Features\", Proc. ICCV'17\n\n[2] M. Teichmann*, A. Araujo*, M. Zhu and J. Sim, \u201cDetect-to-Retrieve: Efficient Regional Aggregation for Image Search\u201d, Proc. CVPR'19\n\n[3] A. Araujo, T. Weyand, \"Google-Landmarks: A New Dataset and Challenge for Landmark Recognition\", Google Research blog post, available online [here](https://research.googleblog.com/2018/03/google-landmarks-new-dataset-and.html)\n\n[4] Y.-T. Zheng, M. Zhao, Y. Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher T.-S. Chua, H. Neven, \u201cTour the World: Building a Web-Scale Landmark Recognition Engine,\u201d Proc. CVPR\u201909",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "arts and entertainment",
    "computer science",
    "computer vision",
    "classification",
    "deep learning",
    "image"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}