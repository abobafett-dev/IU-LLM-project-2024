{
  "id": "ignaciorlando/ussimandsegm",
  "id_no": 762107,
  "datasetSlugNullable": "ussimandsegm",
  "ownerUserNullable": "ignaciorlando",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "US simulation & segmentation",
  "subtitleNullable": "Real and synthetic abdominal ultrasound scans, with manual segmentations",
  "descriptionNullable": "### Context\n\nUltrasound (US) imaging allows a fast, cost-effective and non-invasive way to quickly assess inner areas of the body. This modality is therefore widely applied in emergentology, where it is used e.g. to identify potential bleedings or other traumatic injuries. Accurate interpretations, however, required particularly trained readers that must discriminate between variable echogenicity properties and speckle noise characteristics of the tissues. Moreover, the quality of the interpretation is also linked with the ability of the radiologists to localize anatomical areas using the US transducer. Consequently, a significant effort is being made to develop tools to train the readers both in US image acquisition and interpretation. In this sense, US simulation offers a promising tool as it allows to recreate artificial scenarios with different abnormalities without needing volunteers with pathologies. Furthermore, specific training exercises can be introduced to learn how to manipulate the transducer without using a real device.\n\nGenerating realistic scans is a cornerstone to ensure a smooth transition of human trainees from US simulations to real US acquisitions, specially for image interpretation. However, current approaches still struggle to produce truthful artificial scans. In particular, US simulators based on ray-casting techniques have demonstrated to be efficient in terms of computational cost but fails in modeling some anatomical features as muscle fibers, fat streaks and microcalcifications or even imaging artifacts such as shadows and reverberations, comet tail artifacts, etc. In (Vitale et al., 2019) we introduced a CycleGAN based approach to improve realism in patient-specific abdominal US simulations obtained using ray-tracing algorithms. Although the method showed to outperform the baseline technique in a user based study, an in-depth analysis allowed us to observe that the generative model eventually introduces artifacts inconsistent with the anatomy of the patient. This is a typical problem of cycle-consistency based GANs, as stated in several papers.  To overcome this limitation, several approaches have been recently proposed in the field of computer vision. In particular, the so-called ContrastGANs (Liang et al. 2017) alleviate this issue by using object-mask annotations provided in the training sets to guide the generation. However, collecting these segmentations is extremely hard when working with US images, as the tissues interfaces are usually difficult to discriminate and identifying the abdominal organs is challenging by itself.\n\nIn this Kaggle dataset we provide you with real US scans and synthetic images generated with a ray-casting based simulator. Both datasets are extended with annotations of several abdominal organs, in an effort to provide tools to improve current existing methods.\n\n\n### Content\n\nThe real US scans were acquired from 11 subjects without any abdominal pathology or known disease (40 % were female and 60 % were male, with age=27\u00b13years). The procedure was a routine abdominal ultrasound session, following the corresponding acquisition protocol (9 hours of fasting before taking the examination). Some scans that make up the data set are:  Left and right intercostal scan, left and right subcostal margin scan, longitudinal scan, transverse scan, FAST- Right upper quadrant, FAST- Left upper quadrant and FAST- Heart. To  obtain  the  images  we  used  a  SonoSite  M  Turbo  V  1.3  ultrasound  device. This data set comprises 617 real US scans. Manual annotations of several abdominal organs are being performed, including liver,  kidneys,  gallbladder,  spleen and vessels on a subset of 61 real US scans.  \n\nThe simulated US scans were generated from our ray-casting based simulator (Rubi et al.,2017) using source images from the VISCERAL Anatomy3 challenge (Jimenez et al.2016). Since the original volumes comprise the entire body, 13 CT scans were manually cropped to focus only in the abdominal cavity (from the thoracic diaphragm to the pelvic inlet) and were adapted to used in the simulator. In addition, the simulator requires a scattering volume that was retrieved per each individual CT scan using the efficient approach presented in (D'Amato et al., 2015). As a result, this data set comprises 926 artificially US scans. Annotations of each scan was generated using silver standard annotations of several abdominal organs (including liver, pancreas, adrenals, gallbladder, spleen, liver, vessels and bones) included in the CT dataset. \n\nColor Labels:\nviolet = liver\nyellow = kidney\nblue = pancreas\nred = vessels\nlight blue= adrenals\ngreen = gallbladder\nwhite = bones\npink = spleen\n\n### Acknowledgements\n\nThis dataset has been publicly released after winning a Kaggle Open Data Research Grant.\n\n\n### Inspiration\n\nSeveral different methods can be trained taking advantage of our dataset, including:\n* Unpaired image-to-image translation methods to improve the realism of US simulations.\n* US segmentation methods both on real and synthetic images.\n\n\n### Credits\n\nIf you use this dataset, please cite the following publication:\n\nVitale, S., Orlando, J. I., Iarussi, E., & Larrabide, I. (2019). Improving realism in patient-specific abdominal ultrasound simulation using CycleGANs. International Journal of Computer Assisted Radiology and Surgery, 15(2), 183-192.\n\n\n### References\n\nLiang, X., Zhang, H., & Xing, E. P. (2017). Generative semantic manipulation with contrasting gan. arXiv preprint arXiv:1708.00315.\n\nVitale, S., Orlando, J. I., Iarussi, E., & Larrabide, I. (2019). Improving realism in patient-specific abdominal ultrasound simulation using CycleGANs. International Journal of Computer Assisted Radiology and Surgery, 15(2), 183-192.\n\nRubi, P., Vera, E. F., Larrabide, J., Calvo, M., D'Amato, J. P., & Larrabide, I. (2017, January). Comparison of real-time ultrasound simulation models using abdominal CT images. In 12th international symposium on medical information processing and analysis (Vol. 10160, p. 1016009). International Society for Optics and Photonics.\n\nJimenez-del-Toro, O., M\u00fcller, H., Krenn, M., Gruenberg, K., Taha, A. A., Winterstein, M., ... & Kontokotsios, G. (2016). Cloud-based evaluation of anatomical structure segmentation and landmark detection algorithms: VISCERAL anatomy benchmarks. IEEE transactions on medical imaging, 35(11), 2459-2475.\n\nD'Amato, J. P., Vercio, L. L., Rubi, P., Vera, E. F., Barbuzza, R., Del Fresno, M., & Larrabide, I. (2015, December). Efficient scatter model for simulation of ultrasound images from computed tomography data. In 11th International Symposium on Medical Information Processing and Analysis (Vol. 9681, p. 968105). International Society for Optics and Photonics.",
  "datasetId": 762107,
  "datasetSlug": "ussimandsegm",
  "hasDatasetSlug": true,
  "ownerUser": "ignaciorlando",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 34046,
  "totalVotes": 42,
  "totalDownloads": 2440,
  "title": "US simulation & segmentation",
  "hasTitle": true,
  "subtitle": "Real and synthetic abdominal ultrasound scans, with manual segmentations",
  "hasSubtitle": true,
  "description": "### Context\n\nUltrasound (US) imaging allows a fast, cost-effective and non-invasive way to quickly assess inner areas of the body. This modality is therefore widely applied in emergentology, where it is used e.g. to identify potential bleedings or other traumatic injuries. Accurate interpretations, however, required particularly trained readers that must discriminate between variable echogenicity properties and speckle noise characteristics of the tissues. Moreover, the quality of the interpretation is also linked with the ability of the radiologists to localize anatomical areas using the US transducer. Consequently, a significant effort is being made to develop tools to train the readers both in US image acquisition and interpretation. In this sense, US simulation offers a promising tool as it allows to recreate artificial scenarios with different abnormalities without needing volunteers with pathologies. Furthermore, specific training exercises can be introduced to learn how to manipulate the transducer without using a real device.\n\nGenerating realistic scans is a cornerstone to ensure a smooth transition of human trainees from US simulations to real US acquisitions, specially for image interpretation. However, current approaches still struggle to produce truthful artificial scans. In particular, US simulators based on ray-casting techniques have demonstrated to be efficient in terms of computational cost but fails in modeling some anatomical features as muscle fibers, fat streaks and microcalcifications or even imaging artifacts such as shadows and reverberations, comet tail artifacts, etc. In (Vitale et al., 2019) we introduced a CycleGAN based approach to improve realism in patient-specific abdominal US simulations obtained using ray-tracing algorithms. Although the method showed to outperform the baseline technique in a user based study, an in-depth analysis allowed us to observe that the generative model eventually introduces artifacts inconsistent with the anatomy of the patient. This is a typical problem of cycle-consistency based GANs, as stated in several papers.  To overcome this limitation, several approaches have been recently proposed in the field of computer vision. In particular, the so-called ContrastGANs (Liang et al. 2017) alleviate this issue by using object-mask annotations provided in the training sets to guide the generation. However, collecting these segmentations is extremely hard when working with US images, as the tissues interfaces are usually difficult to discriminate and identifying the abdominal organs is challenging by itself.\n\nIn this Kaggle dataset we provide you with real US scans and synthetic images generated with a ray-casting based simulator. Both datasets are extended with annotations of several abdominal organs, in an effort to provide tools to improve current existing methods.\n\n\n### Content\n\nThe real US scans were acquired from 11 subjects without any abdominal pathology or known disease (40 % were female and 60 % were male, with age=27\u00b13years). The procedure was a routine abdominal ultrasound session, following the corresponding acquisition protocol (9 hours of fasting before taking the examination). Some scans that make up the data set are:  Left and right intercostal scan, left and right subcostal margin scan, longitudinal scan, transverse scan, FAST- Right upper quadrant, FAST- Left upper quadrant and FAST- Heart. To  obtain  the  images  we  used  a  SonoSite  M  Turbo  V  1.3  ultrasound  device. This data set comprises 617 real US scans. Manual annotations of several abdominal organs are being performed, including liver,  kidneys,  gallbladder,  spleen and vessels on a subset of 61 real US scans.  \n\nThe simulated US scans were generated from our ray-casting based simulator (Rubi et al.,2017) using source images from the VISCERAL Anatomy3 challenge (Jimenez et al.2016). Since the original volumes comprise the entire body, 13 CT scans were manually cropped to focus only in the abdominal cavity (from the thoracic diaphragm to the pelvic inlet) and were adapted to used in the simulator. In addition, the simulator requires a scattering volume that was retrieved per each individual CT scan using the efficient approach presented in (D'Amato et al., 2015). As a result, this data set comprises 926 artificially US scans. Annotations of each scan was generated using silver standard annotations of several abdominal organs (including liver, pancreas, adrenals, gallbladder, spleen, liver, vessels and bones) included in the CT dataset. \n\nColor Labels:\nviolet = liver\nyellow = kidney\nblue = pancreas\nred = vessels\nlight blue= adrenals\ngreen = gallbladder\nwhite = bones\npink = spleen\n\n### Acknowledgements\n\nThis dataset has been publicly released after winning a Kaggle Open Data Research Grant.\n\n\n### Inspiration\n\nSeveral different methods can be trained taking advantage of our dataset, including:\n* Unpaired image-to-image translation methods to improve the realism of US simulations.\n* US segmentation methods both on real and synthetic images.\n\n\n### Credits\n\nIf you use this dataset, please cite the following publication:\n\nVitale, S., Orlando, J. I., Iarussi, E., & Larrabide, I. (2019). Improving realism in patient-specific abdominal ultrasound simulation using CycleGANs. International Journal of Computer Assisted Radiology and Surgery, 15(2), 183-192.\n\n\n### References\n\nLiang, X., Zhang, H., & Xing, E. P. (2017). Generative semantic manipulation with contrasting gan. arXiv preprint arXiv:1708.00315.\n\nVitale, S., Orlando, J. I., Iarussi, E., & Larrabide, I. (2019). Improving realism in patient-specific abdominal ultrasound simulation using CycleGANs. International Journal of Computer Assisted Radiology and Surgery, 15(2), 183-192.\n\nRubi, P., Vera, E. F., Larrabide, J., Calvo, M., D'Amato, J. P., & Larrabide, I. (2017, January). Comparison of real-time ultrasound simulation models using abdominal CT images. In 12th international symposium on medical information processing and analysis (Vol. 10160, p. 1016009). International Society for Optics and Photonics.\n\nJimenez-del-Toro, O., M\u00fcller, H., Krenn, M., Gruenberg, K., Taha, A. A., Winterstein, M., ... & Kontokotsios, G. (2016). Cloud-based evaluation of anatomical structure segmentation and landmark detection algorithms: VISCERAL anatomy benchmarks. IEEE transactions on medical imaging, 35(11), 2459-2475.\n\nD'Amato, J. P., Vercio, L. L., Rubi, P., Vera, E. F., Barbuzza, R., Del Fresno, M., & Larrabide, I. (2015, December). Efficient scatter model for simulation of ultrasound images from computed tomography data. In 11th International Symposium on Medical Information Processing and Analysis (Vol. 9681, p. 968105). International Society for Optics and Photonics.",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "pre-trained model"
  ],
  "licenses": [
    {
      "nameNullable": "copyright-authors",
      "name": "copyright-authors",
      "hasName": true
    }
  ],
  "collaborators": [
    {
      "username": "santiagovitale",
      "role": "writer"
    }
  ],
  "data": []
}