{
  "id": "impapan/human-activity-recognition-using-smartphone",
  "id_no": 1291489,
  "datasetSlugNullable": "human-activity-recognition-using-smartphone",
  "ownerUserNullable": "impapan",
  "usabilityRatingNullable": 0.8125,
  "titleNullable": "Human Activity Recognition Using Smartphone",
  "subtitleNullable": "Human Activity Recognition database built from the recordings of daily activity.",
  "descriptionNullable": "## About this dataset\n\n```\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nCheck the README.txt file for further details about this dataset.\n```\n\n## Attribute Information:\n\n```\nFor each record in the dataset it is provided:\n- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n- Triaxial Angular velocity from the gyroscope.\n- A 561-feature vector with time and frequency domain variables.\n- Its activity label.\n- An identifier of the subject who carried out the experiment.\n```\n\n## Acknowledgements\n\nIf you use this dataset in your research, please credit the authors\n\n### Citations\n\n```\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n```",
  "datasetId": 1291489,
  "datasetSlug": "human-activity-recognition-using-smartphone",
  "hasDatasetSlug": true,
  "ownerUser": "impapan",
  "hasOwnerUser": true,
  "usabilityRating": 0.8125,
  "hasUsabilityRating": true,
  "totalViews": 2763,
  "totalVotes": 10,
  "totalDownloads": 99,
  "title": "Human Activity Recognition Using Smartphone",
  "hasTitle": true,
  "subtitle": "Human Activity Recognition database built from the recordings of daily activity.",
  "hasSubtitle": true,
  "description": "## About this dataset\n\n```\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\nCheck the README.txt file for further details about this dataset.\n```\n\n## Attribute Information:\n\n```\nFor each record in the dataset it is provided:\n- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\n- Triaxial Angular velocity from the gyroscope.\n- A 561-feature vector with time and frequency domain variables.\n- Its activity label.\n- An identifier of the subject who carried out the experiment.\n```\n\n## Acknowledgements\n\nIf you use this dataset in your research, please credit the authors\n\n### Citations\n\n```\nDavide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.\n```",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "physics",
    "time series analysis"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}