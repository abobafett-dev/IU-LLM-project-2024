{
  "id": "jmmvutu/lottery-features-for-machine-learning-ai",
  "id_no": 723130,
  "datasetSlugNullable": "lottery-features-for-machine-learning-ai",
  "ownerUserNullable": "jmmvutu",
  "usabilityRatingNullable": 0.8235294117647058,
  "titleNullable": "Lottery features for time series Machine Learning",
  "subtitleNullable": "Signal processing and Feature-engineering based on lottery draw results",
  "descriptionNullable": "## Context\n\nWinning the lottery has always been a dream for a lot of people. Because of that, a lot of work have been done in the past to try and tackle the challenge.\nWe saw the rise of different approaches, from computer softwares that optimize your lottery picks to numerical and statistical analysis, as well as esoteric approaches.\n\nMore recently, with the rise of *machine learning* (_hereafter: **ML**_), people have tried to tackle the problem by trying to have a ML model predict the next combination.\nHowever, most of those who undertook to try this approach had poor understanding of ML-related basics such as data preprocessing and signal processing.\n&gt; As an example of bad things that were done, there were projects that built a ML model that took raw draw results and outputted N numbers, hoping that those N numbers would be the correct combination of the next draw.\n&gt; To give you an analogy with a  real world example, it would be like taking in the raw data (bytes) of an audio file as the (sole) input of a model and hope that it outputs a concept like *bpm*, *music genre*, ...\n&gt; Of course, this resulted in extremely bad results and **ML models that didn't learn a single thing**.\n&gt; Similarly, it would be the same as trying to predict stock prices using only the price as the sole input variable. It would be bound to fail without creating higher-level features. Models that do not do that would never succeed.\n\n\nBecause Machine-Learning-based approaches *were bound* to fail before even beginning *unless something was done* regarding data and signal processing, I decided to make my contribution by crafting higher-level features (/ abstract concepts) from historic lottery data.\n\nI leave under mouth discussions about the mathematical theory of probability (which I explained more in [the repository of Lofea](https://github.com/JeffMv/Lofea), a project I created to generate this dataset) or why mathematicians say it would be theoretically impossible to predict. Those specifics and other questions can be discussed in the comments section.\nTo let people still dream enough to try and tackle the problem, I'd like to point out that stock market prices (which are also numerical time series data) are said to be unpredictable due to the [Efficient Market Hypothesis](https://en.wikipedia.org/wiki/Efficient-market_hypothesis). Regardless, firms and individuals have been trying their best to try and predict the evolution of stock prices. Although theories tell us something is impossible in theory, there might be a practical implementation flaw that might get exploited if studied carefully enough. Who knows unless they try ?\n\n\n## Content\nPreprocessed historical results.\n\n**Tackling a big and complex task** often requires problem solving methodologies, such as *divide and conquer*. This is why instead of tackling a regular pick 6 among 49 or so lottery, this dataset focuses on simple 1/10 lottery data (i.e. pick 1 among 10). But it also includes a version for the Euromillions (a 5/50 lottery).\n\nYou will find in the archive `features.04-2021` files containing computed features, as well as the whole draw histories used to compute them.\nOne lottery is the [Euromillions](https://www.fdj.fr/jeux-de-tirage/euromillions-my-million/resultats), and the other is  [TrioMagic](https://jeux.loro.ch/games/magic3), though similar datasets can be crafted for lotteries that share their respective formats.\nRegarding *1/10* lottery, since most of *1/10* lotteries have several *pools* (/columns) from which one has to pick, this kind of dataset with higher-level features can be created for each column individually and compared among different lotteries.\n\n### Preprocessing\n\nThe big idea here is to preprocess historic draws as if it was a *signal* or a *time serie* and create higher-level features based on it.\nInspired by the approach of working with numerical time series signals such as stock market prices.\n\n### Labels (referred to as \"_Targets_\" in the dataset)\n\nThere are several high-level concepts we may want to predict, such as the parity of the next draw. This would be a *classification problem*. You may also choose to tackle a *regression* problem, such as trying to predict the repartition of rate of even numbers in the next N draws (for instance N=2, 3 or 5).\nThere are also other possible targets besides the parity, such as the *Universe length*, which will be described below.\n\nThe target you choose to predict may influence what kind of features you will try to include or craft.\n\n### Features\n\nSeveral of the features included in this dataset are based on a concept I came up with I called \u00ab _**Universe Length**_ \u00bb.\nBasically, *Universe Length* (referred to as `ULen` in the dataset) is the number of different numbers in a given time frame.\nFor instance in a 1/10 lottery, *Universe Length* over a _**time frame** of 10 draws_ with the following draw history `[3,4,1,4,9,5,5,9,8,1]` would be `6` since there are 6 different numbers drawn in this running window frame.\n\nSimilarly, there are other features that are based on a **running window `frame`**.\nFor the lotteries 5/50 and 1/10, setting the running window to 10 was a reasonable choice. (For the stars of Euromillions, which is a 2/12, the runnig window `frame` was set to 6). \n\n#### **Approach - Features of draws**\n\nThere are different kind of approaches. Among them, one approach is to make statistics and features related to each ball. And another approach is to study the characteristics of draws (instead of individual balls).\nIn this dataset (or at least the early version of it) the chosen approach was the latter: studying draws.\n\nThe names of features and targets of this dataset have been chosen for the clarity and unicity (in spite of shortness). Thus they are quite verbose, so feel free to rename them when you get used to them.\nTheir verbosity allows distinguishing between variations around a same concept, or variations of running window or such...\n\n\n**Explanation of features:**\n\n- `universe-length` : Number of *different* numbers in the current running window. (Always see the file's description to know the applied running window `frame`).\n- `universe-length-offset-from-center` : based on `universe-length`. It just shows the distance to the center of all possible universe length values. Note that just because a universe length is possible does not mean it will ever occur. For instance, the lowest possible universe length would mean that only the same ball/set of drawn number is drawn over and over again within the running window. Although this could theoretically happen, its low probability makes it impossible.\n- `parity` : Number of even numbers in the *current* draw\n- `parity-over-frame` : Number of even numbers in the draws of the *running window*\n- `last-moving-direction-of-universe-length` : Las moving direction of the `universe-length` feature. -1 means decreasing, +1 means increasing.\n\n- `move-balance-of-universe-length_latest-minus-mean` : formula: The mean of universe lengths over the running window, minus the current value of the `universe length`\n- `move-balance-of-universe-length_mean-minus-earliest` : formula: Current value of the `universe length` minus the mean of universe lengths over the running window\n- `move-balance-of-universe-length_latest-minus-mean_runningWindowX2`: same as the other one, but the mean is applied on a frame twice bigger.\n- `move-balance-of-universe-length_mean-minus-earliest_runningWindowX2`: same concept as the above feature\n\n- `universe-length-drop` : How much the universe length can at most drop in the next draw's result\n- `universe-length-increase` : How much the universe length can at most increase in the next draw's result\n- `universe-length-repetition-same` : the number of times the current `universe length` has been repeated successively\n- `greater-universe-length-than-repetition` : the number successive times we find a higher `universe length` than the current one over the running window\n- `universe-length-didfollowincrease` : how much did the universe length increase *from the previous* draw. 0 means no increase or decrease.\n\n- `mean-frequency-of-drawn-numbers` : the mean frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n- `median-frequency-of-drawn-numbers` : the median frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n- `mean-frequency-of-drawn-numbers-over-X-draws` : same as `mean-frequency-of-drawn-numbers` but over a running window of `X` draws.\n- `median-frequency-of-drawn-numbers-over-X-draws` : same as `median-frequency-of-drawn-numbers` but over a running window of `X` draws.\n\n- `mean-gap-of-drawn-numbers` : the mean appearance gap of each the drawn balls. Takes the individual gaps of each of the balls in the current result draw (i.e. the number of draws between the last time a given ball was drawn), sums them up, and then divides by the number of balls in the draw.\n- `median-gap-of-drawn-numbers` : like `mean-gap-of-drawn-numbers` but using the median instead of the mean. Note that in 1/N lottery pools (one ball pulled out of N), the two features are always equal, which is logic.\n- `mean-gap-of-drawn-numbers-bounded-at-X-draws` : where `X` is a number. Same as its counterpart `mean-gap-of-drawn-numbers` but over a running window of `X` draws.\n- `median-gap-of-drawn-numbers-bounded-at-X-draws` : similar to the previous one, but using the median.\n\n- `mean-of-4-gaps-of-each-drawn-numbers` : formula : systematically take the last 4 gaps of each of the balls in the current draw. Sum all these gaps together and take the overall mean. (i.e. average of all those gaps)\n- `median-of-4-gaps-of-each-drawn-numbers` : same formula as its paronym, but takes the median of all. For 1/N lotteries, both are equal.\n- `median-of-means-of-X-gaps-of-each-drawn-numbers` : where `X` is a number. Formula : systematically take the last X gaps of each of the balls of the current draw. Take the mean for each ball. Then sum up all the means together and take the median of them. In short, it is a median( of average( of [X-latest-gaps-of-symbol-Y] )). A median of averages may not make much sense statistically speaking. This feature is provided as is.\n\n\n\n**TARGET columns** (i.e. supervised learning)\n(i.e. ideas of what you might want to predict)\n\n- `target_universe-length-willFollowIncrease` : TARGET feature (supervised learning). Same as the `universe-length-didfollowincrease` feature but for the draw that comes in the future. (DO NOT mistake them). One goal can be to predict this value given only the other features. Was named `universe-length-willfollowincrease` in an earlier version of the dataset.\n- `target_coming-universe-length-change-in-next-draw` : same as `target_universe-length-willFollowIncrease` but indicates decreases as well as increases and stagnations. Might be better suited for a regression, but you do as you see fit.\n- `target_coming-mean-universe-length-change-in-next-2-draws` : takes the mean change of universe length in the next 2 draws instead of only one. See `target_coming-universe-length-change-in-next-draw`.\n- `target_future-1rst-value-of-universe-length-center-offset-from-center` : the next value of the feature `universe-length-offset-from-center`. You may want to try to predict that.\n- `target_coming-universe-length-center-offset-change-in-next-draw` : the relative change of the feature `universe-length-offset-from-center` that will occur in the next draw.\n- `target_coming-mean-universe-length-center-offset-change-in-next-2-draws` : same as the previous feature, but we take the mean change in the next 2 draws.\n\n\n**Special columns:**\n- `date` : date of the lottery draw\n- `draw` / `draw-result` : most recent draw of the specified date. displayed for convenience\n- `running-window-frame-length` : convenience a CONSTANT column. it is there to remind you of the base *running window* size (also called *frame length*) used to compute most features. When some features use a given multiple of the *frame length*, this is the value that gets multiplied.\n- `draw-id` : the draw identifier. It can be the date or in another form (such as reversed date `yyyymmdd`). Only displayed for convenience\n\n\n## Inspiration / Ideas of approaches\n\n- Transforming the problem into another that outputs binary values.\n   This would allow you/us to tap into the enormity of works and theorems done for _**binomial problems**_ both in statistics and probability.\n   It would allow you to compare the distribution of the randomness to much more scholarly examples (such as heads or tails) and deduce the law of probability / probability distribution / random variable behind a particular lottery.\n   &gt; If I remember correctly a maths course, there should be a **theorem / lemme** in probability that say how likely a binomial distribution is to take exaggerated values (big outliers). Regardless, there are a lot of results already, such as **Bernoulli trial** to study and compare theoretical and practical results.\n\n\n- Treating the problem as a numerical time series problem\n- Treating the problem as a signal processing problem\n- Asking different high-level questions and formulating the problem differently. For instance\n  - Trying to predict the repartition of a feature (like parity) in the next 5 draws (instead of only predicting the parity of the next draw)\n\n- Trying to identify critical points in time where several features converge towards predicting the same thing.\n   For instance, there could be a time where the parity of the last 10 drawn numbers was even. If on top of that several even numbers have had a particularly high appearance rate in a preceding big frame of time, then these two features would lean towards thinking that there is a higher chance of seeing an odd number drawn next. (Though probability independence would say that such convergence are meaningless, here we are to suppose the opposite until we face the hard truth, as in a proof by contradiction).\n   See the note about **Bernoulli trials**-related *theorem / lemme* as mentioned above.\n\n\n## Acknowledgements\n\n\n## License\n\nDataset licensed under [CC-BY](https://creativecommons.org/licenses/by/4.0/)\n\nPersonal project for generating the dataset: [Lofea](https://github.com/JeffMv/Lofea) under [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
  "datasetId": 723130,
  "datasetSlug": "lottery-features-for-machine-learning-ai",
  "hasDatasetSlug": true,
  "ownerUser": "jmmvutu",
  "hasOwnerUser": true,
  "usabilityRating": 0.8235294117647058,
  "hasUsabilityRating": true,
  "totalViews": 7544,
  "totalVotes": 8,
  "totalDownloads": 271,
  "title": "Lottery features for time series Machine Learning",
  "hasTitle": true,
  "subtitle": "Signal processing and Feature-engineering based on lottery draw results",
  "hasSubtitle": true,
  "description": "## Context\n\nWinning the lottery has always been a dream for a lot of people. Because of that, a lot of work have been done in the past to try and tackle the challenge.\nWe saw the rise of different approaches, from computer softwares that optimize your lottery picks to numerical and statistical analysis, as well as esoteric approaches.\n\nMore recently, with the rise of *machine learning* (_hereafter: **ML**_), people have tried to tackle the problem by trying to have a ML model predict the next combination.\nHowever, most of those who undertook to try this approach had poor understanding of ML-related basics such as data preprocessing and signal processing.\n&gt; As an example of bad things that were done, there were projects that built a ML model that took raw draw results and outputted N numbers, hoping that those N numbers would be the correct combination of the next draw.\n&gt; To give you an analogy with a  real world example, it would be like taking in the raw data (bytes) of an audio file as the (sole) input of a model and hope that it outputs a concept like *bpm*, *music genre*, ...\n&gt; Of course, this resulted in extremely bad results and **ML models that didn't learn a single thing**.\n&gt; Similarly, it would be the same as trying to predict stock prices using only the price as the sole input variable. It would be bound to fail without creating higher-level features. Models that do not do that would never succeed.\n\n\nBecause Machine-Learning-based approaches *were bound* to fail before even beginning *unless something was done* regarding data and signal processing, I decided to make my contribution by crafting higher-level features (/ abstract concepts) from historic lottery data.\n\nI leave under mouth discussions about the mathematical theory of probability (which I explained more in [the repository of Lofea](https://github.com/JeffMv/Lofea), a project I created to generate this dataset) or why mathematicians say it would be theoretically impossible to predict. Those specifics and other questions can be discussed in the comments section.\nTo let people still dream enough to try and tackle the problem, I'd like to point out that stock market prices (which are also numerical time series data) are said to be unpredictable due to the [Efficient Market Hypothesis](https://en.wikipedia.org/wiki/Efficient-market_hypothesis). Regardless, firms and individuals have been trying their best to try and predict the evolution of stock prices. Although theories tell us something is impossible in theory, there might be a practical implementation flaw that might get exploited if studied carefully enough. Who knows unless they try ?\n\n\n## Content\nPreprocessed historical results.\n\n**Tackling a big and complex task** often requires problem solving methodologies, such as *divide and conquer*. This is why instead of tackling a regular pick 6 among 49 or so lottery, this dataset focuses on simple 1/10 lottery data (i.e. pick 1 among 10). But it also includes a version for the Euromillions (a 5/50 lottery).\n\nYou will find in the archive `features.04-2021` files containing computed features, as well as the whole draw histories used to compute them.\nOne lottery is the [Euromillions](https://www.fdj.fr/jeux-de-tirage/euromillions-my-million/resultats), and the other is  [TrioMagic](https://jeux.loro.ch/games/magic3), though similar datasets can be crafted for lotteries that share their respective formats.\nRegarding *1/10* lottery, since most of *1/10* lotteries have several *pools* (/columns) from which one has to pick, this kind of dataset with higher-level features can be created for each column individually and compared among different lotteries.\n\n### Preprocessing\n\nThe big idea here is to preprocess historic draws as if it was a *signal* or a *time serie* and create higher-level features based on it.\nInspired by the approach of working with numerical time series signals such as stock market prices.\n\n### Labels (referred to as \"_Targets_\" in the dataset)\n\nThere are several high-level concepts we may want to predict, such as the parity of the next draw. This would be a *classification problem*. You may also choose to tackle a *regression* problem, such as trying to predict the repartition of rate of even numbers in the next N draws (for instance N=2, 3 or 5).\nThere are also other possible targets besides the parity, such as the *Universe length*, which will be described below.\n\nThe target you choose to predict may influence what kind of features you will try to include or craft.\n\n### Features\n\nSeveral of the features included in this dataset are based on a concept I came up with I called \u00ab _**Universe Length**_ \u00bb.\nBasically, *Universe Length* (referred to as `ULen` in the dataset) is the number of different numbers in a given time frame.\nFor instance in a 1/10 lottery, *Universe Length* over a _**time frame** of 10 draws_ with the following draw history `[3,4,1,4,9,5,5,9,8,1]` would be `6` since there are 6 different numbers drawn in this running window frame.\n\nSimilarly, there are other features that are based on a **running window `frame`**.\nFor the lotteries 5/50 and 1/10, setting the running window to 10 was a reasonable choice. (For the stars of Euromillions, which is a 2/12, the runnig window `frame` was set to 6). \n\n#### **Approach - Features of draws**\n\nThere are different kind of approaches. Among them, one approach is to make statistics and features related to each ball. And another approach is to study the characteristics of draws (instead of individual balls).\nIn this dataset (or at least the early version of it) the chosen approach was the latter: studying draws.\n\nThe names of features and targets of this dataset have been chosen for the clarity and unicity (in spite of shortness). Thus they are quite verbose, so feel free to rename them when you get used to them.\nTheir verbosity allows distinguishing between variations around a same concept, or variations of running window or such...\n\n\n**Explanation of features:**\n\n- `universe-length` : Number of *different* numbers in the current running window. (Always see the file's description to know the applied running window `frame`).\n- `universe-length-offset-from-center` : based on `universe-length`. It just shows the distance to the center of all possible universe length values. Note that just because a universe length is possible does not mean it will ever occur. For instance, the lowest possible universe length would mean that only the same ball/set of drawn number is drawn over and over again within the running window. Although this could theoretically happen, its low probability makes it impossible.\n- `parity` : Number of even numbers in the *current* draw\n- `parity-over-frame` : Number of even numbers in the draws of the *running window*\n- `last-moving-direction-of-universe-length` : Las moving direction of the `universe-length` feature. -1 means decreasing, +1 means increasing.\n\n- `move-balance-of-universe-length_latest-minus-mean` : formula: The mean of universe lengths over the running window, minus the current value of the `universe length`\n- `move-balance-of-universe-length_mean-minus-earliest` : formula: Current value of the `universe length` minus the mean of universe lengths over the running window\n- `move-balance-of-universe-length_latest-minus-mean_runningWindowX2`: same as the other one, but the mean is applied on a frame twice bigger.\n- `move-balance-of-universe-length_mean-minus-earliest_runningWindowX2`: same concept as the above feature\n\n- `universe-length-drop` : How much the universe length can at most drop in the next draw's result\n- `universe-length-increase` : How much the universe length can at most increase in the next draw's result\n- `universe-length-repetition-same` : the number of times the current `universe length` has been repeated successively\n- `greater-universe-length-than-repetition` : the number successive times we find a higher `universe length` than the current one over the running window\n- `universe-length-didfollowincrease` : how much did the universe length increase *from the previous* draw. 0 means no increase or decrease.\n\n- `mean-frequency-of-drawn-numbers` : the mean frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n- `median-frequency-of-drawn-numbers` : the median frequency of appearance of the currently drawn numbers. Appearance frequency is computed over the running window only.\n- `mean-frequency-of-drawn-numbers-over-X-draws` : same as `mean-frequency-of-drawn-numbers` but over a running window of `X` draws.\n- `median-frequency-of-drawn-numbers-over-X-draws` : same as `median-frequency-of-drawn-numbers` but over a running window of `X` draws.\n\n- `mean-gap-of-drawn-numbers` : the mean appearance gap of each the drawn balls. Takes the individual gaps of each of the balls in the current result draw (i.e. the number of draws between the last time a given ball was drawn), sums them up, and then divides by the number of balls in the draw.\n- `median-gap-of-drawn-numbers` : like `mean-gap-of-drawn-numbers` but using the median instead of the mean. Note that in 1/N lottery pools (one ball pulled out of N), the two features are always equal, which is logic.\n- `mean-gap-of-drawn-numbers-bounded-at-X-draws` : where `X` is a number. Same as its counterpart `mean-gap-of-drawn-numbers` but over a running window of `X` draws.\n- `median-gap-of-drawn-numbers-bounded-at-X-draws` : similar to the previous one, but using the median.\n\n- `mean-of-4-gaps-of-each-drawn-numbers` : formula : systematically take the last 4 gaps of each of the balls in the current draw. Sum all these gaps together and take the overall mean. (i.e. average of all those gaps)\n- `median-of-4-gaps-of-each-drawn-numbers` : same formula as its paronym, but takes the median of all. For 1/N lotteries, both are equal.\n- `median-of-means-of-X-gaps-of-each-drawn-numbers` : where `X` is a number. Formula : systematically take the last X gaps of each of the balls of the current draw. Take the mean for each ball. Then sum up all the means together and take the median of them. In short, it is a median( of average( of [X-latest-gaps-of-symbol-Y] )). A median of averages may not make much sense statistically speaking. This feature is provided as is.\n\n\n\n**TARGET columns** (i.e. supervised learning)\n(i.e. ideas of what you might want to predict)\n\n- `target_universe-length-willFollowIncrease` : TARGET feature (supervised learning). Same as the `universe-length-didfollowincrease` feature but for the draw that comes in the future. (DO NOT mistake them). One goal can be to predict this value given only the other features. Was named `universe-length-willfollowincrease` in an earlier version of the dataset.\n- `target_coming-universe-length-change-in-next-draw` : same as `target_universe-length-willFollowIncrease` but indicates decreases as well as increases and stagnations. Might be better suited for a regression, but you do as you see fit.\n- `target_coming-mean-universe-length-change-in-next-2-draws` : takes the mean change of universe length in the next 2 draws instead of only one. See `target_coming-universe-length-change-in-next-draw`.\n- `target_future-1rst-value-of-universe-length-center-offset-from-center` : the next value of the feature `universe-length-offset-from-center`. You may want to try to predict that.\n- `target_coming-universe-length-center-offset-change-in-next-draw` : the relative change of the feature `universe-length-offset-from-center` that will occur in the next draw.\n- `target_coming-mean-universe-length-center-offset-change-in-next-2-draws` : same as the previous feature, but we take the mean change in the next 2 draws.\n\n\n**Special columns:**\n- `date` : date of the lottery draw\n- `draw` / `draw-result` : most recent draw of the specified date. displayed for convenience\n- `running-window-frame-length` : convenience a CONSTANT column. it is there to remind you of the base *running window* size (also called *frame length*) used to compute most features. When some features use a given multiple of the *frame length*, this is the value that gets multiplied.\n- `draw-id` : the draw identifier. It can be the date or in another form (such as reversed date `yyyymmdd`). Only displayed for convenience\n\n\n## Inspiration / Ideas of approaches\n\n- Transforming the problem into another that outputs binary values.\n   This would allow you/us to tap into the enormity of works and theorems done for _**binomial problems**_ both in statistics and probability.\n   It would allow you to compare the distribution of the randomness to much more scholarly examples (such as heads or tails) and deduce the law of probability / probability distribution / random variable behind a particular lottery.\n   &gt; If I remember correctly a maths course, there should be a **theorem / lemme** in probability that say how likely a binomial distribution is to take exaggerated values (big outliers). Regardless, there are a lot of results already, such as **Bernoulli trial** to study and compare theoretical and practical results.\n\n\n- Treating the problem as a numerical time series problem\n- Treating the problem as a signal processing problem\n- Asking different high-level questions and formulating the problem differently. For instance\n  - Trying to predict the repartition of a feature (like parity) in the next 5 draws (instead of only predicting the parity of the next draw)\n\n- Trying to identify critical points in time where several features converge towards predicting the same thing.\n   For instance, there could be a time where the parity of the last 10 drawn numbers was even. If on top of that several even numbers have had a particularly high appearance rate in a preceding big frame of time, then these two features would lean towards thinking that there is a higher chance of seeing an odd number drawn next. (Though probability independence would say that such convergence are meaningless, here we are to suppose the opposite until we face the hard truth, as in a proof by contradiction).\n   See the note about **Bernoulli trials**-related *theorem / lemme* as mentioned above.\n\n\n## Acknowledgements\n\n\n## License\n\nDataset licensed under [CC-BY](https://creativecommons.org/licenses/by/4.0/)\n\nPersonal project for generating the dataset: [Lofea](https://github.com/JeffMv/Lofea) under [CC-BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/)",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "signal processing",
    "time series analysis",
    "feature engineering"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "name": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}