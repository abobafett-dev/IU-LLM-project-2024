{
  "id": "joebeachcapital/harth-dataset",
  "id_no": 3879218,
  "datasetSlugNullable": "harth-dataset",
  "ownerUserNullable": "joebeachcapital",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "HARTH Dataset",
  "subtitleNullable": "22 subjects wearing 3-axial accelerometers for around 2h in free-living setting",
  "descriptionNullable": "The Human Activity Recognition Trondheim (HARTH) dataset is a professionally-annotated dataset containing 22 subjects wearing two 3-axial accelerometers for around 2 hours in a free-living setting. The sensors were attached to the right thigh and lower back. The professional recordings and annotations provide a promising benchmark dataset for researchers to develop innovative machine learning approaches for precise HAR in free living.\n\n**For what purpose was the dataset created?**\n\nThe dataset was created to train machine learning classifiers for human activity recognition based on professional annotations of activities in a free-living setting.\n\n**Who funded the creation of the dataset?**\n\nNTNU Helse\n\n**Additional Information**\n\nThe HARTH dataset contains recordings of 22 participants wearing two 3-axial Axivity AX3 accelerometers for around 2 hours in a free-living setting. One sensor was attached to the right front thigh and the other to the lower back. The provided sampling rate is 50Hz. Video recordings of a chest-mounted camera were used to annotate the performed activities frame-by-frame.\n\nEach subject's recordings are provided in a separate .csv file. One such .csv file contains the following columns:\n1. timestamp: date and time of recorded sample\n2. back_x: acceleration of back sensor in x-direction (down) in the unit g\n3. back_y: acceleration of back sensor in y-direction (left) in the unit g\n4. back_z: acceleration of back sensor in z-direction (forward) in the unit g\n5. thigh_x: acceleration of thigh sensor in x-direction (down) in the unit g\n6. thigh_y: acceleration of thigh sensor in y-direction (right) in the unit g\n7. thigh_z: acceleration of thigh sensor in z-direction (backward) in the unit g\n8. label: annotated activity code\n\nThe dataset contains the following annotated activities with the corresponding coding:\n1: walking\t\n2: running\t\n3: shuffling\n4: stairs (ascending)\t\n5: stairs (descending)\t\n6: standing\t\n7: sitting\t\n8: lying\t\n13: cycling (sit)\t\n14: cycling (stand)\t\n130: cycling (sit, inactive)\n140: cycling (stand, inactive)\n\n**Introductory Paper**\n\n[HARTH: A Human Activity Recognition Dataset for Machine Learning](https://archive.ics.uci.edu/dataset/779/harth)\n\nBy Aleksej Logacjov, Kerstin Bach, Atle Kongsvold, H. B\u00e5rdstu, P. Mork. 2021\n\nPublished in Italian National Conference on Sensors",
  "datasetId": 3879218,
  "datasetSlug": "harth-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "joebeachcapital",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 3516,
  "totalVotes": 25,
  "totalDownloads": 305,
  "title": "HARTH Dataset",
  "hasTitle": true,
  "subtitle": "22 subjects wearing 3-axial accelerometers for around 2h in free-living setting",
  "hasSubtitle": true,
  "description": "The Human Activity Recognition Trondheim (HARTH) dataset is a professionally-annotated dataset containing 22 subjects wearing two 3-axial accelerometers for around 2 hours in a free-living setting. The sensors were attached to the right thigh and lower back. The professional recordings and annotations provide a promising benchmark dataset for researchers to develop innovative machine learning approaches for precise HAR in free living.\n\n**For what purpose was the dataset created?**\n\nThe dataset was created to train machine learning classifiers for human activity recognition based on professional annotations of activities in a free-living setting.\n\n**Who funded the creation of the dataset?**\n\nNTNU Helse\n\n**Additional Information**\n\nThe HARTH dataset contains recordings of 22 participants wearing two 3-axial Axivity AX3 accelerometers for around 2 hours in a free-living setting. One sensor was attached to the right front thigh and the other to the lower back. The provided sampling rate is 50Hz. Video recordings of a chest-mounted camera were used to annotate the performed activities frame-by-frame.\n\nEach subject's recordings are provided in a separate .csv file. One such .csv file contains the following columns:\n1. timestamp: date and time of recorded sample\n2. back_x: acceleration of back sensor in x-direction (down) in the unit g\n3. back_y: acceleration of back sensor in y-direction (left) in the unit g\n4. back_z: acceleration of back sensor in z-direction (forward) in the unit g\n5. thigh_x: acceleration of thigh sensor in x-direction (down) in the unit g\n6. thigh_y: acceleration of thigh sensor in y-direction (right) in the unit g\n7. thigh_z: acceleration of thigh sensor in z-direction (backward) in the unit g\n8. label: annotated activity code\n\nThe dataset contains the following annotated activities with the corresponding coding:\n1: walking\t\n2: running\t\n3: shuffling\n4: stairs (ascending)\t\n5: stairs (descending)\t\n6: standing\t\n7: sitting\t\n8: lying\t\n13: cycling (sit)\t\n14: cycling (stand)\t\n130: cycling (sit, inactive)\n140: cycling (stand, inactive)\n\n**Introductory Paper**\n\n[HARTH: A Human Activity Recognition Dataset for Machine Learning](https://archive.ics.uci.edu/dataset/779/harth)\n\nBy Aleksej Logacjov, Kerstin Bach, Atle Kongsvold, H. B\u00e5rdstu, P. Mork. 2021\n\nPublished in Italian National Conference on Sensors",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "computer science",
    "time series analysis",
    "classification",
    "multiclass classification"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}