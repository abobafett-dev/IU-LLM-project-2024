{
  "id": "kapitanov/hagrid",
  "id_no": 2274665,
  "datasetSlugNullable": "hagrid",
  "ownerUserNullable": "kapitanov",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "HaGRID - HAnd Gesture Recognition Image Dataset",
  "subtitleNullable": "HAnd Gesture Recognition Image Dataset",
  "descriptionNullable": "![](https://github.com/hukenovs/hagrid/blob/master/images/hagrid.jpg?raw=true)\n\nWe introduce a large image dataset **HaGRID** (**HA**nd **G**esture **R**ecognition **I**mage **D**ataset) for hand gesture recognition (HGR) systems. You can use it for image classification or image detection tasks. Proposed dataset allows to build HGR systems, which can be used in video conferencing services (Zoom, Skype, Discord, Jazz etc.), home automation systems, the automotive sector, etc.\n\n**HaGRID** size is **716GB** and dataset contains **552,992 FullHD** (1920 \u00d7 1080) RGB images divided into **18** classes of gestures. Also, some images have `no_gesture` class if there is a second free hand in the frame. This extra class contains **123,589** samples. The data were split into training **92%**, and testing **8%** sets by subject **user-id**, with **509,323** images for train and 43,669 images for test.\n\n![](https://github.com/hukenovs/hagrid/raw/master/images/gestures.jpg)\n\nThe dataset contains **34,730** unique persons and at least this number of unique scenes. The subjects are people from 18 to 65 years old. The dataset was collected mainly indoors with considerable variation in lighting, including artificial and natural light. Besides, the dataset includes images taken in extreme conditions such as facing and backing to a window. Also, the subjects had to show gestures at a distance of 0.5 to 4 meters from the camera.\n\n## Annotations\nThe annotations consist of bounding boxes of hands with gesture labels in COCO format `[top left X position, top left Y position, width, height]`. Also, annotations have 21 `landmarks` in format `[x,y]` relative image coordinates, markups of `leading hands` (`left` of `right` for gesture hand) and `leading_conf` as confidence for `leading_hand` annotation. We provide `user_id` field that will allow you to split the train / val dataset yourself.\n```json\n\"0534147c-4548-4ab4-9a8c-f297b43e8ffb\": {\n  \"bboxes\": [\n    [0.38038597, 0.74085361, 0.08349486, 0.09142549],\n    [0.67322755, 0.37933984, 0.06350809, 0.09187757]\n  ],\n  \"landmarks\"[\n    [\n      [\n        [0.39917091, 0.74502739],\n        [0.42500172, 0.74984396],\n        ...\n      ],\n        [0.70590734, 0.46012364],\n        [0.69208878, 0.45407018],\n        ...\n    ],\n  ], \n  \"labels\": [\n    \"no_gesture\",\n    \"one\"\n  ],\n  \"leading_hand\": \"left\",\n  \"leading_conf\": 1.0,\n  \"user_id\": \"bb138d5db200f29385f...\"\n}\n```\n\n## Downloads\nWe split the train dataset into 18 archives by gestures because of the large size of data. Download and unzip them from the following links:\n\n### Trainval\n\n| Gesture                           | Size     | Gesture                                   | Size    |\n|-----------------------------------|----------|-------------------------------------------|---------|\n| [call](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_call.zip)  | 39.1 GB  | [peace](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_peace.zip)  | 38.6 GB |\n| [dislike](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_dislike.zip)  | 38.7 GB  | [peace_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_peace_inverted.zip)  | 38.6 GB |\n| [fist](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_fist.zip)  | 38.0 GB  | [rock](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_rock.zip)  | 38.9 GB |\n| [four](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_four.zip)  | 40.5 GB  | [stop](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_stop.zip)  | 38.3 GB |\n| [like](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_like.zip)  | 38.3 GB  | [stop_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_stop_inverted.zip)  | 40.2 GB |\n| [mute](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_mute.zip)  | 39.5 GB  | [three](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_three.zip)  | 39.4 GB |\n| [ok](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_ok.zip)  | 39.0 GB  | [three2](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_three2.zip)  | 38.5 GB |\n| [one](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_one.zip)  | 39.9 GB  | [two_up](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_two_up.zip)  | 41.2 GB |\n| [palm](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_palm.zip)  | 39.3 GB  | [two_up_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_two_up_inverted.zip) | 39.2 GB |\n  \n\n`train_val` **annotations**: [ann_train_val](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_train_val.zip)\n\n### Test\n\n| Test        | Archives                            | Size      |\n|-------------|-------------------------------------|-----------|\n| images      | [test](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/test.zip) | 60.4 GB   |\n| annotations | [ann_test](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_test.zip) | 27.3 MB    |\n\n### Subsample\nSubsample has 100 items per gesture.\n\n| Subsample   | Archives                                | Size      |\n|-------------|-----------------------------------------|-----------|\n| images      | [subsample](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/subsample.zip) | 2.5 GB    |\n| annotations | [ann_subsample](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip) | 1.2 \u041c\u0411  |\n\n## Models\nWe provide some pre-trained classifiers and detectors as the baseline solutions. You can download them via github repo.\n\n## Links\n- [Github](https://github.com/hukenovs/hagrid), [Mirror](https://gitlab.aicloud.sbercloud.ru/rndcv/hagrid)\n- [arXiv](https://arxiv.org/abs/2206.08219)\n- [Paperswithcode](https://paperswithcode.com/paper/hagrid-hand-gesture-recognition-image-dataset)",
  "datasetId": 2274665,
  "datasetSlug": "hagrid",
  "hasDatasetSlug": true,
  "ownerUser": "kapitanov",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 33876,
  "totalVotes": 102,
  "totalDownloads": 2766,
  "title": "HaGRID - HAnd Gesture Recognition Image Dataset",
  "hasTitle": true,
  "subtitle": "HAnd Gesture Recognition Image Dataset",
  "hasSubtitle": true,
  "description": "![](https://github.com/hukenovs/hagrid/blob/master/images/hagrid.jpg?raw=true)\n\nWe introduce a large image dataset **HaGRID** (**HA**nd **G**esture **R**ecognition **I**mage **D**ataset) for hand gesture recognition (HGR) systems. You can use it for image classification or image detection tasks. Proposed dataset allows to build HGR systems, which can be used in video conferencing services (Zoom, Skype, Discord, Jazz etc.), home automation systems, the automotive sector, etc.\n\n**HaGRID** size is **716GB** and dataset contains **552,992 FullHD** (1920 \u00d7 1080) RGB images divided into **18** classes of gestures. Also, some images have `no_gesture` class if there is a second free hand in the frame. This extra class contains **123,589** samples. The data were split into training **92%**, and testing **8%** sets by subject **user-id**, with **509,323** images for train and 43,669 images for test.\n\n![](https://github.com/hukenovs/hagrid/raw/master/images/gestures.jpg)\n\nThe dataset contains **34,730** unique persons and at least this number of unique scenes. The subjects are people from 18 to 65 years old. The dataset was collected mainly indoors with considerable variation in lighting, including artificial and natural light. Besides, the dataset includes images taken in extreme conditions such as facing and backing to a window. Also, the subjects had to show gestures at a distance of 0.5 to 4 meters from the camera.\n\n## Annotations\nThe annotations consist of bounding boxes of hands with gesture labels in COCO format `[top left X position, top left Y position, width, height]`. Also, annotations have 21 `landmarks` in format `[x,y]` relative image coordinates, markups of `leading hands` (`left` of `right` for gesture hand) and `leading_conf` as confidence for `leading_hand` annotation. We provide `user_id` field that will allow you to split the train / val dataset yourself.\n```json\n\"0534147c-4548-4ab4-9a8c-f297b43e8ffb\": {\n  \"bboxes\": [\n    [0.38038597, 0.74085361, 0.08349486, 0.09142549],\n    [0.67322755, 0.37933984, 0.06350809, 0.09187757]\n  ],\n  \"landmarks\"[\n    [\n      [\n        [0.39917091, 0.74502739],\n        [0.42500172, 0.74984396],\n        ...\n      ],\n        [0.70590734, 0.46012364],\n        [0.69208878, 0.45407018],\n        ...\n    ],\n  ], \n  \"labels\": [\n    \"no_gesture\",\n    \"one\"\n  ],\n  \"leading_hand\": \"left\",\n  \"leading_conf\": 1.0,\n  \"user_id\": \"bb138d5db200f29385f...\"\n}\n```\n\n## Downloads\nWe split the train dataset into 18 archives by gestures because of the large size of data. Download and unzip them from the following links:\n\n### Trainval\n\n| Gesture                           | Size     | Gesture                                   | Size    |\n|-----------------------------------|----------|-------------------------------------------|---------|\n| [call](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_call.zip)  | 39.1 GB  | [peace](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_peace.zip)  | 38.6 GB |\n| [dislike](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_dislike.zip)  | 38.7 GB  | [peace_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_peace_inverted.zip)  | 38.6 GB |\n| [fist](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_fist.zip)  | 38.0 GB  | [rock](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_rock.zip)  | 38.9 GB |\n| [four](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_four.zip)  | 40.5 GB  | [stop](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_stop.zip)  | 38.3 GB |\n| [like](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_like.zip)  | 38.3 GB  | [stop_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_stop_inverted.zip)  | 40.2 GB |\n| [mute](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_mute.zip)  | 39.5 GB  | [three](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_three.zip)  | 39.4 GB |\n| [ok](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_ok.zip)  | 39.0 GB  | [three2](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_three2.zip)  | 38.5 GB |\n| [one](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_one.zip)  | 39.9 GB  | [two_up](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_two_up.zip)  | 41.2 GB |\n| [palm](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_palm.zip)  | 39.3 GB  | [two_up_inverted](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/train_val_two_up_inverted.zip) | 39.2 GB |\n  \n\n`train_val` **annotations**: [ann_train_val](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_train_val.zip)\n\n### Test\n\n| Test        | Archives                            | Size      |\n|-------------|-------------------------------------|-----------|\n| images      | [test](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/test.zip) | 60.4 GB   |\n| annotations | [ann_test](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_test.zip) | 27.3 MB    |\n\n### Subsample\nSubsample has 100 items per gesture.\n\n| Subsample   | Archives                                | Size      |\n|-------------|-----------------------------------------|-----------|\n| images      | [subsample](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/subsample.zip) | 2.5 GB    |\n| annotations | [ann_subsample](https://n-usr-2uzac.s3pd12.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip) | 1.2 \u041c\u0411  |\n\n## Models\nWe provide some pre-trained classifiers and detectors as the baseline solutions. You can download them via github repo.\n\n## Links\n- [Github](https://github.com/hukenovs/hagrid), [Mirror](https://gitlab.aicloud.sbercloud.ru/rndcv/hagrid)\n- [arXiv](https://arxiv.org/abs/2206.08219)\n- [Paperswithcode](https://paperswithcode.com/paper/hagrid-hand-gesture-recognition-image-dataset)",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "computer science",
    "computer vision",
    "classification",
    "deep learning",
    "image",
    "pytorch"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-SA-4.0",
      "name": "CC-BY-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [
    {
      "username": "nagatron",
      "role": "writer"
    },
    {
      "username": "vhsgltich",
      "role": "writer"
    },
    {
      "username": "karinakvanchiani",
      "role": "writer"
    }
  ],
  "data": []
}