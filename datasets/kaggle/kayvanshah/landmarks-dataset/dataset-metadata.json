{
  "id": "kayvanshah/landmarks-dataset",
  "id_no": 3161816,
  "datasetSlugNullable": "landmarks-dataset",
  "ownerUserNullable": "kayvanshah",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "Landmarks Dataset",
  "subtitleNullable": "Classify the category and type of the landmarks",
  "descriptionNullable": "The dataset consists of images of famous (or not-so-famous) landmarks. The collection is organized into a two-level hierarchy structure. The first level is the categories for the landmarks, and the second level is the individual landmarks. There are 6 categories, and categories are:\n1. Gothic\n2. Modern\n3. Mughal\n4. Neoclassical\n5. Pagodas\n6. Pyramids\n\nFor each category, there are 5 landmarks, for a total of 30 landmarks. Each landmark has 14\nimages.\n\nTasks:\n---\n- This group project is comprised of two machine-learning tasks:\n- Category classification: predict the category names of images\n- Landmark classification: predict the landmark names of images\n\n\nThe landmarks dataset is too small to train convolutional neural networks (CNNs) from scratch.\nThe resulting network will overfit the data. Instead, use transfer learning by reusing part of a\npre-trained CNN. In transfer learning, instead of training the neural network starting from\nrandom weights, the weights for the lower parts of the network are taken from a pre-trained\nnetwork. Only the higher parts of the network will have to be learned. Chapter 14 of G\u00e9ron discusses how to apply pre-trained models for transfer learning.\n\n\nFor this group project, the only allowed pre-trained networks are EfficientNetB0 and VGG16,\nwhich are smaller CNNs. The objective of this restriction is to avoid penalizing groups that do\nnot have access to powerful machines and/or machines with GPUs. Groups are allowed to use\nGoogle Colab with GPUs to train the models, but be aware of resource usage limitations.\n\n\nData augmentation is another way to overcome the problem of small datasets.\nKeras/TensorFlow provides various image manipulation functions (hitps://www.tensorflow.org/api_docs/python/tf/image) that can be used to generate additional images. Refer to Lecture 9 slides and Chapter 14 of G\u00e9ron.\n\n\n\fYet another way to overcome the small dataset problem is experimenting with various ways of\ncombining the models for the two tasks. It is possible to train two distinct models, one for\ncategory classification and one for landmark classification. But would landmark classification\nbenefit from knowing the output of classification? Or vice versa?\n\n\nCode and Model Submission:\n---\n- The details of the submission will be provided later. We are in the process of setting up a\nVocareum site that will allow you to run your model against part of the holdout test images.\n- You are strongly encouraged to use Keras/TensorFlow.\n",
  "datasetId": 3161816,
  "datasetSlug": "landmarks-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "kayvanshah",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 1343,
  "totalVotes": 2,
  "totalDownloads": 115,
  "title": "Landmarks Dataset",
  "hasTitle": true,
  "subtitle": "Classify the category and type of the landmarks",
  "hasSubtitle": true,
  "description": "The dataset consists of images of famous (or not-so-famous) landmarks. The collection is organized into a two-level hierarchy structure. The first level is the categories for the landmarks, and the second level is the individual landmarks. There are 6 categories, and categories are:\n1. Gothic\n2. Modern\n3. Mughal\n4. Neoclassical\n5. Pagodas\n6. Pyramids\n\nFor each category, there are 5 landmarks, for a total of 30 landmarks. Each landmark has 14\nimages.\n\nTasks:\n---\n- This group project is comprised of two machine-learning tasks:\n- Category classification: predict the category names of images\n- Landmark classification: predict the landmark names of images\n\n\nThe landmarks dataset is too small to train convolutional neural networks (CNNs) from scratch.\nThe resulting network will overfit the data. Instead, use transfer learning by reusing part of a\npre-trained CNN. In transfer learning, instead of training the neural network starting from\nrandom weights, the weights for the lower parts of the network are taken from a pre-trained\nnetwork. Only the higher parts of the network will have to be learned. Chapter 14 of G\u00e9ron discusses how to apply pre-trained models for transfer learning.\n\n\nFor this group project, the only allowed pre-trained networks are EfficientNetB0 and VGG16,\nwhich are smaller CNNs. The objective of this restriction is to avoid penalizing groups that do\nnot have access to powerful machines and/or machines with GPUs. Groups are allowed to use\nGoogle Colab with GPUs to train the models, but be aware of resource usage limitations.\n\n\nData augmentation is another way to overcome the problem of small datasets.\nKeras/TensorFlow provides various image manipulation functions (hitps://www.tensorflow.org/api_docs/python/tf/image) that can be used to generate additional images. Refer to Lecture 9 slides and Chapter 14 of G\u00e9ron.\n\n\n\fYet another way to overcome the small dataset problem is experimenting with various ways of\ncombining the models for the two tasks. It is possible to train two distinct models, one for\ncategory classification and one for landmark classification. But would landmark classification\nbenefit from knowing the output of classification? Or vice versa?\n\n\nCode and Model Submission:\n---\n- The details of the submission will be provided later. We are in the process of setting up a\nVocareum site that will allow you to run your model against part of the holdout test images.\n- You are strongly encouraged to use Keras/TensorFlow.\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth science",
    "computer vision",
    "image",
    "transfer learning",
    "image classification",
    "image augmentation"
  ],
  "licenses": [
    {
      "nameNullable": "DbCL-1.0",
      "name": "DbCL-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}