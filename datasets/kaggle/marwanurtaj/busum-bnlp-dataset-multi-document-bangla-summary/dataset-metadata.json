{
  "id": "marwanurtaj/busum-bnlp-dataset-multi-document-bangla-summary",
  "id_no": 3846378,
  "datasetSlugNullable": "busum-bnlp-dataset-multi-document-bangla-summary",
  "ownerUserNullable": "marwanurtaj",
  "usabilityRatingNullable": 0.75,
  "titleNullable": "BUSUM-BNLP Dataset (Multi-Document Bangla Summary)",
  "subtitleNullable": "Multi-Document Update Summarization Corpus of Bangali news",
  "descriptionNullable": "===================================================================================== \n\n**BUSUM-BNLP DATASET**\n\n===================================================================================== \n\nA Public Dataset for Multi-Document Update Summarization Task: To Improve the artificial intelligence-centered Information Retrieval Mission \n\n===================================================================================== \n\nThis is the README file for the BUSUM-BNLP dataset. Good performance in NLP projects depends on high-quality datasets. For the multi-document update summarization task, we have researched some NLP datasets and also tried to generate a new one in Bangla. After researching various literature, we have found many English datasets like DUC2002, DUC2007, Daily-Mail, TAC dataset, and some Bangla datasets like Bangla Summarization Dataset (Prothom Alo), Bangla-Extra Sum Dataset, and BNLPC Dataset. In several papers, DUC2002 and DUC2007 were used to generate update summaries, while Daily Mail was tested for extractive and abstractive summaries. Though in Bengali summarization, there are not so many works done yet. \n\n===================================================================================== \n\nFor our dataset, we have collected some old and new-dated articles of the same news from the online websites of Prothom Alo, Kaler Kontho, BBC News, Jugantor, etc. Both old and new news can contain slimier information. We have sorted out the old and new news and developed our latest dataset for applying our multi-document update summarization task. \n\n=====================================================================================\n\n Our multi-document Bangla dataset can be used for keyword detection in multiple files related to a specific topic. You can develop various summarization models, including updated summarizers and generic summarizers, using machine learning and deep learning techniques. This dataset can serve as a valuable resource for inspiring and generating new datasets by scraping news articles and other sources. It can also aid in developing domain-specific title detection models and extracting relevant features to improve accuracy. \n\n===================================================================================== \n\nOne can follow the given methods for pre-processing the data:\n(POS) Tagging: This method will group or organise text phrases corresponding to language types such as nouns, verbs, adverbs, adjectives, etc.\nCleaning Stop Words: It will eliminate common words from a document that give no useful information. Some of the stop words are like they, there, this, were, etc.\nDiscard words or numerals containing digits: This sort of term, such as wordscapes59 or game5ts7, is tough to handle, so it's best to eliminate them or change them with an empty set and use regular expressions instead. \nErase extra white spaces: In the pre-processing, the regular expression library works well to remove extra space, which is unnecessary. \nCut-off Punctuations: There are 32 major punctuations in all that need to be considered. The string module and a regular expression can be used to replace any punctuation in the text with an empty string. \nConverting into the Same Case: If the text is in the same case throughout, a computer can easily understand the words since machines perceive lowercase and uppercase letters differently. \nRecognition of Named Entities: Keywords in the response text should be identified as item labels (i.e., individual, place, company title, etc.). \nLemmatization or stemming: Reduction of the words to their base (like run is the ground word of runs, running, runed) form will be performed by lemmatization (which matches words with a linguistic dictionary) and stemming (which removes the suffix and the prefix from the word). \nEnlarge Contractions: Words like don't, which signifies \"do not,\" and are not, which means \"are not,\" are examples of words that have been contracted. It will be easier to carry out sentence processing duties if the contraction is expanded. \nTokenization: The tokenization technique breaks text flows into tokens, which can be words, phrases, symbols, or other significant pieces of information. \n\n==================================================================================== \n\nDuring our project, we encountered a few limitations that affected our data collection and modeling efforts. Firstly, to collect news on a particular topic, we had to put in a considerable amount of effort, as newspapers do not publish news in a serial manner every day. This meant that we had to read through entire texts to select relevant news articles for our dataset. Additionally, generating human-generated summaries was not an easy task, as we had to read lengthy documents and condense them into shorter summaries. However, due to time constraints and difficulties in recognizing news articles with similar topics, we were unable to create a very large dataset. Secondly, we encountered challenges in implementing summarizing models due to limitations in RAM and GPU resources, as well as the fact that the pre-trained Transformer models take a limited amount of text as input. \n\n===================================================================================== \n\nPaper Title: Implementation of Bangla Extractive Update Summarization Task on BUSUM-BNLP-Dataset: A Multi-Document Update Summarization Corpus\nAuthors: Marwa Khanom Nurtaj, Rafsan Bari Shafin, and Md. Nahid Hasan \nDOI: 10.1109/HORA58378.2023.10156794\nPublisher: IEEE \n\n=====================================================================================",
  "datasetId": 3846378,
  "datasetSlug": "busum-bnlp-dataset-multi-document-bangla-summary",
  "hasDatasetSlug": true,
  "ownerUser": "marwanurtaj",
  "hasOwnerUser": true,
  "usabilityRating": 0.75,
  "hasUsabilityRating": true,
  "totalViews": 389,
  "totalVotes": 5,
  "totalDownloads": 36,
  "title": "BUSUM-BNLP Dataset (Multi-Document Bangla Summary)",
  "hasTitle": true,
  "subtitle": "Multi-Document Update Summarization Corpus of Bangali news",
  "hasSubtitle": true,
  "description": "===================================================================================== \n\n**BUSUM-BNLP DATASET**\n\n===================================================================================== \n\nA Public Dataset for Multi-Document Update Summarization Task: To Improve the artificial intelligence-centered Information Retrieval Mission \n\n===================================================================================== \n\nThis is the README file for the BUSUM-BNLP dataset. Good performance in NLP projects depends on high-quality datasets. For the multi-document update summarization task, we have researched some NLP datasets and also tried to generate a new one in Bangla. After researching various literature, we have found many English datasets like DUC2002, DUC2007, Daily-Mail, TAC dataset, and some Bangla datasets like Bangla Summarization Dataset (Prothom Alo), Bangla-Extra Sum Dataset, and BNLPC Dataset. In several papers, DUC2002 and DUC2007 were used to generate update summaries, while Daily Mail was tested for extractive and abstractive summaries. Though in Bengali summarization, there are not so many works done yet. \n\n===================================================================================== \n\nFor our dataset, we have collected some old and new-dated articles of the same news from the online websites of Prothom Alo, Kaler Kontho, BBC News, Jugantor, etc. Both old and new news can contain slimier information. We have sorted out the old and new news and developed our latest dataset for applying our multi-document update summarization task. \n\n=====================================================================================\n\n Our multi-document Bangla dataset can be used for keyword detection in multiple files related to a specific topic. You can develop various summarization models, including updated summarizers and generic summarizers, using machine learning and deep learning techniques. This dataset can serve as a valuable resource for inspiring and generating new datasets by scraping news articles and other sources. It can also aid in developing domain-specific title detection models and extracting relevant features to improve accuracy. \n\n===================================================================================== \n\nOne can follow the given methods for pre-processing the data:\n(POS) Tagging: This method will group or organise text phrases corresponding to language types such as nouns, verbs, adverbs, adjectives, etc.\nCleaning Stop Words: It will eliminate common words from a document that give no useful information. Some of the stop words are like they, there, this, were, etc.\nDiscard words or numerals containing digits: This sort of term, such as wordscapes59 or game5ts7, is tough to handle, so it's best to eliminate them or change them with an empty set and use regular expressions instead. \nErase extra white spaces: In the pre-processing, the regular expression library works well to remove extra space, which is unnecessary. \nCut-off Punctuations: There are 32 major punctuations in all that need to be considered. The string module and a regular expression can be used to replace any punctuation in the text with an empty string. \nConverting into the Same Case: If the text is in the same case throughout, a computer can easily understand the words since machines perceive lowercase and uppercase letters differently. \nRecognition of Named Entities: Keywords in the response text should be identified as item labels (i.e., individual, place, company title, etc.). \nLemmatization or stemming: Reduction of the words to their base (like run is the ground word of runs, running, runed) form will be performed by lemmatization (which matches words with a linguistic dictionary) and stemming (which removes the suffix and the prefix from the word). \nEnlarge Contractions: Words like don't, which signifies \"do not,\" and are not, which means \"are not,\" are examples of words that have been contracted. It will be easier to carry out sentence processing duties if the contraction is expanded. \nTokenization: The tokenization technique breaks text flows into tokens, which can be words, phrases, symbols, or other significant pieces of information. \n\n==================================================================================== \n\nDuring our project, we encountered a few limitations that affected our data collection and modeling efforts. Firstly, to collect news on a particular topic, we had to put in a considerable amount of effort, as newspapers do not publish news in a serial manner every day. This meant that we had to read through entire texts to select relevant news articles for our dataset. Additionally, generating human-generated summaries was not an easy task, as we had to read lengthy documents and condense them into shorter summaries. However, due to time constraints and difficulties in recognizing news articles with similar topics, we were unable to create a very large dataset. Secondly, we encountered challenges in implementing summarizing models due to limitations in RAM and GPU resources, as well as the fact that the pre-trained Transformer models take a limited amount of text as input. \n\n===================================================================================== \n\nPaper Title: Implementation of Bangla Extractive Update Summarization Task on BUSUM-BNLP-Dataset: A Multi-Document Update Summarization Corpus\nAuthors: Marwa Khanom Nurtaj, Rafsan Bari Shafin, and Md. Nahid Hasan \nDOI: 10.1109/HORA58378.2023.10156794\nPublisher: IEEE \n\n=====================================================================================",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "artificial intelligence",
    "nlp",
    "text",
    "summarization",
    "bengali"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}