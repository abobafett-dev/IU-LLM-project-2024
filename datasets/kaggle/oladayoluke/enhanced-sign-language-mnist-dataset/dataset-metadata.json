{
  "id": "oladayoluke/enhanced-sign-language-mnist-dataset",
  "id_no": 4991620,
  "datasetSlugNullable": "enhanced-sign-language-mnist-dataset",
  "ownerUserNullable": "oladayoluke",
  "usabilityRatingNullable": 0.7058823529411765,
  "titleNullable": "Enhanced Sign Language MNIST Dataset",
  "subtitleNullable": "Enhanced Sign Language MNIST Dataset for Hand Gesture Recognition",
  "descriptionNullable": "The Enhanced Sign Language MNIST dataset is a comprehensive collection of grayscale images representing American Sign Language (ASL) gestures. This dataset serves as an enhancement to the original Sign Language MNIST dataset, providing a more diverse and extensive set of hand gesture samples for machine learning tasks.\n\nInspired by the need for more challenging benchmarks in image-based machine learning, this dataset is consistent with the original [Sign Language MNIST dataset](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data) to acquire a self-generated dataset, resulting in a more robust and varied collection of hand gesture images. The original Sign Language MNIST dataset, available on Kaggle, provided a solid foundation with 27,455 training cases and 7,172 test cases, each representing a label (0-25) mapped to an alphabetic letter A-Z (excluding J and Z). \n\nThe Enhanced Sign Language MNIST dataset builds upon this foundation by incorporating additional images generated through a process involving various image manipulation techniques. These techniques include hand tracking using MediaPipe, cropping, grayscale conversion, and resizing, to create approximately 1400 samples of each alphabetic letter. The enhanced dataset contains 69,252 samples in total, with 55,402 samples for training and validation, and 13,850 samples for testing.\n\nThis dataset is invaluable for researchers and developers working on sign language recognition, hand gesture detection, and related computer vision tasks. It offers a challenging benchmark for evaluating the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in recognizing ASL gestures.\n\nThe dataset is divided into training and testing sets following the methodology outlined in [Oladayo's research (2024)](https://www.proquest.com/dissertations-theses/enhancing-sign-language-recognition-hand-gesture/docview/3054372234/se-2), ensuring the consistency and reproducibility of experimental setups. The experimentation framework incorporated four distinct Convolutional Neural Network (CNN) models: CNN1, CNN2, CNN3, and CNN4. Additionally, four diverse data augmentation techniques were employed, denoted as DAM1, DAM2, DAM3, and DAM4. Notably, DAM1 represents the scenario where no data augmentation is applied.\n\n\nCNN2 achieved a remarkable 99.89% validation accuracy on the enhanced test samples and 99.78% on the generated test samples. Training the model on a GPU/TPU took approximately 209 seconds (3.5 minutes), which is close to the results reported in the research report. This success underscores the effectiveness of sample generation in enhancing the model's performance, showcasing its superiority over traditional data augmentation methods.\n\nWith the Enhanced Sign Language MNIST dataset, researchers can explore new approaches to sign language recognition, develop more robust machine learning models, and ultimately contribute to the advancement of assistive technologies for the deaf and hard-of-hearing community.\n\nIf you use this code or the datasets in your research, please cite the following dissertation:\n[Oladayo Luke. (2024)](https://www.proquest.com/dissertations-theses/enhancing-sign-language-recognition-hand-gesture/docview/3054372234/se-2). Enhancing Sign Language Recognition and Hand Gesture Detection using Convolutional Neural Networks and Data Augmentation Techniques. (Doctoral dissertation, Nova Southeastern University).",
  "datasetId": 4991620,
  "datasetSlug": "enhanced-sign-language-mnist-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "oladayoluke",
  "hasOwnerUser": true,
  "usabilityRating": 0.7058823529411765,
  "hasUsabilityRating": true,
  "totalViews": 132,
  "totalVotes": 1,
  "totalDownloads": 16,
  "title": "Enhanced Sign Language MNIST Dataset",
  "hasTitle": true,
  "subtitle": "Enhanced Sign Language MNIST Dataset for Hand Gesture Recognition",
  "hasSubtitle": true,
  "description": "The Enhanced Sign Language MNIST dataset is a comprehensive collection of grayscale images representing American Sign Language (ASL) gestures. This dataset serves as an enhancement to the original Sign Language MNIST dataset, providing a more diverse and extensive set of hand gesture samples for machine learning tasks.\n\nInspired by the need for more challenging benchmarks in image-based machine learning, this dataset is consistent with the original [Sign Language MNIST dataset](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data) to acquire a self-generated dataset, resulting in a more robust and varied collection of hand gesture images. The original Sign Language MNIST dataset, available on Kaggle, provided a solid foundation with 27,455 training cases and 7,172 test cases, each representing a label (0-25) mapped to an alphabetic letter A-Z (excluding J and Z). \n\nThe Enhanced Sign Language MNIST dataset builds upon this foundation by incorporating additional images generated through a process involving various image manipulation techniques. These techniques include hand tracking using MediaPipe, cropping, grayscale conversion, and resizing, to create approximately 1400 samples of each alphabetic letter. The enhanced dataset contains 69,252 samples in total, with 55,402 samples for training and validation, and 13,850 samples for testing.\n\nThis dataset is invaluable for researchers and developers working on sign language recognition, hand gesture detection, and related computer vision tasks. It offers a challenging benchmark for evaluating the performance of machine learning models, particularly Convolutional Neural Networks (CNNs), in recognizing ASL gestures.\n\nThe dataset is divided into training and testing sets following the methodology outlined in [Oladayo's research (2024)](https://www.proquest.com/dissertations-theses/enhancing-sign-language-recognition-hand-gesture/docview/3054372234/se-2), ensuring the consistency and reproducibility of experimental setups. The experimentation framework incorporated four distinct Convolutional Neural Network (CNN) models: CNN1, CNN2, CNN3, and CNN4. Additionally, four diverse data augmentation techniques were employed, denoted as DAM1, DAM2, DAM3, and DAM4. Notably, DAM1 represents the scenario where no data augmentation is applied.\n\n\nCNN2 achieved a remarkable 99.89% validation accuracy on the enhanced test samples and 99.78% on the generated test samples. Training the model on a GPU/TPU took approximately 209 seconds (3.5 minutes), which is close to the results reported in the research report. This success underscores the effectiveness of sample generation in enhancing the model's performance, showcasing its superiority over traditional data augmentation methods.\n\nWith the Enhanced Sign Language MNIST dataset, researchers can explore new approaches to sign language recognition, develop more robust machine learning models, and ultimately contribute to the advancement of assistive technologies for the deaf and hard-of-hearing community.\n\nIf you use this code or the datasets in your research, please cite the following dissertation:\n[Oladayo Luke. (2024)](https://www.proquest.com/dissertations-theses/enhancing-sign-language-recognition-hand-gesture/docview/3054372234/se-2). Enhancing Sign Language Recognition and Hand Gesture Detection using Convolutional Neural Networks and Data Augmentation Techniques. (Doctoral dissertation, Nova Southeastern University).",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "computer science"
  ],
  "licenses": [
    {
      "nameNullable": "MIT",
      "name": "MIT",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}