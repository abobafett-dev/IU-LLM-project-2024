{
  "id": "patrickfleith/astronautics-multiple-choice-questions-and-answers",
  "id_no": 5174720,
  "datasetSlugNullable": "astronautics-multiple-choice-questions-and-answers",
  "ownerUserNullable": "patrickfleith",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Astronautics Multiple Choice Questions and Answers",
  "subtitleNullable": "AstroMCQA is designed for comparative assessment of LLMs in Astronautics domain",
  "descriptionNullable": "# AstroMCQA Dataset\n\n## Purpose and scope\n\nThe primary purpose of AstroMCQA is for application developers in the domain of space engineering to be able to comparatively assess LLM performances on the specific task of multiple-choice question-answering\n\n## Intended Usage\n\nComparative assessement of differents LLMs, Model evaluation, audit, and model selection. Assessment of different quantization levels, different prompting strategies, and assessing effectiveness of domain adaptation or domain-specific fine-tuning.\n\n## Quickstart\n\n- Explore the dataset here:  https://huggingface.co/datasets/patrickfleith/Astro-mcqa/viewer/default/train\n- Evaluate an LLM (Mistral-7b) on AstroMCQA on collab here:<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickfleith/astro-llms-notebooks/blob/main/Evaluate_an_HuggingFace_LLM_on_a_Domain_Specific_Benchmark_Dataset.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n</a>\n\n## What is AstroMCQA GOOD for?\n\nWhat is AstroMCQA good for?\nThe primary purpose of AstroMCQA is for application developers in the domain of space mission design and operations to be able to address some questions such as: which LLM to use and how does it perform in the different subdomains? It enables to benchmark different models, different size, quantization methods, prompt engineering strategies, effectiveness of fine-tuning on the specific task of multiple-choice question-answering in space engineering.\n\n## What is AstroMCQA NOT GOOD for?\n\nIt is not suitable for training / fine-tuning LLM due to the very limited size of the dataset even if it could be combined with other tasks and science dataset for meta-learning.\n\n# DATASET DESCRIPTION\n### Access\n- Manual download from Hugging face hub: https://huggingface.co/datasets/patrickfleith/Astro-mcqa\n- Or with python:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"patrickfleith/Astro-mcqa\")\n```\n\n### Structure\n200 expert-created Multiple Choice Questions and Answers, one question per row in a comma separated file. Each instance is made of the following field (column):\n- **question**: a string.\n- **propositions**: a list of string. Each item in the list is one choice. At least one of the propositions correctly answer the question, but there can be multiple correct propositions. Even all propositions can be correct.\n- **labels**: list of integer (0/1). Each element in the labels list correspond to proposition at the same position within the proposition list. A label of 0 means that the proposition is incorrect. A label of 1 means that the proposition is a correct choice to answer the question.\n- **justification**: Optional string. An optional field which may provide a justification of the answer.\n- **answerable**: A boolean, whether the question is answerable or not. At the moment, AstroMCQA only includes answerable questions.\n- **uid**: A unique identifier for the MCQA instance. May be useful for traceability in further processing tasks.\n\n### Metadata\nDataset is version controlled and commits history is available here: https://huggingface.co/datasets/patrickfleith/Astro-mcqa/commits/main\n \n### Languages\nAll instances in the dataset are in english\n \n### Size\n200 expert-created Multiple Choice Questions and Answers\n \n### Types of Questions\n- Some questions request expected generic knowledge in the field of space science and engineering.\n- Some questions require reasoning capabilities\n- Some questions require mathematical operations since a numerical result is expected (exam-style questions)\n\n### Topics Covered\nDifferent subdomains of space engineering are covered, including propulsion, operations, human spaceflight, space environment and effects, space project lifecycle, communication and link analysis, and more.\n\n# USAGE AND GUIDELINES\n#### License\nAstroMCQA \u00a9 2024 by Patrick Fleith is licensed under Creative Commons Attribution 4.0 International\n\n#### Restrictions\nNo restriction. Please provide the correct attribution following the license terms.\n \n#### Citation\nP. Fleith, AstroMCQA \u2013 Astronautics multiple choice questions and answers benchmark dataset for domain of Space Mission Engineering for LLM Evaluation, (2024). \n \n#### Update Frequency\nMay be updated based on feedbacks. If you want to become a contributor, let me know.\n \n#### Have a feedback or spot an error?\nUse the community discussion tab directly on the huggingface Astro-mcqa dataset page.\n \n#### Contact Information\nReach me here on the community tab or on LinkedIn (Patrick Fleith) with a Note.\n\n#### Current Limitations and future work\n- Only 200 multiple choice questions and answers. This makes it useless for fine-tuning purpose, although it could be integrated as part of a larger pool of datasets compiled for a larger fine-tuning.\n- While being a descent size enabling LLM evaluation, the space engineering expert time is scarce and expensive. On average it takes 8 minutes to create one MCQA example. Having more examples would be much better for robustness.\n- The dataset might be biased toward the very low number of annotators.\n- The dataset might be biased toward European Space Programs.\n- The dataset might not cover all subsystems or subdomain of astronautics although we tried to do our best covering the annotator\u2019s domains of expertise.\n- No peer-reviewing. Ideally we would like to have a Quality Control process to ensure high quality, and correctness of each example in the dataset. Given the limited resources, this is not yet possible. Feel free to come and contribute if you feel that is an issue",
  "datasetId": 5174720,
  "datasetSlug": "astronautics-multiple-choice-questions-and-answers",
  "hasDatasetSlug": true,
  "ownerUser": "patrickfleith",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 80,
  "totalVotes": 4,
  "totalDownloads": 12,
  "title": "Astronautics Multiple Choice Questions and Answers",
  "hasTitle": true,
  "subtitle": "AstroMCQA is designed for comparative assessment of LLMs in Astronautics domain",
  "hasSubtitle": true,
  "description": "# AstroMCQA Dataset\n\n## Purpose and scope\n\nThe primary purpose of AstroMCQA is for application developers in the domain of space engineering to be able to comparatively assess LLM performances on the specific task of multiple-choice question-answering\n\n## Intended Usage\n\nComparative assessement of differents LLMs, Model evaluation, audit, and model selection. Assessment of different quantization levels, different prompting strategies, and assessing effectiveness of domain adaptation or domain-specific fine-tuning.\n\n## Quickstart\n\n- Explore the dataset here:  https://huggingface.co/datasets/patrickfleith/Astro-mcqa/viewer/default/train\n- Evaluate an LLM (Mistral-7b) on AstroMCQA on collab here:<a target=\"_blank\" href=\"https://colab.research.google.com/github/patrickfleith/astro-llms-notebooks/blob/main/Evaluate_an_HuggingFace_LLM_on_a_Domain_Specific_Benchmark_Dataset.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n</a>\n\n## What is AstroMCQA GOOD for?\n\nWhat is AstroMCQA good for?\nThe primary purpose of AstroMCQA is for application developers in the domain of space mission design and operations to be able to address some questions such as: which LLM to use and how does it perform in the different subdomains? It enables to benchmark different models, different size, quantization methods, prompt engineering strategies, effectiveness of fine-tuning on the specific task of multiple-choice question-answering in space engineering.\n\n## What is AstroMCQA NOT GOOD for?\n\nIt is not suitable for training / fine-tuning LLM due to the very limited size of the dataset even if it could be combined with other tasks and science dataset for meta-learning.\n\n# DATASET DESCRIPTION\n### Access\n- Manual download from Hugging face hub: https://huggingface.co/datasets/patrickfleith/Astro-mcqa\n- Or with python:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"patrickfleith/Astro-mcqa\")\n```\n\n### Structure\n200 expert-created Multiple Choice Questions and Answers, one question per row in a comma separated file. Each instance is made of the following field (column):\n- **question**: a string.\n- **propositions**: a list of string. Each item in the list is one choice. At least one of the propositions correctly answer the question, but there can be multiple correct propositions. Even all propositions can be correct.\n- **labels**: list of integer (0/1). Each element in the labels list correspond to proposition at the same position within the proposition list. A label of 0 means that the proposition is incorrect. A label of 1 means that the proposition is a correct choice to answer the question.\n- **justification**: Optional string. An optional field which may provide a justification of the answer.\n- **answerable**: A boolean, whether the question is answerable or not. At the moment, AstroMCQA only includes answerable questions.\n- **uid**: A unique identifier for the MCQA instance. May be useful for traceability in further processing tasks.\n\n### Metadata\nDataset is version controlled and commits history is available here: https://huggingface.co/datasets/patrickfleith/Astro-mcqa/commits/main\n \n### Languages\nAll instances in the dataset are in english\n \n### Size\n200 expert-created Multiple Choice Questions and Answers\n \n### Types of Questions\n- Some questions request expected generic knowledge in the field of space science and engineering.\n- Some questions require reasoning capabilities\n- Some questions require mathematical operations since a numerical result is expected (exam-style questions)\n\n### Topics Covered\nDifferent subdomains of space engineering are covered, including propulsion, operations, human spaceflight, space environment and effects, space project lifecycle, communication and link analysis, and more.\n\n# USAGE AND GUIDELINES\n#### License\nAstroMCQA \u00a9 2024 by Patrick Fleith is licensed under Creative Commons Attribution 4.0 International\n\n#### Restrictions\nNo restriction. Please provide the correct attribution following the license terms.\n \n#### Citation\nP. Fleith, AstroMCQA \u2013 Astronautics multiple choice questions and answers benchmark dataset for domain of Space Mission Engineering for LLM Evaluation, (2024). \n \n#### Update Frequency\nMay be updated based on feedbacks. If you want to become a contributor, let me know.\n \n#### Have a feedback or spot an error?\nUse the community discussion tab directly on the huggingface Astro-mcqa dataset page.\n \n#### Contact Information\nReach me here on the community tab or on LinkedIn (Patrick Fleith) with a Note.\n\n#### Current Limitations and future work\n- Only 200 multiple choice questions and answers. This makes it useless for fine-tuning purpose, although it could be integrated as part of a larger pool of datasets compiled for a larger fine-tuning.\n- While being a descent size enabling LLM evaluation, the space engineering expert time is scarce and expensive. On average it takes 8 minutes to create one MCQA example. Having more examples would be much better for robustness.\n- The dataset might be biased toward the very low number of annotators.\n- The dataset might be biased toward European Space Programs.\n- The dataset might not cover all subsystems or subdomain of astronautics although we tried to do our best covering the annotator\u2019s domains of expertise.\n- No peer-reviewing. Ideally we would like to have a Quality Control process to ensure high quality, and correctness of each example in the dataset. Given the limited resources, this is not yet possible. Feel free to come and contribute if you feel that is an issue",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "business",
    "education",
    "science and technology"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}