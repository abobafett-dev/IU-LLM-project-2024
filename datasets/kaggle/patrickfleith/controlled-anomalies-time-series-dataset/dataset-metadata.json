{
  "id": "patrickfleith/controlled-anomalies-time-series-dataset",
  "id_no": 3739719,
  "datasetSlugNullable": "controlled-anomalies-time-series-dataset",
  "ownerUserNullable": "patrickfleith",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Controlled Anomalies Time Series (CATS) Dataset",
  "subtitleNullable": "Awesome Dataset to benchmark Anomaly Detection in Multivariate Time Series",
  "descriptionNullable": "The Controlled Anomalies Time Series (CATS) Dataset consists of commands, external stimuli, and telemetry readings of a simulated complex dynamical system with 200 injected anomalies.\n\nThe CATS Dataset exhibits a set of desirable properties that make it very suitable for benchmarking **Anomaly Detection Algorithms in Multivariate Time Series** [1]:\n\n- **Multivariate (17 variables)**including sensors reading and control signals. It simulates the operational behaviour of an arbitrary complex system including:\n  - **4 Deliberate Actuations / Control Commands sent by a simulated operator / controller**, for instance, commands of an operator to turn ON/OFF some equipment. \n  - **3 Environmental Stimuli / External Forces** acting on the system and affecting its behaviour, for instance, the wind affecting the orientation of a large ground antenna.\n  - **10 Telemetry Readings representing the observable states of the complex system by means of sensors**, for instance, a position, a temperature, a pressure, a voltage, current, humidity, velocity, acceleration, etc.\n- **5 million timestamps**. Sensors readings are at 1Hz sampling frequency.\n  - **1 million nominal observations** (the first 1 million datapoints). This is suitable to start learning the \"normal\" behaviour.\n  - **4 million** observations that include** both nominal and anomalous segments.** This is suitable to evaluate both semi-supervised approaches (novelty detection) as well as unsupervised approaches (outlier detection).\n- **200 anomalous segments**. One anomalous segment may contain several successive anomalous observations / timestamps. Only the last 4 million observations contain anomalous segments.\n  - **Contamination level of 0.038.** This means about 3.8% of the observations (rows) are anomalous.\n- **Different types of anomalies** to understand what anomaly types can be detected by different approaches. The categories are available in the dataset and in the metadata.\n- **Fine control over ground truth.** As this is a simulated system with deliberate anomaly injection, the start and end time of the anomalous behaviour is known very precisely. In contrast to real world datasets, there is no risk that the ground truth contains mislabelled segments which is often the case for real data.\n- **Suitable for root cause analysis**. In addition to the anomaly category, the time series channel in which the anomaly first developed itself is recorded and made available as part of the metadata. This can be useful to evaluate the performance of algorithm to trace back anomalies to the right root cause channel.\n- **Affected channels.** In addition to the knowledge of the root cause channel in which the anomaly first developed itself, we provide information of channels possibly affected by the anomaly. This can also be useful to evaluate the explainability of anomaly detection systems which may point out to the anomalous channels (root cause and affected).\n- **Obvious anomalies.** The simulated anomalies have been designed to be \"easy\" to be detected for human eyes (i.e., there are very large spikes or oscillations), hence also detectable for most algorithms. It makes this synthetic dataset useful for screening tasks (i.e., to eliminate algorithms that are not capable to detect those obvious anomalies). However, during**** our initial experiments, the dataset turned out to be challenging enough even for state-of-the-art anomaly detection approaches, making it suitable also for regular benchmark studies.\n- **Context provided**. Some variables can only be considered anomalous in relation to other behaviours. A typical example consists of a light and switch pair. The light being either on or off is nominal, the same goes for the switch, but having the switch on and the light off shall be considered anomalous. In the CATS dataset, users can choose (or not) to use the available context, and external stimuli, to test the usefulness of the context for detecting anomalies in this simulation.\n- **Pure signal ideal for robustness-to-noise analysis.** The simulated signals are provided without noise: while this may seem unrealistic at first, it is an advantage since users of the dataset can decide to add on top of the provided series any type of noise and choose an amplitude. This makes it well suited to test how sensitive and robust detection algorithms are against various levels of noise.\n- **No missing data.** You can drop whatever data you want to assess the impact of missing values on your detector with respect to a clean baseline.\n\n[1] Example Benchmark of Anomaly Detection in Time Series: \u201cSebastian Schmidl, Phillip Wenig, and Thorsten Papenbrock. Anomaly Detection in Time Series: A Comprehensive Evaluation. PVLDB, 15(9): 1779 - 1797, 2022. doi:10.14778/3538598.3538602\u201d\n\n**About Solenix**\n\nThe dataset provider, Solenix, is an international company providing software engineering, consulting services and software products for the space market. Solenix is a dynamic company that brings innovative technologies and concepts to the aerospace market, keeping up to date with technical advancements and actively promoting spin-in and spin-out technology activities. We combine modern solutions which complement conventional practices. We aspire to achieve maximum customer satisfaction by fostering collaboration, constructivism, and flexibility.",
  "datasetId": 3739719,
  "datasetSlug": "controlled-anomalies-time-series-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "patrickfleith",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 3541,
  "totalVotes": 14,
  "totalDownloads": 398,
  "title": "Controlled Anomalies Time Series (CATS) Dataset",
  "hasTitle": true,
  "subtitle": "Awesome Dataset to benchmark Anomaly Detection in Multivariate Time Series",
  "hasSubtitle": true,
  "description": "The Controlled Anomalies Time Series (CATS) Dataset consists of commands, external stimuli, and telemetry readings of a simulated complex dynamical system with 200 injected anomalies.\n\nThe CATS Dataset exhibits a set of desirable properties that make it very suitable for benchmarking **Anomaly Detection Algorithms in Multivariate Time Series** [1]:\n\n- **Multivariate (17 variables)**including sensors reading and control signals. It simulates the operational behaviour of an arbitrary complex system including:\n  - **4 Deliberate Actuations / Control Commands sent by a simulated operator / controller**, for instance, commands of an operator to turn ON/OFF some equipment. \n  - **3 Environmental Stimuli / External Forces** acting on the system and affecting its behaviour, for instance, the wind affecting the orientation of a large ground antenna.\n  - **10 Telemetry Readings representing the observable states of the complex system by means of sensors**, for instance, a position, a temperature, a pressure, a voltage, current, humidity, velocity, acceleration, etc.\n- **5 million timestamps**. Sensors readings are at 1Hz sampling frequency.\n  - **1 million nominal observations** (the first 1 million datapoints). This is suitable to start learning the \"normal\" behaviour.\n  - **4 million** observations that include** both nominal and anomalous segments.** This is suitable to evaluate both semi-supervised approaches (novelty detection) as well as unsupervised approaches (outlier detection).\n- **200 anomalous segments**. One anomalous segment may contain several successive anomalous observations / timestamps. Only the last 4 million observations contain anomalous segments.\n  - **Contamination level of 0.038.** This means about 3.8% of the observations (rows) are anomalous.\n- **Different types of anomalies** to understand what anomaly types can be detected by different approaches. The categories are available in the dataset and in the metadata.\n- **Fine control over ground truth.** As this is a simulated system with deliberate anomaly injection, the start and end time of the anomalous behaviour is known very precisely. In contrast to real world datasets, there is no risk that the ground truth contains mislabelled segments which is often the case for real data.\n- **Suitable for root cause analysis**. In addition to the anomaly category, the time series channel in which the anomaly first developed itself is recorded and made available as part of the metadata. This can be useful to evaluate the performance of algorithm to trace back anomalies to the right root cause channel.\n- **Affected channels.** In addition to the knowledge of the root cause channel in which the anomaly first developed itself, we provide information of channels possibly affected by the anomaly. This can also be useful to evaluate the explainability of anomaly detection systems which may point out to the anomalous channels (root cause and affected).\n- **Obvious anomalies.** The simulated anomalies have been designed to be \"easy\" to be detected for human eyes (i.e., there are very large spikes or oscillations), hence also detectable for most algorithms. It makes this synthetic dataset useful for screening tasks (i.e., to eliminate algorithms that are not capable to detect those obvious anomalies). However, during**** our initial experiments, the dataset turned out to be challenging enough even for state-of-the-art anomaly detection approaches, making it suitable also for regular benchmark studies.\n- **Context provided**. Some variables can only be considered anomalous in relation to other behaviours. A typical example consists of a light and switch pair. The light being either on or off is nominal, the same goes for the switch, but having the switch on and the light off shall be considered anomalous. In the CATS dataset, users can choose (or not) to use the available context, and external stimuli, to test the usefulness of the context for detecting anomalies in this simulation.\n- **Pure signal ideal for robustness-to-noise analysis.** The simulated signals are provided without noise: while this may seem unrealistic at first, it is an advantage since users of the dataset can decide to add on top of the provided series any type of noise and choose an amplitude. This makes it well suited to test how sensitive and robust detection algorithms are against various levels of noise.\n- **No missing data.** You can drop whatever data you want to assess the impact of missing values on your detector with respect to a clean baseline.\n\n[1] Example Benchmark of Anomaly Detection in Time Series: \u201cSebastian Schmidl, Phillip Wenig, and Thorsten Papenbrock. Anomaly Detection in Time Series: A Comprehensive Evaluation. PVLDB, 15(9): 1779 - 1797, 2022. doi:10.14778/3538598.3538602\u201d\n\n**About Solenix**\n\nThe dataset provider, Solenix, is an international company providing software engineering, consulting services and software products for the space market. Solenix is a dynamic company that brings innovative technologies and concepts to the aerospace market, keeping up to date with technical advancements and actively promoting spin-in and spin-out technology activities. We combine modern solutions which complement conventional practices. We aspire to achieve maximum customer satisfaction by fostering collaboration, constructivism, and flexibility.",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "business",
    "engineering",
    "time series analysis",
    "outlier analysis",
    "neural networks",
    "tabular"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}