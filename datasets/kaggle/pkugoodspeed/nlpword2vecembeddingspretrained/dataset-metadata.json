{
  "id": "pkugoodspeed/nlpword2vecembeddingspretrained",
  "id_no": 11594,
  "datasetSlugNullable": "nlpword2vecembeddingspretrained",
  "ownerUserNullable": "pkugoodspeed",
  "usabilityRatingNullable": 0.8125,
  "titleNullable": "NLP-Word2Vec-Embeddings(pretrained)",
  "subtitleNullable": "Existing word2vec embeddings including glove and google news",
  "descriptionNullable": "### Context\n![word2vec][1]\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n### Content\n\nExisting Word2Vec Embeddings. \nGoogleNews-vectors-negative300.bin\nglove.6B.50d.txt\nglove.6B.100d.txt\nglove.6B.200d.txt\nglove.6B.300d.txt\n\n\n### Acknowledgements\n\nWe wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n\n\n### Inspiration\n\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?\n\n\n  [1]: https://www.adityathakker.com/wp-content/uploads/2017/06/word-embeddings-994x675.png",
  "datasetId": 11594,
  "datasetSlug": "nlpword2vecembeddingspretrained",
  "hasDatasetSlug": true,
  "ownerUser": "pkugoodspeed",
  "hasOwnerUser": true,
  "usabilityRating": 0.8125,
  "hasUsabilityRating": true,
  "totalViews": 18224,
  "totalVotes": 29,
  "totalDownloads": 1683,
  "title": "NLP-Word2Vec-Embeddings(pretrained)",
  "hasTitle": true,
  "subtitle": "Existing word2vec embeddings including glove and google news",
  "hasSubtitle": true,
  "description": "### Context\n![word2vec][1]\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.\n### Content\n\nExisting Word2Vec Embeddings. \nGoogleNews-vectors-negative300.bin\nglove.6B.50d.txt\nglove.6B.100d.txt\nglove.6B.200d.txt\nglove.6B.300d.txt\n\n\n### Acknowledgements\n\nWe wouldn't be here without the help of others. If you owe any attributions or thanks, include them here along with any citations of past research.\n\n\n### Inspiration\n\nYour data will be in front of the world's largest data science community. What questions do you want to see answered?\n\n\n  [1]: https://www.adityathakker.com/wp-content/uploads/2017/06/word-embeddings-994x675.png",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "linguistics",
    "computer science",
    "nlp"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}