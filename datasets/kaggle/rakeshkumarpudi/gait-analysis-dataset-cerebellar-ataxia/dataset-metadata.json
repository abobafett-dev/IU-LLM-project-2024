{
  "id": "rakeshkumarpudi/gait-analysis-dataset-cerebellar-ataxia",
  "id_no": 5196580,
  "datasetSlugNullable": "gait-analysis-dataset-cerebellar-ataxia",
  "ownerUserNullable": "rakeshkumarpudi",
  "usabilityRatingNullable": 0.6875,
  "titleNullable": "Gait Analysis Dataset: Cerebellar Ataxia",
  "subtitleNullable": "University Student Gait Dataset: Simulated Ataxia & Normal Walking",
  "descriptionNullable": "**Dataset Creation and Description**\nThe dataset comprises gait videos from a total of 100 university students (50 males and 50 females) with a mean age of 20 \u00b1 3 years. All participants provided informed consent for their involvement in the study and the use of their de-identified motion data for research purposes.\n\n**Data Collection Protocol**\n\nEach participant was instructed to perform two distinct walking trials:\n\n1.  **Normal Walking:** Participants walked at their natural pace for approximately 30 seconds along a designated path in a controlled environment.\n2.  **Ataxic Gait Simulation:** Participants were asked to mimic an ataxic gait pattern, characterized by uncoordinated and unsteady movements, for approximately 30 seconds.\n\n**Video Acquisition and Preprocessing**\n\nAll gait videos were recorded using a high-definition camera positioned to capture the full-body motion of the participants. The recording environment was carefully controlled to ensure consistent lighting conditions and minimize background distractions.  A total of 200 gait videos (100 normal and 100 ataxic) were collected and served as input for subsequent analysis.\n\n**Joint Coordinate Extraction**\n\nMediaPipe Pose, a state-of-the-art pose estimation model, was employed to extract 3D joint coordinates from the raw video data. The model identified and tracked the positions of 12 key body joints (left and right shoulder, elbow, wrist, hip, knee, and ankle) in each frame of the videos. These coordinates formed the basis for further feature engineering.\n\n**Feature Engineering**\n\nKinematic and spatio-temporal features were derived from the extracted joint coordinates. Kinematic features included joint angles (shoulder, hip, knee), calculated using the inverse cosine of the dot product of normalized limb segment vectors. Spatio-temporal features encompassed step length, step width, feet clearance, and left/right stride speed, calculated using the 3D ankle coordinates and timestamps.\n\n**Data Augmentation**\n\nTo enhance dataset diversity and model robustness, data augmentation techniques were applied. Limb permutations were performed by swapping the lengths of corresponding left and right limbs, resulting in a wider range of gait variations. Additionally, random noise was introduced to the augmented limb lengths to simulate natural movement variability.\n\n**Data Splitting**\n\nThe final dataset, comprising both original and augmented samples, was divided into training, validation, and test sets. The training set was used to train the CNN model, the validation set was used for hyperparameter tuning and model selection, and the test set was reserved for the final evaluation of the model's performance.\n",
  "datasetId": 5196580,
  "datasetSlug": "gait-analysis-dataset-cerebellar-ataxia",
  "hasDatasetSlug": true,
  "ownerUser": "rakeshkumarpudi",
  "hasOwnerUser": true,
  "usabilityRating": 0.6875,
  "hasUsabilityRating": true,
  "totalViews": 73,
  "totalVotes": 1,
  "totalDownloads": 6,
  "title": "Gait Analysis Dataset: Cerebellar Ataxia",
  "hasTitle": true,
  "subtitle": "University Student Gait Dataset: Simulated Ataxia & Normal Walking",
  "hasSubtitle": true,
  "description": "**Dataset Creation and Description**\nThe dataset comprises gait videos from a total of 100 university students (50 males and 50 females) with a mean age of 20 \u00b1 3 years. All participants provided informed consent for their involvement in the study and the use of their de-identified motion data for research purposes.\n\n**Data Collection Protocol**\n\nEach participant was instructed to perform two distinct walking trials:\n\n1.  **Normal Walking:** Participants walked at their natural pace for approximately 30 seconds along a designated path in a controlled environment.\n2.  **Ataxic Gait Simulation:** Participants were asked to mimic an ataxic gait pattern, characterized by uncoordinated and unsteady movements, for approximately 30 seconds.\n\n**Video Acquisition and Preprocessing**\n\nAll gait videos were recorded using a high-definition camera positioned to capture the full-body motion of the participants. The recording environment was carefully controlled to ensure consistent lighting conditions and minimize background distractions.  A total of 200 gait videos (100 normal and 100 ataxic) were collected and served as input for subsequent analysis.\n\n**Joint Coordinate Extraction**\n\nMediaPipe Pose, a state-of-the-art pose estimation model, was employed to extract 3D joint coordinates from the raw video data. The model identified and tracked the positions of 12 key body joints (left and right shoulder, elbow, wrist, hip, knee, and ankle) in each frame of the videos. These coordinates formed the basis for further feature engineering.\n\n**Feature Engineering**\n\nKinematic and spatio-temporal features were derived from the extracted joint coordinates. Kinematic features included joint angles (shoulder, hip, knee), calculated using the inverse cosine of the dot product of normalized limb segment vectors. Spatio-temporal features encompassed step length, step width, feet clearance, and left/right stride speed, calculated using the 3D ankle coordinates and timestamps.\n\n**Data Augmentation**\n\nTo enhance dataset diversity and model robustness, data augmentation techniques were applied. Limb permutations were performed by swapping the lengths of corresponding left and right limbs, resulting in a wider range of gait variations. Additionally, random noise was introduced to the augmented limb lengths to simulate natural movement variability.\n\n**Data Splitting**\n\nThe final dataset, comprising both original and augmented samples, was divided into training, validation, and test sets. The training set was used to train the CNN model, the validation set was used for hyperparameter tuning and model selection, and the test set was reserved for the final evaluation of the model's performance.\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "mental health",
    "healthcare",
    "earth and nature",
    "health",
    "medicine",
    "cnn",
    "tabular"
  ],
  "licenses": [
    {
      "nameNullable": "MIT",
      "name": "MIT",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}