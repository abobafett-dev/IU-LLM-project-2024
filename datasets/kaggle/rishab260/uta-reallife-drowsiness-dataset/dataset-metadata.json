{
  "id": "rishab260/uta-reallife-drowsiness-dataset",
  "id_no": 2839089,
  "datasetSlugNullable": "uta-reallife-drowsiness-dataset",
  "ownerUserNullable": "rishab260",
  "usabilityRatingNullable": 0.75,
  "titleNullable": "UTA Real-Life Drowsiness Dataset",
  "subtitleNullable": "The RLDD dataset consists of around 30 hours of RGB videos.",
  "descriptionNullable": "****Description :****\n\nThe University of Texas at Arlington Real-Life Drowsiness Dataset (UTA-RLDD) was created for the task of multi-stage drowsiness detection, targeting not only extreme and easily visible cases, but also subtle cases when subtle micro-expressions are the discriminative factors. Detection of these subtle cases can be important for detecting drowsiness at an early stage, so as to activate drowsiness prevention mechanisms. Subtle micro-expressions of drowsiness have physiological and instinctive sources, so it can be difficult for actors who pretend to be drowsy to realistically simulate such expressions. Our UTA-RLDD dataset is the largest to date realistic drowsiness dataset.\n\nThe RLDD dataset consists of around 30 hours of RGB videos of 60 healthy participants. For each participant we obtained one video for each of three different classes: alertness, low vigilance, and drowsiness, for a total of 180 videos. Subjects were undergraduate or graduate students and staff members who took part voluntarily or upon receiving extra credit in a course. All participants were over 18 years old. There were 51 men and 9 women, from different ethnicities (10 Caucasian, 5 non-white Hispanic, 30 IndoAryan and Dravidian, 8 Middle Eastern, and 7 East Asian) and ages (from 20 to 59 years old with a mean of 25 and standard deviation of 6). The subjects wore glasses in 21 of the 180 videos, and had considerable facial hair in 72 out of the 180 videos. Videos were taken from roughly different angles in different real-life environments and backgrounds. Each video was self-recorded by the participant, using their cell phone or web camera. The frame rate was always less than 30 fps, which is representative of the frame rate expected of typical cameras used by the general population.\n\nEach video was self-recorded by the participant, using a cell phone or web camera of the participant. The frame rate was always less than 30 fps, which is representative of the frame rate expected of normal cameras used by the general population.\n\n****Data Collection :****\n\nSixty healthy participants took part in the data collection. Subjects were instructed to take three videos of themselves by their phone/web camera (of any model or type) in three different drowsiness states, based on the KSS table(Table 1), for around 10 minutes each, and upload the videos as well as their corresponding labels on an online portal provided via a link. Subjects were given ample time (20 days) to produce the three videos. Furthermore, they were given the freedom to record the videos at home or at the university, any time they felt alert, low vigilant or drowsy while keeping the camera set up (angle and distance) roughly the same. All videos were recorded in such an angle that both eyes were visible, and the camera was placed within one arm length away from the subject. These instructions were used to make the videos similar to videos that would be obtained in a car, by phone placed in a phone holder on the dash of the car while driving. The proposed set up was to lay the phone against the display of their laptop while they are watching or reading something on their computer((Fig 1). Also, they were asked to do the same task (reading, watching or idle) in all of the three videos for consistency reasons. The three classes were explained to the participants as follows:\n\n1) Alert : One of the first three states highlighted in the KSS table in Table 1. Subjects were told that being alert meant they were completely conscious so they could easily drive for long hours.\n\n2) Low Vigilant : As stated in level 6 and 7 of Table 1, this state corresponds to subtle cases when some signs of sleepiness appear, or sleepiness is present but no effort to keep alert is required. While subjects could possibly drive in this state, driving would be discouraged.\n\n3) Drowsy : This state means that the subject needs to actively try to not fall asleep (level 8 and 9 in Table 1).\n\n![https://lh4.googleusercontent.com/_x9tO5i9vAnUef2xP3udAsRu4abi8e53U-uoy5GCRWMrIoPDKBuVkpcfId3EbcnThl-6_1bwPf4stK3Xws9HfpXW3ie8iEXA_BC0KoYq2bt30TtpJUl5E-pMUMNIw62Ekyfbf4xcX1b1qJecawFoWd2i9PWPojNqfk_V4ezsDmaDodc=w1280]\n\n@inproceedings{ghoddoosian2019realistic,\n\ntitle={A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection},\n\nauthor={Ghoddoosian, Reza and Galib, Marnim and Athitsos, Vassilis},\n\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},\n\npages={0--0},\n\nyear={2019}\n\n}\nURL to home page:\nhttps://sites.google.com/view/utarldd/home",
  "datasetId": 2839089,
  "datasetSlug": "uta-reallife-drowsiness-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "rishab260",
  "hasOwnerUser": true,
  "usabilityRating": 0.75,
  "hasUsabilityRating": true,
  "totalViews": 8183,
  "totalVotes": 15,
  "totalDownloads": 2217,
  "title": "UTA Real-Life Drowsiness Dataset",
  "hasTitle": true,
  "subtitle": "The RLDD dataset consists of around 30 hours of RGB videos.",
  "hasSubtitle": true,
  "description": "****Description :****\n\nThe University of Texas at Arlington Real-Life Drowsiness Dataset (UTA-RLDD) was created for the task of multi-stage drowsiness detection, targeting not only extreme and easily visible cases, but also subtle cases when subtle micro-expressions are the discriminative factors. Detection of these subtle cases can be important for detecting drowsiness at an early stage, so as to activate drowsiness prevention mechanisms. Subtle micro-expressions of drowsiness have physiological and instinctive sources, so it can be difficult for actors who pretend to be drowsy to realistically simulate such expressions. Our UTA-RLDD dataset is the largest to date realistic drowsiness dataset.\n\nThe RLDD dataset consists of around 30 hours of RGB videos of 60 healthy participants. For each participant we obtained one video for each of three different classes: alertness, low vigilance, and drowsiness, for a total of 180 videos. Subjects were undergraduate or graduate students and staff members who took part voluntarily or upon receiving extra credit in a course. All participants were over 18 years old. There were 51 men and 9 women, from different ethnicities (10 Caucasian, 5 non-white Hispanic, 30 IndoAryan and Dravidian, 8 Middle Eastern, and 7 East Asian) and ages (from 20 to 59 years old with a mean of 25 and standard deviation of 6). The subjects wore glasses in 21 of the 180 videos, and had considerable facial hair in 72 out of the 180 videos. Videos were taken from roughly different angles in different real-life environments and backgrounds. Each video was self-recorded by the participant, using their cell phone or web camera. The frame rate was always less than 30 fps, which is representative of the frame rate expected of typical cameras used by the general population.\n\nEach video was self-recorded by the participant, using a cell phone or web camera of the participant. The frame rate was always less than 30 fps, which is representative of the frame rate expected of normal cameras used by the general population.\n\n****Data Collection :****\n\nSixty healthy participants took part in the data collection. Subjects were instructed to take three videos of themselves by their phone/web camera (of any model or type) in three different drowsiness states, based on the KSS table(Table 1), for around 10 minutes each, and upload the videos as well as their corresponding labels on an online portal provided via a link. Subjects were given ample time (20 days) to produce the three videos. Furthermore, they were given the freedom to record the videos at home or at the university, any time they felt alert, low vigilant or drowsy while keeping the camera set up (angle and distance) roughly the same. All videos were recorded in such an angle that both eyes were visible, and the camera was placed within one arm length away from the subject. These instructions were used to make the videos similar to videos that would be obtained in a car, by phone placed in a phone holder on the dash of the car while driving. The proposed set up was to lay the phone against the display of their laptop while they are watching or reading something on their computer((Fig 1). Also, they were asked to do the same task (reading, watching or idle) in all of the three videos for consistency reasons. The three classes were explained to the participants as follows:\n\n1) Alert : One of the first three states highlighted in the KSS table in Table 1. Subjects were told that being alert meant they were completely conscious so they could easily drive for long hours.\n\n2) Low Vigilant : As stated in level 6 and 7 of Table 1, this state corresponds to subtle cases when some signs of sleepiness appear, or sleepiness is present but no effort to keep alert is required. While subjects could possibly drive in this state, driving would be discouraged.\n\n3) Drowsy : This state means that the subject needs to actively try to not fall asleep (level 8 and 9 in Table 1).\n\n![https://lh4.googleusercontent.com/_x9tO5i9vAnUef2xP3udAsRu4abi8e53U-uoy5GCRWMrIoPDKBuVkpcfId3EbcnThl-6_1bwPf4stK3Xws9HfpXW3ie8iEXA_BC0KoYq2bt30TtpJUl5E-pMUMNIw62Ekyfbf4xcX1b1qJecawFoWd2i9PWPojNqfk_V4ezsDmaDodc=w1280]\n\n@inproceedings{ghoddoosian2019realistic,\n\ntitle={A Realistic Dataset and Baseline Temporal Model for Early Drowsiness Detection},\n\nauthor={Ghoddoosian, Reza and Galib, Marnim and Athitsos, Vassilis},\n\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},\n\npages={0--0},\n\nyear={2019}\n\n}\nURL to home page:\nhttps://sites.google.com/view/utarldd/home",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "research",
    "health",
    "video",
    "video classification"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}