{
  "id": "rmisra/politifact-fact-check-dataset",
  "id_no": 2503801,
  "datasetSlugNullable": "politifact-fact-check-dataset",
  "ownerUserNullable": "rmisra",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Politifact Fact Check Dataset",
  "subtitleNullable": "High-quality dataset with 21k fact check statements between 2008 to 2022",
  "descriptionNullable": "### Context\n\nWe present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https://www.politifact.com/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n\nCover photo credits: https://blog.condati.com/5-claims-retail-marketing-analytics-fact-check\n\n### Content\nEach record consists of 8 attributes:\n\n* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n* ```statement_originator```: the person who made the statement being fact checked\n* ```statement```:  statement being fact checked\n* ```statement_date```:  the date when statement being fact checked was made\n* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n* ```factchecker```: name of the person who fact checked the claim\n* ```factcheck_date```:  date when the fact checked article was published\n* ```factcheck_analysis_link```:  link to the fact checked analysis article\n\nThe cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https://gist.github.com/rishabhmisra/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n\n### Citation\n\nIf you're using this dataset for your work, please cite the following articles:\n\nCitation in text format:\n```\n1. Misra, Rishabh and Jigyasa Grover. \"Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140/RG.2.2.29923.22566 (2022).\n```\n\nCitation in BibTex format:\n```\n@incollection{misra2022not,\n  title={Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n  author={Misra, Rishabh and Grover, Jigyasa},\n  booktitle={Deep Learning for Social Media Data Analytics},\n  pages={213--235},\n  year={2022},\n  publisher={Springer}\n}\n@dataset{misra2022politifact,\n  author = {Misra, Rishabh},\n  year = {2022},\n  month = {09},\n  pages = {},\n  title = {Politifact Fact Check Dataset},\n  doi = {10.13140/RG.2.2.29923.22566}\n}\n```\n\nPlease link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n\n### Acknowledgements\n\nThis dataset was collected from [PolitiFact](https://www.politifact.com/). \n\n### Inspiration\n\n* Can you categorize facts as true or false?  \n\n* Does the sources of false facts have a temporal pattern?\n\n* Is there a linguistic pattern in false facts?\n\n### Want to contribute your own datasets?\n\nIf you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n\n### Other datasets\nPlease also checkout the following datasets collected by me:\n\n* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n\n* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n\n* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n\n* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
  "datasetId": 2503801,
  "datasetSlug": "politifact-fact-check-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "rmisra",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 16699,
  "totalVotes": 41,
  "totalDownloads": 2971,
  "title": "Politifact Fact Check Dataset",
  "hasTitle": true,
  "subtitle": "High-quality dataset with 21k fact check statements between 2008 to 2022",
  "hasSubtitle": true,
  "description": "### Context\n\nWe present a high-quality fact-check dataset collected from a popular fact check website [*PolitiFact*](https://www.politifact.com/). The dataset contains 21,152 statements that are fact checked by experts. All the statements are categorized into one of 6 categories: true, mostly true, half true, mostly false, false, and pants on fire. Along with various details around fact checking, we also include sources where the statement appeared, which could be crucial for extracting various insights about fact checking. Furthermore, we provide links to the fact check article published on Politifact so that extra text can be extracted regarding the published fact check story if needed.\n\nCover photo credits: https://blog.condati.com/5-claims-retail-marketing-analytics-fact-check\n\n### Content\nEach record consists of 8 attributes:\n\n* ```verdict```: The verdict of fact check in one of 6 categories {```true```, ```mostly-true```, ```half-true```, ```mostly-false```, ```false```, and ```pants-fire```}\n* ```statement_originator```: the person who made the statement being fact checked\n* ```statement```:  statement being fact checked\n* ```statement_date```:  the date when statement being fact checked was made\n* ```statement_source```:  the source where the statement was made. It is one of 13 categories: {```speech```,```television```,```news```,```blog```,```social_media```,```advertisement```,```campaign```,```meeting```,```radio```,```email```,```testimony```,```statement```,```other```}\n* ```factchecker```: name of the person who fact checked the claim\n* ```factcheck_date```:  date when the fact checked article was published\n* ```factcheck_analysis_link```:  link to the fact checked analysis article\n\nThe cardinality of ```statement_source``` is reduced from ~5000+ to 13 using this logic: https://gist.github.com/rishabhmisra/5ec256d535cba3c01be45979d79d9872 in order to make the dataset more useful. We used the ```other``` category to combine infrequently appearing sources.\n\n### Citation\n\nIf you're using this dataset for your work, please cite the following articles:\n\nCitation in text format:\n```\n1. Misra, Rishabh and Jigyasa Grover. \"Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning.\" Deep Learning for Social Media Data Analytics (2022).\n2. Misra, Rishabh. \"Politifact Fact Check Dataset.\" DOI: 10.13140/RG.2.2.29923.22566 (2022).\n```\n\nCitation in BibTex format:\n```\n@incollection{misra2022not,\n  title={Do Not \u2018Fake It Till You Make It\u2019! Synopsis of Trending Fake News Detection Methodologies Using Deep Learning},\n  author={Misra, Rishabh and Grover, Jigyasa},\n  booktitle={Deep Learning for Social Media Data Analytics},\n  pages={213--235},\n  year={2022},\n  publisher={Springer}\n}\n@dataset{misra2022politifact,\n  author = {Misra, Rishabh},\n  year = {2022},\n  month = {09},\n  pages = {},\n  title = {Politifact Fact Check Dataset},\n  doi = {10.13140/RG.2.2.29923.22566}\n}\n```\n\nPlease link to [rishabhmisra.github.io/publications](https://rishabhmisra.github.io/publications/) as the source of this dataset. Thanks!\n\n### Acknowledgements\n\nThis dataset was collected from [PolitiFact](https://www.politifact.com/). \n\n### Inspiration\n\n* Can you categorize facts as true or false?  \n\n* Does the sources of false facts have a temporal pattern?\n\n* Is there a linguistic pattern in false facts?\n\n### Want to contribute your own datasets?\n\nIf you are interested in learning how to collect high-quality datasets for various ML tasks and the overall importance of data in the ML ecosystem, consider reading my book [Sculpting Data for ML](https://www.amazon.com/dp/B08RN47C5T).\n\n### Other datasets\nPlease also checkout the following datasets collected by me:\n\n* [News Headlines Dataset For Sarcasm Detection](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)\n\n* [News Category Dataset](https://www.kaggle.com/rmisra/news-category-dataset)\n\n* [Clothing Fit Dataset for Size Recommendation](https://www.kaggle.com/rmisra/clothing-fit-dataset-for-size-recommendation)\n\n* [IMDB Spoiler Dataset](https://www.kaggle.com/rmisra/imdb-spoiler-dataset)\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "linguistics",
    "software",
    "nlp",
    "classification",
    "deep learning",
    "news"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}