{
  "id": "rosenthal/citi-bike-stations",
  "id_no": 1772328,
  "datasetSlugNullable": "citi-bike-stations",
  "ownerUserNullable": "rosenthal",
  "usabilityRatingNullable": 0.8235294117647058,
  "titleNullable": "Citi Bike Stations",
  "subtitleNullable": "High-resolution bike share station time series data from 2016-2021",
  "descriptionNullable": "### Context\n\nThe New York City bikeshare, [Citi Bike](https://citibikenyc.com/homepage), has a real time, [public API](https://ride.citibikenyc.com/system-data). This API conforms to the [General Bikeshare Feed Specification](https://github.com/NABSA/gbfs/blob/master/gbfs.md). As such, this API contains information about the number of bikes and docks available at every station in NYC. \n\nSince 2016, I have been pinging the public API every 2 minutes and storing the results. This dataset contains all of these results, from 8/15/2016 - 12/8/2021. The data unfortunately comes in the form of a bunch of CSVs. I recognize that this is not the best format to read large datasets like this, but a CSV is still a pretty universal format! My suggestion would be to convert these CSVs to parquet or something similar if you plan to do lots of analysis on lots of files.\n\nOriginally, I setup an EC2 instance and pinged a legacy API ([code](https://github.com/EthanRosenthal/bike-listener)). In 2019, I switched to pinging [this API](https://gbfs.citibikenyc.com/gbfs/en/station_status.json) via a Lambda function ([code](https://github.com/EthanRosenthal/citi-bikecaster/blob/6a91a2b8bb2ad5769d3f98d2c3a6a0e2f2d92b90/station_status.py#L58)). \n\nAs part of this 2019 switch, I also started pinging the station information [API](https://gbfs.citibikenyc.com/gbfs/en/station_information.json) once per week in order to collect information about each station, such as the name, latitude and longitude. While this dataset contains columns for all of the station information, these columns are missing data between 2016 and 8/2019. It would probably be reasonable to backfill that data with the earliest info available for each station, although be warned that this is not guaranteed to be accurate.\n \n### Details\n\nIn order to reduce the individual file size, the full dataset has been bucketed by `station_id` into 50 separate files. All historical data for a given `station_id` are in the same file, and the stations are randomly distributed across the 50 files. \n\nAs previously mentioned, station information is missing for all data earlier than 8/2019. I have included a column, `missing_station_information` to indicate when this information is missing. You may wonder why I don't just create a separate station information file which can be joined to the file containing the time series. The reason is that the station information can technically change over time. When station information is provided in a given row, that information is accurate within sometime 7 days prior. This is because I pinged the station information weekly and then had to join it to the time series.\n\nThe CSV files are the result of a [CREATE TABLE AS](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html) AWS Athena query using the `TEXTFILE` format. Consequently, null values are demarcated as `\\N`. The two timestamp columns, `station_status_last_reported` and `station_information_last_updated` are in units of POXIX/UNIX time (i.e. seconds since 1970-01-01 00:00:00 UTC). The following code may be helpful to get you started loading the data as a pandas DataFrame.\n\n```python\ndef read_csv(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Read DataFrame from a CSV file ``filename`` and convert to a \n    preferred schema.\n    \"\"\"\n    df = pd.read_csv(\n        filename,\n        sep=\",\",\n        na_values=\"\\\\N\",\n        dtype={\n            \"station_id\": str,\n            # Use Pandas Int16 dtype to allow for nullable integers\n            \"num_bikes_available\": \"Int16\",\n            \"num_ebikes_available\": \"Int16\",\n            \"num_bikes_disabled\": \"Int16\",\n            \"num_docks_available\": \"Int16\",\n            \"num_docks_disabled\": \"Int16\",\n            \"is_installed\": \"Int16\",\n            \"is_renting\": \"Int16\",\n            \"is_returning\": \"Int16\",\n            \"station_status_last_reported\": \"Int64\",\n            \"station_name\": str,\n            \"lat\": float,\n            \"lon\": float,\n            \"region_id\": str,\n            \"capacity\": \"Int16\",\n            # Use pandas boolean dtype to allow for nullable booleans\n            \"has_kiosk\": \"boolean\",\n            \"station_information_last_updated\": \"Int64\",\n            \"missing_station_information\": \"boolean\"\n        },\n    )\n    # Read in timestamps as UNIX/POSIX epochs but then convert to the local\n    # bike share timezone.\n    df[\"station_status_last_reported\"] = pd.to_datetime(\n        df[\"station_status_last_reported\"], unit=\"s\", origin=\"unix\", utc=True\n    ).dt.tz_convert(\"US/Eastern\")\n\n    df[\"station_information_last_updated\"] = pd.to_datetime(\n        df[\"station_information_last_updated\"], unit=\"s\", origin=\"unix\", utc=True\n    ).dt.tz_convert(\"US/Eastern\")\n    return df\n```\n\nThe column names almost come directly from the `station_status` and `station_information` APIs. See the [GBFS schema](https://github.com/MobilityData/gbfs-json-schema) for more information.\n\n### Other caveats:\n\n- For the pre-8/2019 data, I erroneously recorded the central timestamp `station_status_last_reported` (which corresponds to the time when the station reported the number of bikes available) in the local timezone (`America/New_York`) as opposed to UTC. I have tried to correct this in the data, but things may be very slightly off near the time period in 8/2019 when I switched to using the Lambda function to record this data.\n- Sometimes the `station_status_last_reported` field is clearly wrong. It will read something close to the beginning of UNIX time. These data points should probably be thrown out.\n- There's missing data over some time period around 2017-09-05. I ran out of disk spcae on my EC2 instance and didn't realize about it for some time.\n\n### Other Resources:\n\nI wrote a series of blog posts some time ago about doing time series forecasting, and I used this dataset. If it's helpful, here are some links:\n\n- [Time Series for scikit-learn People (Part I): Where's the X Matrix?](https://www.ethanrosenthal.com/2018/01/28/time-series-for-scikit-learn-people-part1/)\n- [Time Series for scikit-learn People (Part II): Autoregressive Forecasting Pipelines](https://www.ethanrosenthal.com/2018/03/22/time-series-for-scikit-learn-people-part2/)\n- [Time Series for scikit-learn People (Part III): Horizon Optimization](https://www.ethanrosenthal.com/2019/02/18/time-series-for-scikit-learn-people-part3/)\n- [Reducing New Office Anxiety with a New Citi Bike Dataset](https://making.dia.com/reducing-new-office-anxiety-with-a-new-citi-bike-dataset-fb469fd6f5b6)",
  "datasetId": 1772328,
  "datasetSlug": "citi-bike-stations",
  "hasDatasetSlug": true,
  "ownerUser": "rosenthal",
  "hasOwnerUser": true,
  "usabilityRating": 0.8235294117647058,
  "hasUsabilityRating": true,
  "totalViews": 5392,
  "totalVotes": 11,
  "totalDownloads": 447,
  "title": "Citi Bike Stations",
  "hasTitle": true,
  "subtitle": "High-resolution bike share station time series data from 2016-2021",
  "hasSubtitle": true,
  "description": "### Context\n\nThe New York City bikeshare, [Citi Bike](https://citibikenyc.com/homepage), has a real time, [public API](https://ride.citibikenyc.com/system-data). This API conforms to the [General Bikeshare Feed Specification](https://github.com/NABSA/gbfs/blob/master/gbfs.md). As such, this API contains information about the number of bikes and docks available at every station in NYC. \n\nSince 2016, I have been pinging the public API every 2 minutes and storing the results. This dataset contains all of these results, from 8/15/2016 - 12/8/2021. The data unfortunately comes in the form of a bunch of CSVs. I recognize that this is not the best format to read large datasets like this, but a CSV is still a pretty universal format! My suggestion would be to convert these CSVs to parquet or something similar if you plan to do lots of analysis on lots of files.\n\nOriginally, I setup an EC2 instance and pinged a legacy API ([code](https://github.com/EthanRosenthal/bike-listener)). In 2019, I switched to pinging [this API](https://gbfs.citibikenyc.com/gbfs/en/station_status.json) via a Lambda function ([code](https://github.com/EthanRosenthal/citi-bikecaster/blob/6a91a2b8bb2ad5769d3f98d2c3a6a0e2f2d92b90/station_status.py#L58)). \n\nAs part of this 2019 switch, I also started pinging the station information [API](https://gbfs.citibikenyc.com/gbfs/en/station_information.json) once per week in order to collect information about each station, such as the name, latitude and longitude. While this dataset contains columns for all of the station information, these columns are missing data between 2016 and 8/2019. It would probably be reasonable to backfill that data with the earliest info available for each station, although be warned that this is not guaranteed to be accurate.\n \n### Details\n\nIn order to reduce the individual file size, the full dataset has been bucketed by `station_id` into 50 separate files. All historical data for a given `station_id` are in the same file, and the stations are randomly distributed across the 50 files. \n\nAs previously mentioned, station information is missing for all data earlier than 8/2019. I have included a column, `missing_station_information` to indicate when this information is missing. You may wonder why I don't just create a separate station information file which can be joined to the file containing the time series. The reason is that the station information can technically change over time. When station information is provided in a given row, that information is accurate within sometime 7 days prior. This is because I pinged the station information weekly and then had to join it to the time series.\n\nThe CSV files are the result of a [CREATE TABLE AS](https://docs.aws.amazon.com/athena/latest/ug/create-table-as.html) AWS Athena query using the `TEXTFILE` format. Consequently, null values are demarcated as `\\N`. The two timestamp columns, `station_status_last_reported` and `station_information_last_updated` are in units of POXIX/UNIX time (i.e. seconds since 1970-01-01 00:00:00 UTC). The following code may be helpful to get you started loading the data as a pandas DataFrame.\n\n```python\ndef read_csv(filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Read DataFrame from a CSV file ``filename`` and convert to a \n    preferred schema.\n    \"\"\"\n    df = pd.read_csv(\n        filename,\n        sep=\",\",\n        na_values=\"\\\\N\",\n        dtype={\n            \"station_id\": str,\n            # Use Pandas Int16 dtype to allow for nullable integers\n            \"num_bikes_available\": \"Int16\",\n            \"num_ebikes_available\": \"Int16\",\n            \"num_bikes_disabled\": \"Int16\",\n            \"num_docks_available\": \"Int16\",\n            \"num_docks_disabled\": \"Int16\",\n            \"is_installed\": \"Int16\",\n            \"is_renting\": \"Int16\",\n            \"is_returning\": \"Int16\",\n            \"station_status_last_reported\": \"Int64\",\n            \"station_name\": str,\n            \"lat\": float,\n            \"lon\": float,\n            \"region_id\": str,\n            \"capacity\": \"Int16\",\n            # Use pandas boolean dtype to allow for nullable booleans\n            \"has_kiosk\": \"boolean\",\n            \"station_information_last_updated\": \"Int64\",\n            \"missing_station_information\": \"boolean\"\n        },\n    )\n    # Read in timestamps as UNIX/POSIX epochs but then convert to the local\n    # bike share timezone.\n    df[\"station_status_last_reported\"] = pd.to_datetime(\n        df[\"station_status_last_reported\"], unit=\"s\", origin=\"unix\", utc=True\n    ).dt.tz_convert(\"US/Eastern\")\n\n    df[\"station_information_last_updated\"] = pd.to_datetime(\n        df[\"station_information_last_updated\"], unit=\"s\", origin=\"unix\", utc=True\n    ).dt.tz_convert(\"US/Eastern\")\n    return df\n```\n\nThe column names almost come directly from the `station_status` and `station_information` APIs. See the [GBFS schema](https://github.com/MobilityData/gbfs-json-schema) for more information.\n\n### Other caveats:\n\n- For the pre-8/2019 data, I erroneously recorded the central timestamp `station_status_last_reported` (which corresponds to the time when the station reported the number of bikes available) in the local timezone (`America/New_York`) as opposed to UTC. I have tried to correct this in the data, but things may be very slightly off near the time period in 8/2019 when I switched to using the Lambda function to record this data.\n- Sometimes the `station_status_last_reported` field is clearly wrong. It will read something close to the beginning of UNIX time. These data points should probably be thrown out.\n- There's missing data over some time period around 2017-09-05. I ran out of disk spcae on my EC2 instance and didn't realize about it for some time.\n\n### Other Resources:\n\nI wrote a series of blog posts some time ago about doing time series forecasting, and I used this dataset. If it's helpful, here are some links:\n\n- [Time Series for scikit-learn People (Part I): Where's the X Matrix?](https://www.ethanrosenthal.com/2018/01/28/time-series-for-scikit-learn-people-part1/)\n- [Time Series for scikit-learn People (Part II): Autoregressive Forecasting Pipelines](https://www.ethanrosenthal.com/2018/03/22/time-series-for-scikit-learn-people-part2/)\n- [Time Series for scikit-learn People (Part III): Horizon Optimization](https://www.ethanrosenthal.com/2019/02/18/time-series-for-scikit-learn-people-part3/)\n- [Reducing New Office Anxiety with a New Citi Bike Dataset](https://making.dia.com/reducing-new-office-anxiety-with-a-new-citi-bike-dataset-fb469fd6f5b6)",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "transportation",
    "cycling",
    "time series analysis",
    "tabular",
    "pandas"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}