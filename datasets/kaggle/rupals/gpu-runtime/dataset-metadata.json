{
  "id": "rupals/gpu-runtime",
  "id_no": 534866,
  "datasetSlugNullable": "gpu-runtime",
  "ownerUserNullable": "rupals",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "GPU Kernel Performance Dataset",
  "subtitleNullable": "Linear and Logistic Regression by implementing gradient decent algorithm",
  "descriptionNullable": "In this assignment, we will be implementing linear and logistic regression on a given dataset. In addition, we will experiment with design and feature choices.\n\nWe will be using the SGEMM GPU kernel performance Data Set available for download at [https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance](https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)\n\n**Goal:**\nImplement a linear regression model on the dataset to predict the GPU run time. Use the average of four runs as the target variable. You are not allowed to use any available implementation of the regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). Use the sum of squared error normalized by 2*number of samples [J(\u03b20, \u03b21) = (1/2m)[\u03a3(y\u1dba(i) \u2013 y(i))2] as your cost and error measures, where m is number of samples. You should use all 14 features.\n\nAlso implement a logistic regression model as described in Part 4. Again, you are not allowed to use any available implementation of the logistic regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). You should use the logistic regression cost/error function from the class. In addition you can also use accuracy/ROC/etc.\n\n**Tasks:**\n*Part 1: *Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\n*Part 2:* Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\n*Part 3:* Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\n*Part 4:* Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\n\n**Experimentation:**\n*1. *Experiment with various parameters for linear and logistic regression (e.g. learning rate \u221d) and report on your findings as how the error/accuracy varies for train and test sets with varying these parameters. Plot the results. Report the best values of the parameters.\n*2.* Experiment with various thresholds for convergence for linear and logistic regression. Plot error results for train and test sets as a function of threshold and describe how varying the threshold affects error. Pick your best threshold and plot train and test error (in one figure) as a function of number of gradient descent iterations.\n*3.* Pick eight features randomly and retrain your models only on these ten features. Compare train and test error results for the case of using your original set of features (14) and eight random features. Report the ten randomly selected features.\n*4.* Now pick eight features that you think are best suited to predict the output, and retrain your models using these ten features. Compare to the case of using your original set of features and to the random features case. Did your choice of features provide better results than picking random features? Why? Did your choice of features provide better results than using all features? Why?\n\n**Source:**\n\nEnrique G. Paredes (egparedes '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\nRafael Ballester-Ripoll (rballester '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n\n**Citation:**\n\nIf you use this data set, please cite one or both of these:\n\n- Rafael Ballester-Ripoll, Enrique G. Paredes, Renato Pajarola.\nSobol Tensor Trains for Global Sensitivity Analysis.\nIn arXiv Computer Science / Numerical Analysis e-prints, 2017\n\n- Cedric Nugteren and Valeriu Codreanu.\nCLTune: A Generic Auto-Tuner for OpenCL Kernels.\nIn: MCSoC: 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip. IEEE, 2015",
  "datasetId": 534866,
  "datasetSlug": "gpu-runtime",
  "hasDatasetSlug": true,
  "ownerUser": "rupals",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 34554,
  "totalVotes": 47,
  "totalDownloads": 2543,
  "title": "GPU Kernel Performance Dataset",
  "hasTitle": true,
  "subtitle": "Linear and Logistic Regression by implementing gradient decent algorithm",
  "hasSubtitle": true,
  "description": "In this assignment, we will be implementing linear and logistic regression on a given dataset. In addition, we will experiment with design and feature choices.\n\nWe will be using the SGEMM GPU kernel performance Data Set available for download at [https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance](https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance)\n\n**Goal:**\nImplement a linear regression model on the dataset to predict the GPU run time. Use the average of four runs as the target variable. You are not allowed to use any available implementation of the regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). Use the sum of squared error normalized by 2*number of samples [J(\u03b20, \u03b21) = (1/2m)[\u03a3(y\u1dba(i) \u2013 y(i))2] as your cost and error measures, where m is number of samples. You should use all 14 features.\n\nAlso implement a logistic regression model as described in Part 4. Again, you are not allowed to use any available implementation of the logistic regression model. You should implement the gradient descent algorithm with batch update (all training examples used at once). You should use the logistic regression cost/error function from the class. In addition you can also use accuracy/ROC/etc.\n\n**Tasks:**\n*Part 1: *Download the dataset and partition it randomly into train and test set using a good train/test split percentage.\n*Part 2:* Design a linear regression model to model the average GPU run time. Include your regression model equation in the report.\n*Part 3:* Implement the gradient descent algorithm with batch update rule. Use the same cost function as in the class (sum of squared error). Report your initial parameter values.\n*Part 4:* Convert this problem into a binary classification problem. The target variable should have two categories. Implement logistic regression to carry out classification on this data set. Report accuracy/error metrics for train and test sets.\n\n**Experimentation:**\n*1. *Experiment with various parameters for linear and logistic regression (e.g. learning rate \u221d) and report on your findings as how the error/accuracy varies for train and test sets with varying these parameters. Plot the results. Report the best values of the parameters.\n*2.* Experiment with various thresholds for convergence for linear and logistic regression. Plot error results for train and test sets as a function of threshold and describe how varying the threshold affects error. Pick your best threshold and plot train and test error (in one figure) as a function of number of gradient descent iterations.\n*3.* Pick eight features randomly and retrain your models only on these ten features. Compare train and test error results for the case of using your original set of features (14) and eight random features. Report the ten randomly selected features.\n*4.* Now pick eight features that you think are best suited to predict the output, and retrain your models using these ten features. Compare to the case of using your original set of features and to the random features case. Did your choice of features provide better results than picking random features? Why? Did your choice of features provide better results than using all features? Why?\n\n**Source:**\n\nEnrique G. Paredes (egparedes '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\nRafael Ballester-Ripoll (rballester '@' ifi.uzh.ch). Visualization and MultiMedia Lab, Department of Informatics, University of Zurich. Zurich, 8050. Switzerland\n\n**Citation:**\n\nIf you use this data set, please cite one or both of these:\n\n- Rafael Ballester-Ripoll, Enrique G. Paredes, Renato Pajarola.\nSobol Tensor Trains for Global Sensitivity Analysis.\nIn arXiv Computer Science / Numerical Analysis e-prints, 2017\n\n- Cedric Nugteren and Valeriu Codreanu.\nCLTune: A Generic Auto-Tuner for OpenCL Kernels.\nIn: MCSoC: 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip. IEEE, 2015",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "business",
    "software",
    "logistic regression",
    "linear regression"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}