{
  "id": "sadhoss/vale-semantic-terrain-segmentation",
  "id_no": 452430,
  "datasetSlugNullable": "vale-semantic-terrain-segmentation",
  "ownerUserNullable": "sadhoss",
  "usabilityRatingNullable": 0.875,
  "titleNullable": "Vale - Semantic Terrain Segmentation",
  "subtitleNullable": "Semantic Image Segmentation Dataset for Mobile Robots.",
  "descriptionNullable": "### Context\n\nThis dataset was captured for the purpose of segmenting and classifying the terrain based on the movability constraints of three different mobile robots, see [Semantic Terrain Segmentation with an Original RGB Data Set, Targeting Elevation Differences](https://www.duo.uio.no/handle/10852/70921).   \nThe dataset aims to enable autonomous terrain segmentation and classification based on the height characteristics of the terrain.\nThe name of the dataset, *Vale*, is inspired by the capture location. Campus Do Vale, The Federal University of Rio Grande Du Sul (UFRGS), Brazil.  \n\n\n### Content\n\nThe data within is primarily aimed for use with [Deeplabv3+](https://github.com/tensorflow/models/tree/master/research/deeplab) but can be used for any semantic image segmentation purpose.\n\n#### Metadata:   \nEnvironment: Semi-urban    \nSource: DJI Mavic Pro  \nImages: 600    \nSize: 1920x1080 (RGB)  \nCamera angle: 45degrees towards the ground  \nAltitude: ~2 meters.  \nArea: Campus Do Vale UFRGS  \nTime of the day: Midday  \nCapture Date: November 20th, 2018 and May 6th, 2019  \nNaming: 5-digit name (ex. 03001.*), two first digits (03) correspond to origin-video the frame was extracted from. The three following digits (001) correspond to the image/frame number.    \n\n\n#### Classes:   \n**Classes** | **Hight characteristics** | **Color**  | **8bit code**  \n--- | ---  | --- | ---     \nNon-Traversable  | (200 -&gt; mm               | Red           | 4  \nLegged                 | (50 -&gt; 200] mm         | Orange     | 3  \nBelted/Tracked    | (20 -&gt; 50] mm           | Yellow       | 2  \nWheeled               | [0 -&gt; 20] mm             | Green        | 1  \n\n\n#### Pixel distribution with unlabelled spaces between segments\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2F96f8f122abb98ad7bb21559a344a57b0%2Fvalev2_pixel_distribution_original.png?generation=1577009300284899&alt=media)\n\n#### Pixel distribution with dilated segments \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2Ff961ea3a3638fca5a58ae89a7385310d%2Fvalev2_pixel_distribution_filled.png?generation=1577009332159170&alt=media)\n\n#### Segmentation distribution\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2F9ee51631a0c4317b824943a4684d481e%2FSegment_count_valev2.png?generation=1577008998518821&alt=media)\n\n\n#### Folder:  \n- json   \n.json with polygon outline of every class segment per frame/image  \n- mask_rgb_filled  \n.png masks with dilated segments to remove unlabeled pixels  \n- mask_rgb_from_json  \n.png masks with segments based on .json files  \n- mask_uint8_deeplab  \n.png filled masks, labeled with corresponding 8bit(number) code\n- raw_images  \n.png raw images/frames \n\n\n### Acknowledgements\n\nDataset captured by:\nSadegh Hosseinpoor, Mathias Mantelli and Diego \"kindin\" Pittol.  \n \n#### VGG Image Annotator (VIA)  \n`\n@inproceedings{dutta2019vgg,\n  author = {Dutta, Abhishek and Zisserman, Andrew},\n  title = {The {VIA} Annotation Software for Images, Audio and Video},\n  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},\n  series = {MM '19},\n  year = {2019},\n  isbn = {978-1-4503-6889-6/19/10},\n  location = {Nice, France},\n  numpages = {4},\n  url = {https://doi.org/10.1145/3343031.3350535},\n  doi = {10.1145/3343031.3350535},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n} \n`\n\n`\n@misc{dutta2016via,\n  author = \"Dutta, A. and Gupta, A. and Zissermann, A.\",\n  title = \"{VGG} Image Annotator ({VIA})\",\n  year = \"2016\",  \n  howpublished = \"http://www.robots.ox.ac.uk/~vgg/software/via/\",  \n  note = \"Version: 1.0.6, Accessed: 18/02/2019\" \n}\n`\n\n\n#### Deeplabv3+  \n`\n@inproceedings{deeplabv3plus2018,\n  title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},\n  author={Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},\n  booktitle={ECCV},\n  year={2018}\n}\n`\n\n\n### Inspiration\n\nWe hope this will be of use for the machine vision community and push for further development of the field!",
  "datasetId": 452430,
  "datasetSlug": "vale-semantic-terrain-segmentation",
  "hasDatasetSlug": true,
  "ownerUser": "sadhoss",
  "hasOwnerUser": true,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 6968,
  "totalVotes": 8,
  "totalDownloads": 215,
  "title": "Vale - Semantic Terrain Segmentation",
  "hasTitle": true,
  "subtitle": "Semantic Image Segmentation Dataset for Mobile Robots.",
  "hasSubtitle": true,
  "description": "### Context\n\nThis dataset was captured for the purpose of segmenting and classifying the terrain based on the movability constraints of three different mobile robots, see [Semantic Terrain Segmentation with an Original RGB Data Set, Targeting Elevation Differences](https://www.duo.uio.no/handle/10852/70921).   \nThe dataset aims to enable autonomous terrain segmentation and classification based on the height characteristics of the terrain.\nThe name of the dataset, *Vale*, is inspired by the capture location. Campus Do Vale, The Federal University of Rio Grande Du Sul (UFRGS), Brazil.  \n\n\n### Content\n\nThe data within is primarily aimed for use with [Deeplabv3+](https://github.com/tensorflow/models/tree/master/research/deeplab) but can be used for any semantic image segmentation purpose.\n\n#### Metadata:   \nEnvironment: Semi-urban    \nSource: DJI Mavic Pro  \nImages: 600    \nSize: 1920x1080 (RGB)  \nCamera angle: 45degrees towards the ground  \nAltitude: ~2 meters.  \nArea: Campus Do Vale UFRGS  \nTime of the day: Midday  \nCapture Date: November 20th, 2018 and May 6th, 2019  \nNaming: 5-digit name (ex. 03001.*), two first digits (03) correspond to origin-video the frame was extracted from. The three following digits (001) correspond to the image/frame number.    \n\n\n#### Classes:   \n**Classes** | **Hight characteristics** | **Color**  | **8bit code**  \n--- | ---  | --- | ---     \nNon-Traversable  | (200 -&gt; mm               | Red           | 4  \nLegged                 | (50 -&gt; 200] mm         | Orange     | 3  \nBelted/Tracked    | (20 -&gt; 50] mm           | Yellow       | 2  \nWheeled               | [0 -&gt; 20] mm             | Green        | 1  \n\n\n#### Pixel distribution with unlabelled spaces between segments\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2F96f8f122abb98ad7bb21559a344a57b0%2Fvalev2_pixel_distribution_original.png?generation=1577009300284899&alt=media)\n\n#### Pixel distribution with dilated segments \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2Ff961ea3a3638fca5a58ae89a7385310d%2Fvalev2_pixel_distribution_filled.png?generation=1577009332159170&alt=media)\n\n#### Segmentation distribution\n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3695042%2F9ee51631a0c4317b824943a4684d481e%2FSegment_count_valev2.png?generation=1577008998518821&alt=media)\n\n\n#### Folder:  \n- json   \n.json with polygon outline of every class segment per frame/image  \n- mask_rgb_filled  \n.png masks with dilated segments to remove unlabeled pixels  \n- mask_rgb_from_json  \n.png masks with segments based on .json files  \n- mask_uint8_deeplab  \n.png filled masks, labeled with corresponding 8bit(number) code\n- raw_images  \n.png raw images/frames \n\n\n### Acknowledgements\n\nDataset captured by:\nSadegh Hosseinpoor, Mathias Mantelli and Diego \"kindin\" Pittol.  \n \n#### VGG Image Annotator (VIA)  \n`\n@inproceedings{dutta2019vgg,\n  author = {Dutta, Abhishek and Zisserman, Andrew},\n  title = {The {VIA} Annotation Software for Images, Audio and Video},\n  booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},\n  series = {MM '19},\n  year = {2019},\n  isbn = {978-1-4503-6889-6/19/10},\n  location = {Nice, France},\n  numpages = {4},\n  url = {https://doi.org/10.1145/3343031.3350535},\n  doi = {10.1145/3343031.3350535},\n  publisher = {ACM},\n  address = {New York, NY, USA},\n} \n`\n\n`\n@misc{dutta2016via,\n  author = \"Dutta, A. and Gupta, A. and Zissermann, A.\",\n  title = \"{VGG} Image Annotator ({VIA})\",\n  year = \"2016\",  \n  howpublished = \"http://www.robots.ox.ac.uk/~vgg/software/via/\",  \n  note = \"Version: 1.0.6, Accessed: 18/02/2019\" \n}\n`\n\n\n#### Deeplabv3+  \n`\n@inproceedings{deeplabv3plus2018,\n  title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},\n  author={Liang-Chieh Chen and Yukun Zhu and George Papandreou and Florian Schroff and Hartwig Adam},\n  booktitle={ECCV},\n  year={2018}\n}\n`\n\n\n### Inspiration\n\nWe hope this will be of use for the machine vision community and push for further development of the field!",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "robotics",
    "classification",
    "deep learning",
    "reinforcement learning"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}