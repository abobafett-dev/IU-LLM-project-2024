{
  "id": "stevenhans/depression-and-anxiety-in-twitter-id",
  "id_no": 1381462,
  "datasetSlugNullable": "depression-and-anxiety-in-twitter-id",
  "ownerUserNullable": "stevenhans",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Depression and Anxiety in Twitter (ID)",
  "subtitleNullable": "Indonesian tweet entries potentially containing depression or anxiety behavior",
  "descriptionNullable": "### Context\n\nWe create and publish this dataset as a part of the [Bangkit](https://g.co/bangkit) capstone project. The creation of this dataset is based on [this paper](https://arxiv.org/abs/2011.05249v1) by David Owen, Jose Camacho Collados, and Luis Espinosa-Anke.\n\nUsing bidirectional LSTM, our model was able to predict with 95% accuracy, 75% precision, and 79% recall (F1 score 0.766) when evaluated with the random dataset.\n\n### Content\n\nThere are three files in this dataset:\n1. datd_train.csv\n2. datd_test.csv\n3. datd_rand.csv\n\nWe trained our model with `datd_train` and `datd_test`, and evaluated the final result on `datd_rand`. `datd_rand` contains some positive entries on `datd_test`, combined with random tweets that do not contain *depres*, *cemas*, or *gelisah*. These random tweets are all labeled as negative.\n\n### Acknowledgements\n\nThanks to:\n* Lukas Purba Wisesa\n* Dzaky M. Raffy Rianto\n\nfor helping with the dataset annotation.\n\n### Inspiration\n\nWe encountered some problems annotating, mainly because we don't know where to draw the line of ambiguity. Currently, we use the same method as the paper to calculate the measure of agreement between annotators, and we found that our Krippendorff's alpha is roughly 0.6. This may affect model performance and we're not sure if we should improve the model or leave it to the experts to figure out if the tweet is actually \"positive\".\n\n* Are there better guidelines for annotating? \n* What is considered a \"good\" result?\n* How far can we push the model with the current dataset?\n* Assuming we have good guidelines for annotating, how much better will the model perform if we add, say, 10k, 20k, 50k, 100k more entires?",
  "datasetId": 1381462,
  "datasetSlug": "depression-and-anxiety-in-twitter-id",
  "hasDatasetSlug": true,
  "ownerUser": "stevenhans",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 11903,
  "totalVotes": 19,
  "totalDownloads": 1056,
  "title": "Depression and Anxiety in Twitter (ID)",
  "hasTitle": true,
  "subtitle": "Indonesian tweet entries potentially containing depression or anxiety behavior",
  "hasSubtitle": true,
  "description": "### Context\n\nWe create and publish this dataset as a part of the [Bangkit](https://g.co/bangkit) capstone project. The creation of this dataset is based on [this paper](https://arxiv.org/abs/2011.05249v1) by David Owen, Jose Camacho Collados, and Luis Espinosa-Anke.\n\nUsing bidirectional LSTM, our model was able to predict with 95% accuracy, 75% precision, and 79% recall (F1 score 0.766) when evaluated with the random dataset.\n\n### Content\n\nThere are three files in this dataset:\n1. datd_train.csv\n2. datd_test.csv\n3. datd_rand.csv\n\nWe trained our model with `datd_train` and `datd_test`, and evaluated the final result on `datd_rand`. `datd_rand` contains some positive entries on `datd_test`, combined with random tweets that do not contain *depres*, *cemas*, or *gelisah*. These random tweets are all labeled as negative.\n\n### Acknowledgements\n\nThanks to:\n* Lukas Purba Wisesa\n* Dzaky M. Raffy Rianto\n\nfor helping with the dataset annotation.\n\n### Inspiration\n\nWe encountered some problems annotating, mainly because we don't know where to draw the line of ambiguity. Currently, we use the same method as the paper to calculate the measure of agreement between annotators, and we found that our Krippendorff's alpha is roughly 0.6. This may affect model performance and we're not sure if we should improve the model or leave it to the experts to figure out if the tweet is actually \"positive\".\n\n* Are there better guidelines for annotating? \n* What is considered a \"good\" result?\n* How far can we push the model with the current dataset?\n* Assuming we have good guidelines for annotating, how much better will the model perform if we add, say, 10k, 20k, 50k, 100k more entires?",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "mental health",
    "psychology",
    "nlp",
    "text",
    "binary classification"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "name": "Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}