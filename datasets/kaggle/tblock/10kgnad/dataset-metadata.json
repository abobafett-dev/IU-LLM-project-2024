{
  "id": "tblock/10kgnad",
  "id_no": 113569,
  "datasetSlugNullable": "10kgnad",
  "ownerUserNullable": "tblock",
  "usabilityRatingNullable": 0.8235294117647058,
  "titleNullable": "Ten Thousand German News Articles Dataset",
  "subtitleNullable": "10kGNAD for Topic Classification",
  "descriptionNullable": "(see [https://tblock.github.io/10kGNAD/][1] for the original dataset page)\n\n\nThis page introduces the 10k German News Articles Dataset (10kGNAD) german topic classification dataset. \nThe 10kGNAD is based on the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/) and avalaible under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/). You can download the dataset [here](https://github.com/tblock/10kGNAD).\n\n\n## Why a German dataset? \n\nEnglish text classification datasets are common.\nExamples are the big AG News, the class-rich 20 Newsgroups and the large-scale DBpedia ontology datasets for topic classification and for example the commonly used IMDb and Yelp datasets for sentiment analysis.\nNon-english datasets, especially German datasets, are less common.\nThere is a [collection](https://sites.google.com/site/iggsahome/downloads) of sentiment analysis datasets assembled by the Interest Group on German Sentiment Analysis. \nHowever, to my knowlege, no german topic classification dataset is avaliable to the public.  \n\nDue to grammatical differences between the English and the German language, a classifyer might be effective on a English dataset, but not as effectiv on a German dataset.\nThe German language has a higher inflection and long compound words are quite common compared to the English language. \nOne would need to evaluate a classifyer on multiple German datasets to get a sense of it's effectivness.\n\n## The dataset \n\nThe 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset.\nIt consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.\nThese articles are a till now unused part of the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/).\n\nIn the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/) each article has a topic path. For example `Newsroom/Wirtschaft/Wirtschaftpolitik/Finanzmaerkte/Griechenlandkrise`.\nThe 10kGNAD uses the second part of the topic path, here `Wirtschaft`, as class label.\nIn result the dataset can be used for multi-class classification.\n\nI created and used this dataset in my thesis to train and evaluate four text classifyers on the German language.\nBy publishing the dataset I hope to support the advancement of tools and models for the German language.\nAdditionally this dataset can be used as a benchmark dataset for german topic classification.  \n\n\n### Numbers and statistics\n\nAs in most real-world datasets the class distribution of the 10kGNAD is not balanced.\nThe biggest class *Web* consists of 1678, while the smalles class *Kultur* contains only 539 articles.\nHowever articles from the *Web* class have on average the fewest words, while artilces from the culture class have the second most words.\n\n\n### Splitting into train and test\n\nI propose a stratifyed split of 10% for testing and the remaining articles for training.\nTo use the dataset as a benchmark dataset, please used the `train.csv` and `test.csv` files located in the project root.\n\n## Code\n\nPython scripts to extract the articles and split them into a train- and a testset avaliable in the [code directory](https://github.com/tblock/10kGNAD/tree/master/code) of this project.\nMake sure to install the [requirements](https://github.com/tblock/10kGNAD/blob/master/requirements.txt).\nThe original `corpus.sqlite3` is required to extract the articles (download [here (compressed)](https://github.com/OFAI/million-post-corpus/releases/download/v1.0.0/million_post_corpus.tar.bz2) or [here (uncompressed)](https://github.com/tblock/10kGNAD/releases/download/v1.0/corpus.sqlite3)).\n\n\n\n## License\n[![Creative Commons License](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\n\nThis dataset is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\nPlease consider citing the [authors of the One Million Post Corpus](https://ofai.github.io/million-post-corpus/#citation) if you use the dataset. \n\n\n  [1]: https://tblock.github.io/10kGNAD/",
  "datasetId": 113569,
  "datasetSlug": "10kgnad",
  "hasDatasetSlug": true,
  "ownerUser": "tblock",
  "hasOwnerUser": true,
  "usabilityRating": 0.8235294117647058,
  "hasUsabilityRating": true,
  "totalViews": 18044,
  "totalVotes": 37,
  "totalDownloads": 1064,
  "title": "Ten Thousand German News Articles Dataset",
  "hasTitle": true,
  "subtitle": "10kGNAD for Topic Classification",
  "hasSubtitle": true,
  "description": "(see [https://tblock.github.io/10kGNAD/][1] for the original dataset page)\n\n\nThis page introduces the 10k German News Articles Dataset (10kGNAD) german topic classification dataset. \nThe 10kGNAD is based on the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/) and avalaible under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/). You can download the dataset [here](https://github.com/tblock/10kGNAD).\n\n\n## Why a German dataset? \n\nEnglish text classification datasets are common.\nExamples are the big AG News, the class-rich 20 Newsgroups and the large-scale DBpedia ontology datasets for topic classification and for example the commonly used IMDb and Yelp datasets for sentiment analysis.\nNon-english datasets, especially German datasets, are less common.\nThere is a [collection](https://sites.google.com/site/iggsahome/downloads) of sentiment analysis datasets assembled by the Interest Group on German Sentiment Analysis. \nHowever, to my knowlege, no german topic classification dataset is avaliable to the public.  \n\nDue to grammatical differences between the English and the German language, a classifyer might be effective on a English dataset, but not as effectiv on a German dataset.\nThe German language has a higher inflection and long compound words are quite common compared to the English language. \nOne would need to evaluate a classifyer on multiple German datasets to get a sense of it's effectivness.\n\n## The dataset \n\nThe 10kGNAD dataset is intended to solve part of this problem as the first german topic classification dataset.\nIt consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.\nThese articles are a till now unused part of the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/).\n\nIn the [One Million Posts Corpus](https://ofai.github.io/million-post-corpus/) each article has a topic path. For example `Newsroom/Wirtschaft/Wirtschaftpolitik/Finanzmaerkte/Griechenlandkrise`.\nThe 10kGNAD uses the second part of the topic path, here `Wirtschaft`, as class label.\nIn result the dataset can be used for multi-class classification.\n\nI created and used this dataset in my thesis to train and evaluate four text classifyers on the German language.\nBy publishing the dataset I hope to support the advancement of tools and models for the German language.\nAdditionally this dataset can be used as a benchmark dataset for german topic classification.  \n\n\n### Numbers and statistics\n\nAs in most real-world datasets the class distribution of the 10kGNAD is not balanced.\nThe biggest class *Web* consists of 1678, while the smalles class *Kultur* contains only 539 articles.\nHowever articles from the *Web* class have on average the fewest words, while artilces from the culture class have the second most words.\n\n\n### Splitting into train and test\n\nI propose a stratifyed split of 10% for testing and the remaining articles for training.\nTo use the dataset as a benchmark dataset, please used the `train.csv` and `test.csv` files located in the project root.\n\n## Code\n\nPython scripts to extract the articles and split them into a train- and a testset avaliable in the [code directory](https://github.com/tblock/10kGNAD/tree/master/code) of this project.\nMake sure to install the [requirements](https://github.com/tblock/10kGNAD/blob/master/requirements.txt).\nThe original `corpus.sqlite3` is required to extract the articles (download [here (compressed)](https://github.com/OFAI/million-post-corpus/releases/download/v1.0.0/million_post_corpus.tar.bz2) or [here (uncompressed)](https://github.com/tblock/10kGNAD/releases/download/v1.0/corpus.sqlite3)).\n\n\n\n## License\n[![Creative Commons License](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\n\nThis dataset is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\nPlease consider citing the [authors of the One Million Post Corpus](https://ofai.github.io/million-post-corpus/#citation) if you use the dataset. \n\n\n  [1]: https://tblock.github.io/10kGNAD/",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "computer science",
    "programming",
    "nlp",
    "classification",
    "news"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-NC-SA-4.0",
      "name": "CC-BY-NC-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}