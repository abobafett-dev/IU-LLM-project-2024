{
  "id": "thedevastator/ai-based-job-site-matching",
  "id_no": 2888258,
  "datasetSlugNullable": "ai-based-job-site-matching",
  "ownerUserNullable": "thedevastator",
  "usabilityRatingNullable": 0.9411764705882353,
  "titleNullable": "AI-Based Job Site Matching",
  "subtitleNullable": "Leveraging 400k+ Hours of Resource & Performance Data",
  "descriptionNullable": "_____\n# AI-Based Job Site Matching\n### Leveraging 400k+ Hours of Resource & Performance Data\nBy  [[source]](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE)\n_____\n\n### About this dataset\n> As you savvy job-seekers know, selecting an optimal site for GlideinWMS jobs is no small feat -weighing so many critical variables, and performing the highly sophisticated calculations needed to maximize the gains can be a tall order. Our dataset offers a valuable helping hand: with detailed insight into resource metrics and time-series analysis of over 400K hours of data, this treasure trove of information will hasten your journey towards finding just the right spot for all your job needs. \n> \n> Specifically, our dataset contains three files: dataset_classification.csv, which provides information on critical elements such as disk usage and CPU cache size; dataset_time_series_analysis.csv featuring in-depth takeaways from careful time series analysis; And finally dataset_400k_hour.csv gathering computation results from over 400K hours of testing! With columns such as Failure (indicating whether or not the job failed) TotalCpus (the total number of CPUs used by the job), CpuIsBusy (whether or not the CPU is busy), and SlotType (the type of slot used by the job), it's easier than ever to plot that perfect path to success!\n\n### More Datasets\n> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n\n### Featured Notebooks\n> - \ud83d\udea8 **Your notebook can be here!** \ud83d\udea8! \n\n### How to use the dataset\n> This dataset can be used to help identify the most suitable site for GlideinWMS jobs. It contains resource metrics and time-series analysis, which can provide useful insight into the suitability of each potential site. The dataset consists of three sets: dataset_classification.csv, dataset_time_series_analysis.csv and dataset_400k_hour.csv. \n> \n> The first set provides a high-level view of the critical resource metrics that are essential when matching a job and a site: DiskUsage, TotalCpus, TotalMemory, TotalDisk, CpuCacheSize and TotalVirtualMemoryTotalSlots as well as total slot information all important criteria for any job matching process - including whether or not the CpuIsBusy - along with information about the SlotType for each job at each potential site; additionally there is also data regarding Failure should an issue arise during this process; finally Site is provided so that users can ensure they are matching jobs to sites within their own specific environment if required by policy or business rules.   \n> \n> The second set provides detailed time-series analysis related to these metrics over longer timeframes as well LastUpdate indicating when this analysis was generated (without date), ydate indicating year of last update (without date), mdate indicating month of last update (without date) and hdate indicating hour at which data is refreshed on a regular basis without errors so that up-to-the minute decisions can be made during busy times like peak workloads or reallocations caused by anomalies in usage patterns within existing systems/environments;  \n>  \n> Finally our third set takes things one step further with detailed information related to our 400k+ hours analytical data collection allowing you maximize efficiency while selecting best possible matches across multiple sites/criteria using only one tool (which we have conveniently packaged together in this impressive kaggle datasets :)    \n> \n>  By taking advantage of our AI driven approach you will be able benefit from optimal job selection across many different scenarios such maximum efficiency scenarios with boosts in throughput through realtime scaling along with accountability boost ensuring proper system governance when moving from static systems utilizing static strategies towards ones more reactive working utilization dynamics within new agile deployments increasing stability while lowering maintenance costs over longer run!\n\n### Research Ideas\n> - Use the total CPU, memory and disk usage metrics to identify jobs that need additional resources to complete quickly and suggest alternatives sites with more optimal resource availability \n> - Utilize the time-series analysis using failure rate, last update time series, as well as month/hour/year of last update metrics to create predictive models for job site matching and failure avoidance on future jobs \n> - Identify inefficiencies in scheduling by cross-examining job types (slot type), CPU caching size requirements against historical data to find opportunities for optimization or new approaches to job organization\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n\n### Columns\n\n**File: dataset_classification.csv**\n| Column name            | Description                                                          |\n|:-----------------------|:---------------------------------------------------------------------|\n| **DiskUsage**          | The amount of disk space used by the job. (Numeric)                  |\n| **TotalCpus**          | The total number of CPUs allocated to the job. (Numeric)             |\n| **TotalMemory**        | The total amount of memory allocated to the job. (Numeric)           |\n| **TotalDisk**          | The total amount of disk space allocated to the job. (Numeric)       |\n| **CpuCacheSize**       | The size of the CPU cache allocated to the job. (Numeric)            |\n| **TotalVirtualMemory** | The total amount of virtual memory allocated to the job. (Numeric)   |\n| **TotalSlots**         | The total number of slots allocated to the job. (Numeric)            |\n| **CpuIsBusy**          | A boolean value indicating whether the CPU is busy or not. (Boolean) |\n| **SlotType**           | The type of slot allocated to the job. (String)                      |\n| **Failure**            | A boolean value indicating whether the job failed or not. (Boolean)  |\n| **Site**               | The site where the job is running. (String)                          |\n\n_____\n\n**File: dataset_time_series_analysis.csv**\n| Column name     | Description                                                    |\n|:----------------|:---------------------------------------------------------------|\n| **TotalCpus**   | The total number of CPUs allocated to the job. (Numeric)       |\n| **TotalMemory** | The total amount of memory allocated to the job. (Numeric)     |\n| **TotalDisk**   | The total amount of disk space allocated to the job. (Numeric) |\n| **LastUpdate**  | The date and time of the last update to the job. (DateTime)    |\n| **ydate**       | The year of the job's last update. (Numeric)                   |\n| **mdate**       | The month of the job's last update. (Numeric)                  |\n| **hdate**       | The hour of the job's last update. (Numeric)                   |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE).\n\n",
  "datasetId": 2888258,
  "datasetSlug": "ai-based-job-site-matching",
  "hasDatasetSlug": true,
  "ownerUser": "thedevastator",
  "hasOwnerUser": true,
  "usabilityRating": 0.9411764705882353,
  "hasUsabilityRating": true,
  "totalViews": 1528,
  "totalVotes": 4,
  "totalDownloads": 116,
  "title": "AI-Based Job Site Matching",
  "hasTitle": true,
  "subtitle": "Leveraging 400k+ Hours of Resource & Performance Data",
  "hasSubtitle": true,
  "description": "_____\n# AI-Based Job Site Matching\n### Leveraging 400k+ Hours of Resource & Performance Data\nBy  [[source]](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE)\n_____\n\n### About this dataset\n> As you savvy job-seekers know, selecting an optimal site for GlideinWMS jobs is no small feat -weighing so many critical variables, and performing the highly sophisticated calculations needed to maximize the gains can be a tall order. Our dataset offers a valuable helping hand: with detailed insight into resource metrics and time-series analysis of over 400K hours of data, this treasure trove of information will hasten your journey towards finding just the right spot for all your job needs. \n> \n> Specifically, our dataset contains three files: dataset_classification.csv, which provides information on critical elements such as disk usage and CPU cache size; dataset_time_series_analysis.csv featuring in-depth takeaways from careful time series analysis; And finally dataset_400k_hour.csv gathering computation results from over 400K hours of testing! With columns such as Failure (indicating whether or not the job failed) TotalCpus (the total number of CPUs used by the job), CpuIsBusy (whether or not the CPU is busy), and SlotType (the type of slot used by the job), it's easier than ever to plot that perfect path to success!\n\n### More Datasets\n> For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n\n### Featured Notebooks\n> - \ud83d\udea8 **Your notebook can be here!** \ud83d\udea8! \n\n### How to use the dataset\n> This dataset can be used to help identify the most suitable site for GlideinWMS jobs. It contains resource metrics and time-series analysis, which can provide useful insight into the suitability of each potential site. The dataset consists of three sets: dataset_classification.csv, dataset_time_series_analysis.csv and dataset_400k_hour.csv. \n> \n> The first set provides a high-level view of the critical resource metrics that are essential when matching a job and a site: DiskUsage, TotalCpus, TotalMemory, TotalDisk, CpuCacheSize and TotalVirtualMemoryTotalSlots as well as total slot information all important criteria for any job matching process - including whether or not the CpuIsBusy - along with information about the SlotType for each job at each potential site; additionally there is also data regarding Failure should an issue arise during this process; finally Site is provided so that users can ensure they are matching jobs to sites within their own specific environment if required by policy or business rules.   \n> \n> The second set provides detailed time-series analysis related to these metrics over longer timeframes as well LastUpdate indicating when this analysis was generated (without date), ydate indicating year of last update (without date), mdate indicating month of last update (without date) and hdate indicating hour at which data is refreshed on a regular basis without errors so that up-to-the minute decisions can be made during busy times like peak workloads or reallocations caused by anomalies in usage patterns within existing systems/environments;  \n>  \n> Finally our third set takes things one step further with detailed information related to our 400k+ hours analytical data collection allowing you maximize efficiency while selecting best possible matches across multiple sites/criteria using only one tool (which we have conveniently packaged together in this impressive kaggle datasets :)    \n> \n>  By taking advantage of our AI driven approach you will be able benefit from optimal job selection across many different scenarios such maximum efficiency scenarios with boosts in throughput through realtime scaling along with accountability boost ensuring proper system governance when moving from static systems utilizing static strategies towards ones more reactive working utilization dynamics within new agile deployments increasing stability while lowering maintenance costs over longer run!\n\n### Research Ideas\n> - Use the total CPU, memory and disk usage metrics to identify jobs that need additional resources to complete quickly and suggest alternatives sites with more optimal resource availability \n> - Utilize the time-series analysis using failure rate, last update time series, as well as month/hour/year of last update metrics to create predictive models for job site matching and failure avoidance on future jobs \n> - Identify inefficiencies in scheduling by cross-examining job types (slot type), CPU caching size requirements against historical data to find opportunities for optimization or new approaches to job organization\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> [Data Source](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE)\n> \n>\n\n\n### License\n> \n> \n> **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n> No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n\n### Columns\n\n**File: dataset_classification.csv**\n| Column name            | Description                                                          |\n|:-----------------------|:---------------------------------------------------------------------|\n| **DiskUsage**          | The amount of disk space used by the job. (Numeric)                  |\n| **TotalCpus**          | The total number of CPUs allocated to the job. (Numeric)             |\n| **TotalMemory**        | The total amount of memory allocated to the job. (Numeric)           |\n| **TotalDisk**          | The total amount of disk space allocated to the job. (Numeric)       |\n| **CpuCacheSize**       | The size of the CPU cache allocated to the job. (Numeric)            |\n| **TotalVirtualMemory** | The total amount of virtual memory allocated to the job. (Numeric)   |\n| **TotalSlots**         | The total number of slots allocated to the job. (Numeric)            |\n| **CpuIsBusy**          | A boolean value indicating whether the CPU is busy or not. (Boolean) |\n| **SlotType**           | The type of slot allocated to the job. (String)                      |\n| **Failure**            | A boolean value indicating whether the job failed or not. (Boolean)  |\n| **Site**               | The site where the job is running. (String)                          |\n\n_____\n\n**File: dataset_time_series_analysis.csv**\n| Column name     | Description                                                    |\n|:----------------|:---------------------------------------------------------------|\n| **TotalCpus**   | The total number of CPUs allocated to the job. (Numeric)       |\n| **TotalMemory** | The total amount of memory allocated to the job. (Numeric)     |\n| **TotalDisk**   | The total amount of disk space allocated to the job. (Numeric) |\n| **LastUpdate**  | The date and time of the last update to the job. (DateTime)    |\n| **ydate**       | The year of the job's last update. (Numeric)                   |\n| **mdate**       | The month of the job's last update. (Numeric)                  |\n| **hdate**       | The hour of the job's last update. (Numeric)                   |\n\n### Acknowledgements\n> If you use this dataset in your research, please credit the original authors.\n> If you use this dataset in your research, please credit [](https://zenodo.org/record/7097554#.Y9Y3sNJBwUE).\n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "business",
    "clustering"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}