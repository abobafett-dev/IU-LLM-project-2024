{
  "id": "thedevastator/sonyc-ust-audio-tag-dataset",
  "id_no": 2850930,
  "datasetSlugNullable": "sonyc-ust-audio-tag-dataset",
  "ownerUserNullable": "thedevastator",
  "usabilityRatingNullable": 0.9411764705882353,
  "titleNullable": "SONYC-UST Audio Tag Dataset",
  "subtitleNullable": "Annotated Real-World Urban Sounds for Multi-Label Audio Tag Prediction",
  "descriptionNullable": "_____\n# SONYC-UST Audio Tag Dataset\n### Annotated Real-World Urban Sounds for Multi-Label Audio Tag Prediction\nBy  [[source]](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE)\n_____\n\n### About this dataset\n&gt; This dataset contains real-world recordings of urban sounds from New York City collected by an acoustic sensor network. It can be used to build powerful machine learning models capable of recognizing acoustic environments and understanding the relationships between sound events and their environment across multiple formats. The recordings span a wide range of locations, from quiet city blocks to bustling neighborhoods, giving insight into the full scope of potential surrounding audio events. In addition to just basic audio labels, this dataset includes detailed annotations about temporal and spatial context which makes analyzing the data even more valuable for audio tag prediction research. Furthermore, annotations are provided from both volunteer citizen science as well as verified experts which gives researchers reliable ground-truth datasets to test against. \n&gt; \n&gt; This dataset offers enormous potential for research into building models that can understand complex urban acoustic environments in order to accurately predict sound events and their location in space or time. Using SONYC-UST data provides an opportunity for researchers to develop innovative ways of understanding how identifying sounds contribute information about the surroundings they have been recorded in \u2013 it is an invaluable tool that could lead us towards more comprehensive solutions when it comes to urban acoustics understanding!\n\n### More Datasets\n&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n\n### Featured Notebooks\n&gt; - \ud83d\udea8 **Your notebook can be here!** \ud83d\udea8! \n\n### How to use the dataset\n&gt; \n&gt; In this guide, we will provide an overview of how you can use this dataset to your own research or applications. \n&gt; \n&gt; - Explore: Have a look around the dataset and get acquainted with the audio files and labels associated with them. Gain familiarity with some of the more common tags such as powered saws, machinery-impact, human voices, alert signals etc \n&gt; - Preprocessing: Do any necessary preprocessing such as removing silence at beginnings/ends or reducing volume levels if necessary before attempting any machine learning tasks on the audio files. Some commonly used libraries for performing nonlinear signal processing on digital speech include librosa python library for Python or MATLAB Language for Matlab users . \n&gt; - Machine Learning Model Training: Split your training set into train/validation data in order split ratio you choose (80% train / 20% validation is often good). Then start training your model using one of popular machine learning libraries Sckikit Learn , keras , neon etc using available packages appropriate for classification tasks such as pandas or numpy.. Once trained evaluate performance on validation data set ,if good go ahead and test your model against test set .Also make sure features extracted from each audio file represent meaningful outcomes related to environmental impacts like noise pollution which requires domain knowledge  \n&gt; 4 Data analysis & Results Interpretation : After completing task 3 you should have produced result detailing built model\u2019s accuracy on validation & test sets wrt maeasures like precision recall etc .Also pick top 7\u20138 important features that best helped capture meaning information about environment either proven by other papers or based upon intuition obtained from project setup .Now decide upon whether these quantitative results match upto qualitative inference made during exploration phase above thus helping verify key hypothesis particular problem was trying focus\n\n### Research Ideas\n&gt; - Utilizing the ground truth annotations to develop models for multi-label audio event recognition/classification in an urban environment. \n&gt; - Building models that leverage sound events and their temporal and spatial context for noise pollution forecasting. \n&gt; - Using the sensor network data for creating real-time sound alerts when unusual sounds are detected (e.g alerting citizens of situations such as construction or traffic congestion)\n\n### Acknowledgements\n&gt; If you use this dataset in your research, please credit the original authors.\n&gt; [Data Source](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE)\n&gt; \n&gt;\n\n\n### License\n&gt; \n&gt; \n&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n\n### Columns\n\n**File: annotations.csv**\n| Column name                                     | Description                                                           |\n|:------------------------------------------------|:----------------------------------------------------------------------|\n| **split**                                       | Number indicating relation with train/validation/test sets. (Integer) |\n| **audio_filename**                              | Name and type of file. (String)                                       |\n| **borough**                                     | Specified borough. (String)                                           |\n| **block**                                       | Stated block where located. (String)                                  |\n| **latitude**                                    | Geographic position expressed through coordinates. (Float)            |\n| **longitude**                                   | Geographic position expressed through coordinates. (Float)            |\n| **year**                                        | Year when collected. (Integer)                                        |\n| **week**                                        | Week when collected. (Integer)                                        |\n| **day**                                         | Day when collected. (Integer)                                         |\n| **hour**                                        | Hour when collected. (Integer)                                        |\n| **1-1_small-sounding-engine_presence**          | Presence of small sounding engine. (Boolean)                          |\n| **1-2_medium-sounding-engine_presence**         | Presence of medium sounding engine. (Boolean)                         |\n| **1-3_large-sounding-engine_presence**          | Presence of large sounding engine. (Boolean)                          |\n| **1-X_engine-of-uncertain-size_presence**       | Presence of engine of uncertain size. (Boolean)                       |\n| **2-1_rock-drill_presence**                     | Presence of rock drill. (Boolean)                                     |\n| **2-2_jackhammer_presence**                     | Presence of jackhammer. (Boolean)                                     |\n| **2-3_hoe-ram_presence**                        | Presence of hoe ram. (Boolean)                                        |\n| **2-4_pile-driver_presence**                    | Presence of pile driver. (Boolean)                                    |\n| **2-X_other-unknown-impact-machinery_presence** | Presence of other unknown impact machinery. (Boolean)                 |\n| **3-1_non-machinery-impact_presence**           | Presence of non-machinery impact. (Boolean)                           |\n| **4-1_chainsaw_presence**                       | Presence of chainsaw. (Boolean)                                       |\n| **4-2_small-medium-rotating-saw_presence**      | Presence of small-medium rotating saw. (Boolean)                      |\n| **4-3_large-rotating-saw_presence**             | Presence of large rotating saw. (Boolean)                             |\n\n### Acknowledgements\n&gt; If you use this dataset in your research, please credit the original authors.\n&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE).\n\n",
  "datasetId": 2850930,
  "datasetSlug": "sonyc-ust-audio-tag-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "thedevastator",
  "hasOwnerUser": true,
  "usabilityRating": 0.9411764705882353,
  "hasUsabilityRating": true,
  "totalViews": 765,
  "totalVotes": 5,
  "totalDownloads": 110,
  "title": "SONYC-UST Audio Tag Dataset",
  "hasTitle": true,
  "subtitle": "Annotated Real-World Urban Sounds for Multi-Label Audio Tag Prediction",
  "hasSubtitle": true,
  "description": "_____\n# SONYC-UST Audio Tag Dataset\n### Annotated Real-World Urban Sounds for Multi-Label Audio Tag Prediction\nBy  [[source]](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE)\n_____\n\n### About this dataset\n&gt; This dataset contains real-world recordings of urban sounds from New York City collected by an acoustic sensor network. It can be used to build powerful machine learning models capable of recognizing acoustic environments and understanding the relationships between sound events and their environment across multiple formats. The recordings span a wide range of locations, from quiet city blocks to bustling neighborhoods, giving insight into the full scope of potential surrounding audio events. In addition to just basic audio labels, this dataset includes detailed annotations about temporal and spatial context which makes analyzing the data even more valuable for audio tag prediction research. Furthermore, annotations are provided from both volunteer citizen science as well as verified experts which gives researchers reliable ground-truth datasets to test against. \n&gt; \n&gt; This dataset offers enormous potential for research into building models that can understand complex urban acoustic environments in order to accurately predict sound events and their location in space or time. Using SONYC-UST data provides an opportunity for researchers to develop innovative ways of understanding how identifying sounds contribute information about the surroundings they have been recorded in \u2013 it is an invaluable tool that could lead us towards more comprehensive solutions when it comes to urban acoustics understanding!\n\n### More Datasets\n&gt; For more datasets, click [here](https://www.kaggle.com/thedevastator/datasets).\n\n### Featured Notebooks\n&gt; - \ud83d\udea8 **Your notebook can be here!** \ud83d\udea8! \n\n### How to use the dataset\n&gt; \n&gt; In this guide, we will provide an overview of how you can use this dataset to your own research or applications. \n&gt; \n&gt; - Explore: Have a look around the dataset and get acquainted with the audio files and labels associated with them. Gain familiarity with some of the more common tags such as powered saws, machinery-impact, human voices, alert signals etc \n&gt; - Preprocessing: Do any necessary preprocessing such as removing silence at beginnings/ends or reducing volume levels if necessary before attempting any machine learning tasks on the audio files. Some commonly used libraries for performing nonlinear signal processing on digital speech include librosa python library for Python or MATLAB Language for Matlab users . \n&gt; - Machine Learning Model Training: Split your training set into train/validation data in order split ratio you choose (80% train / 20% validation is often good). Then start training your model using one of popular machine learning libraries Sckikit Learn , keras , neon etc using available packages appropriate for classification tasks such as pandas or numpy.. Once trained evaluate performance on validation data set ,if good go ahead and test your model against test set .Also make sure features extracted from each audio file represent meaningful outcomes related to environmental impacts like noise pollution which requires domain knowledge  \n&gt; 4 Data analysis & Results Interpretation : After completing task 3 you should have produced result detailing built model\u2019s accuracy on validation & test sets wrt maeasures like precision recall etc .Also pick top 7\u20138 important features that best helped capture meaning information about environment either proven by other papers or based upon intuition obtained from project setup .Now decide upon whether these quantitative results match upto qualitative inference made during exploration phase above thus helping verify key hypothesis particular problem was trying focus\n\n### Research Ideas\n&gt; - Utilizing the ground truth annotations to develop models for multi-label audio event recognition/classification in an urban environment. \n&gt; - Building models that leverage sound events and their temporal and spatial context for noise pollution forecasting. \n&gt; - Using the sensor network data for creating real-time sound alerts when unusual sounds are detected (e.g alerting citizens of situations such as construction or traffic congestion)\n\n### Acknowledgements\n&gt; If you use this dataset in your research, please credit the original authors.\n&gt; [Data Source](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE)\n&gt; \n&gt;\n\n\n### License\n&gt; \n&gt; \n&gt; **License: [CC0 1.0 Universal (CC0 1.0) - Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/)**\n&gt; No Copyright - You can copy, modify, distribute and perform the work, even for commercial purposes, all without asking permission. [See Other Information](https://creativecommons.org/publicdomain/zero/1.0/).\n\n### Columns\n\n**File: annotations.csv**\n| Column name                                     | Description                                                           |\n|:------------------------------------------------|:----------------------------------------------------------------------|\n| **split**                                       | Number indicating relation with train/validation/test sets. (Integer) |\n| **audio_filename**                              | Name and type of file. (String)                                       |\n| **borough**                                     | Specified borough. (String)                                           |\n| **block**                                       | Stated block where located. (String)                                  |\n| **latitude**                                    | Geographic position expressed through coordinates. (Float)            |\n| **longitude**                                   | Geographic position expressed through coordinates. (Float)            |\n| **year**                                        | Year when collected. (Integer)                                        |\n| **week**                                        | Week when collected. (Integer)                                        |\n| **day**                                         | Day when collected. (Integer)                                         |\n| **hour**                                        | Hour when collected. (Integer)                                        |\n| **1-1_small-sounding-engine_presence**          | Presence of small sounding engine. (Boolean)                          |\n| **1-2_medium-sounding-engine_presence**         | Presence of medium sounding engine. (Boolean)                         |\n| **1-3_large-sounding-engine_presence**          | Presence of large sounding engine. (Boolean)                          |\n| **1-X_engine-of-uncertain-size_presence**       | Presence of engine of uncertain size. (Boolean)                       |\n| **2-1_rock-drill_presence**                     | Presence of rock drill. (Boolean)                                     |\n| **2-2_jackhammer_presence**                     | Presence of jackhammer. (Boolean)                                     |\n| **2-3_hoe-ram_presence**                        | Presence of hoe ram. (Boolean)                                        |\n| **2-4_pile-driver_presence**                    | Presence of pile driver. (Boolean)                                    |\n| **2-X_other-unknown-impact-machinery_presence** | Presence of other unknown impact machinery. (Boolean)                 |\n| **3-1_non-machinery-impact_presence**           | Presence of non-machinery impact. (Boolean)                           |\n| **4-1_chainsaw_presence**                       | Presence of chainsaw. (Boolean)                                       |\n| **4-2_small-medium-rotating-saw_presence**      | Presence of small-medium rotating saw. (Boolean)                      |\n| **4-3_large-rotating-saw_presence**             | Presence of large rotating saw. (Boolean)                             |\n\n### Acknowledgements\n&gt; If you use this dataset in your research, please credit the original authors.\n&gt; If you use this dataset in your research, please credit [](https://zenodo.org/record/3966543#.Y9Y_OdJBwUE).\n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "music",
    "data cleaning",
    "data visualization"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}