{
  "id": "thefamousrat/synthetic-chess-board-images",
  "id_no": 1931106,
  "datasetSlugNullable": "synthetic-chess-board-images",
  "ownerUserNullable": "thefamousrat",
  "usabilityRatingNullable": 0.8125,
  "titleNullable": "Synthetic Chess Board Images",
  "subtitleNullable": "Photorealistic synthetic images representing pictures of chess game states",
  "descriptionNullable": "### Context\n\nData collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.\n\nWith high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. \n\nAs an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.\n\n### Content\n\nThe dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.\n\nThe data has been generated using the Python scripts and .blend file present in [this repository](https://github.com/TheFamousRat/ChessR). The chess board and pieces models that have been used for those renders are not provided with the code.\n\nData characteristics :\n\n- Images : 1280x1280 JPEG images representing pictures of chess game boards.\n- Annotations : JSON files containing two variables : \n    - \"config\", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.\n    - \"corners\", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.\n- config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : \n    - \"cellsCoordinates\", a dictionary associating a cell name to its coordinates on the board. We have for example {\"A1\" : [0,0], \"A2\" : [1,0], ...}\n    - \"piecesTypes\", a list of strings containing the types of pieces present in the renders.\n\nNo distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.\nA proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.\n\n### Acknowledgements\n\nI would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. \n\n### Inspiration\n\nTwo interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : \n\nErroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt (2021) *Fake It Till You Make It: Face analysis in the wild using synthetic data alone* https://arxiv.org/abs/2109.15102\nSalehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) *PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision* https://arxiv.org/abs/2112.09290",
  "datasetId": 1931106,
  "datasetSlug": "synthetic-chess-board-images",
  "hasDatasetSlug": true,
  "ownerUser": "thefamousrat",
  "hasOwnerUser": true,
  "usabilityRating": 0.8125,
  "hasUsabilityRating": true,
  "totalViews": 2489,
  "totalVotes": 5,
  "totalDownloads": 286,
  "title": "Synthetic Chess Board Images",
  "hasTitle": true,
  "subtitle": "Photorealistic synthetic images representing pictures of chess game states",
  "hasSubtitle": true,
  "description": "### Context\n\nData collection is perhaps the most crucial part of any machine learning model: without it being done properly, not enough information is present for the model to learn from the patterns leading to one output or another. Data collection is however a very complex endeavor, time-consuming due to the volume of data that needs to be acquired and annotated. Annotation is an especially problematic step, due to its difficulty, length, and vulnerability to human error and inaccuracies when annotating complex data.\n\nWith high processing power becoming ever more accessible, synthetic dataset generation is becoming a viable option when looking to generate large volumes of accurately annotated data. With the help of photorealistic renderers, it is for example possible now to generate immense amounts of data, annotated with pixel-perfect precision and whose content is virtually indistinguishable from real-world pictures. \n\nAs an exercise of synthetic dataset generation, the data offered here was generated using the Python API of Blender, with the images rendered through the Cycles raycaster. It represents plausible images representing pictures of chessboard and pieces. The goal is, from those pictures and their annotation, to build a model capable of recognizing the pieces, as well as their positions on the board.\n\n### Content\n\nThe dataset contains a large amount of synthetic, randomly generated images representing pictures of chess images, taken at an angle overlooking the board and its pieces. Each image is associated with a .json file containing its annotations. The naming convention is that each render is associated with a number X, and that the images and annotations associated with that render are respectively named X.jpg and X.json.\n\nThe data has been generated using the Python scripts and .blend file present in [this repository](https://github.com/TheFamousRat/ChessR). The chess board and pieces models that have been used for those renders are not provided with the code.\n\nData characteristics :\n\n- Images : 1280x1280 JPEG images representing pictures of chess game boards.\n- Annotations : JSON files containing two variables : \n    - \"config\", a dictionary associating a cell to the type of piece it contains. If a cell is not presented in the keys, it means that it is empty.\n    - \"corners\", a 4x2 list which contains the coordinates, in the image, of the board corners. Those corners coordinates are normalized to the [0;1] range.\n- config.json : A JSON file generated before rendering, which contains variables relative to the constant properties of the boards in the renders : \n    - \"cellsCoordinates\", a dictionary associating a cell name to its coordinates on the board. We have for example {\"A1\" : [0,0], \"A2\" : [1,0], ...}\n    - \"piecesTypes\", a list of strings containing the types of pieces present in the renders.\n\nNo distinction has been hard-built between training, validation, and testing data, and is left completely up to the users.\nA proposed pipeline for the extraction, recognition, and placement of chess pieces is proposed in a notebook added with this dataset.\n\n### Acknowledgements\n\nI would like to express my gratitude for the efforts of the Blender Foundation and all its participants, for their incredible open-source tool which once again has allowed me to conduct interesting projects with great ease. \n\n### Inspiration\n\nTwo interesting papers on the generation and use of synthetic data, which have inspired me to conduct this project : \n\nErroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt (2021) *Fake It Till You Make It: Face analysis in the wild using synthetic data alone* https://arxiv.org/abs/2109.15102\nSalehe Erfanian Ebadi, You-Cyuan Jhang, Alex Zook (2021) *PeopleSansPeople: A Synthetic Data Generator for Human-Centric Computer Vision* https://arxiv.org/abs/2112.09290",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "board games",
    "computer vision",
    "classification",
    "image"
  ],
  "licenses": [
    {
      "nameNullable": "CC0-1.0",
      "name": "CC0-1.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}