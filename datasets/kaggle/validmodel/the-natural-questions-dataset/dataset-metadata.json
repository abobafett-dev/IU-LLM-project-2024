{
  "id": "validmodel/the-natural-questions-dataset",
  "id_no": 2912461,
  "datasetSlugNullable": "the-natural-questions-dataset",
  "ownerUserNullable": "validmodel",
  "usabilityRatingNullable": 1.0,
  "titleNullable": "Natural Questions Answering ",
  "subtitleNullable": "NLP , Question Answering and open-domain question answering",
  "descriptionNullable": "\nNatural Questions corpus, a question answering dataset. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and a further 7,842 examples 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.\n\n\n\n\nSQuAD 2.0:\n\n\nQuestions generated by hired workers whose task was to write questions about a given article.\nSets of questions are from paragraphs in a wikipedia article.\n442 different articles.\nThere are unanswerable questions (33.4% of the dataset is unanswerable), which forces a model to \u201cknow what it doesn\u2019t know\u201d.\nEnsures there are plausible answers in the paragraph if the question is unanswerable so that model can\u2019t just use superficial clues to determine if an answer is in the paragraph.\nAll questions end with a question mark.\nSince there are series of questions that refer to the same article, some of the questions use pronouns such as \u201cwhere did she grow up?\u201d.\nThere are some ungrammatical sentences such as: \u201cWhy political movement was named for Joseph McCarthy?\u201d.\nThere are occasional misspellings.\nGoogle Natural Questions:\n\nReal, user generated questions by people who are actually seeking information, not people who were hired for the explicit purpose of writing questions.\nRequires a model to read a whole Wikipedia article, not just a paragraph, and has two different tasks: identifying a long answer, which is the paragraph or table that the contains the information to answer the question, and the short answer, which is the exact text that provides the answer to the question.\nNote that since the article is so long BERT models are unequipped to handle them as they have a max sequence length capped at 512.\nAll questions aren\u2019t necessarily \u201cquestions\u201d per se, some of them are phrase searches like \u201cbenefits of colonial life for single celled organisms\u201d.\nQuestions don\u2019t necessarily have a long answer or short answer, so there is some of the unanswerable functionality that is also present in SQuAD 2.0.\nIf a question has a short answer, it definitely has a long answer, but not the other way around.\n\nThere is a \u201cYes or No\u201d answer field which is \u201cYes\u201d if the answer is yes, \u201cNo\u201d if the answer is no, and \u201cNone\u201d if there is not a yes or no answer to the question. If the \u201cYes or No\u201d answer is not \u201cNone\u201d then there is no short answer.\nAll questions don\u2019t end with a question mark.\nSince only one question per article there is more variety of topics of questions.\nThere seem to be less ungrammatical or misspelled sentences.",
  "datasetId": 2912461,
  "datasetSlug": "the-natural-questions-dataset",
  "hasDatasetSlug": true,
  "ownerUser": "validmodel",
  "hasOwnerUser": true,
  "usabilityRating": 1.0,
  "hasUsabilityRating": true,
  "totalViews": 7907,
  "totalVotes": 41,
  "totalDownloads": 484,
  "title": "Natural Questions Answering ",
  "hasTitle": true,
  "subtitle": "NLP , Question Answering and open-domain question answering",
  "hasSubtitle": true,
  "description": "\nNatural Questions corpus, a question answering dataset. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and a further 7,842 examples 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.\n\n\n\n\nSQuAD 2.0:\n\n\nQuestions generated by hired workers whose task was to write questions about a given article.\nSets of questions are from paragraphs in a wikipedia article.\n442 different articles.\nThere are unanswerable questions (33.4% of the dataset is unanswerable), which forces a model to \u201cknow what it doesn\u2019t know\u201d.\nEnsures there are plausible answers in the paragraph if the question is unanswerable so that model can\u2019t just use superficial clues to determine if an answer is in the paragraph.\nAll questions end with a question mark.\nSince there are series of questions that refer to the same article, some of the questions use pronouns such as \u201cwhere did she grow up?\u201d.\nThere are some ungrammatical sentences such as: \u201cWhy political movement was named for Joseph McCarthy?\u201d.\nThere are occasional misspellings.\nGoogle Natural Questions:\n\nReal, user generated questions by people who are actually seeking information, not people who were hired for the explicit purpose of writing questions.\nRequires a model to read a whole Wikipedia article, not just a paragraph, and has two different tasks: identifying a long answer, which is the paragraph or table that the contains the information to answer the question, and the short answer, which is the exact text that provides the answer to the question.\nNote that since the article is so long BERT models are unequipped to handle them as they have a max sequence length capped at 512.\nAll questions aren\u2019t necessarily \u201cquestions\u201d per se, some of them are phrase searches like \u201cbenefits of colonial life for single celled organisms\u201d.\nQuestions don\u2019t necessarily have a long answer or short answer, so there is some of the unanswerable functionality that is also present in SQuAD 2.0.\nIf a question has a short answer, it definitely has a long answer, but not the other way around.\n\nThere is a \u201cYes or No\u201d answer field which is \u201cYes\u201d if the answer is yes, \u201cNo\u201d if the answer is no, and \u201cNone\u201d if there is not a yes or no answer to the question. If the \u201cYes or No\u201d answer is not \u201cNone\u201d then there is no short answer.\nAll questions don\u2019t end with a question mark.\nSince only one question per article there is more variety of topics of questions.\nThere seem to be less ungrammatical or misspelled sentences.",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "computer science",
    "nlp",
    "text generation",
    "question answering",
    "retrieval question answering"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}