{
  "id": "/somaset",
  "id_no": 15886,
  "datasetSlugNullable": "somaset",
  "ownerUserNullable": null,
  "usabilityRatingNullable": 0.875,
  "titleNullable": "SOMASet",
  "subtitleNullable": "Synthetic training data for deep CNNs in re-identification",
  "descriptionNullable": "### Context\n\nSOMASet is an synthetic dataset consisting of 100K images of 50 human prototypes (25 female and 25 male), created by mixing three somatotypical \u201cseeds\u201d:\n\n - ectomorph (long and lean),\n -  mesomorph (athletic, small waist)\n -  endomorph (soft and round body, large frame),\n\nSOMASet is also accounting for different ethnicities.\n Each of these prototypes wears 11 sets of clothes and assumes 250 different poses, over an outdoor background scene, ray-traced for lifelike illumination.\n\n### Content\n\nThe 100.000 Images are organized inside the `somaset` folder in the following form:\n**/somaset/NN/JJ/XXXX.jpg**\n\n -  NN Is a number between `01` and `50` representing an unique human prototype.\n -  JJ is a number between `01` and `08` use to encode a set of garments (clothes).\n - XXXX is a number between `0001` and `0250` encoding different human posing and camera viewpoint.\n\nYou can read more about details about SOMASet in the Paper, and in the Protocol sention below.\n### Acknowledgements\n\n\u26a0 IN THE CASE YOU PUBLISH MATERIAL BASED ON THIS DATASET THEN IN YOUR ACKNOWLEDGEMENTS, PLEASE MENTION THE BASE WORK AS A REFERENCE\n\n@article{barbosa2017looking,\n  title={Looking beyond appearances: Synthetic training data for deep cnns in re-identification},\n  author={Barbosa, Igor Barros and Cristani, Marco and Caputo, Barbara and Rognhaugen, Aleksander and Theoharis, Theoharis},\n  journal={Computer Vision and Image Understanding},\n  year={2017},\n  publisher={Academic Press}\n}\n## Protocol\n\nIn this section we present SOMAset, describe the protocol followed for creating it and discuss the features that make it\nunique compared to other existing re-id collections. The human figure is normally defined as a mixture of three main somatotypes [12]: ectomorph (long and lean), mesomorph (athletic, small waist) and endomorph (soft and round body, large frame). We account for these facets using an open-source program for 3D photo-realistic human design, Makehuman, and a rendering engine, Blender. Starting from a generic 3D human model we created 25 male and 25 female subjects, by manually varying the height, weight and body proportions for each subject so as to represent mixtures of the three aforementioned somatotypes. \n\nIn order to further improve the similarity to real acquisitions, we also slightly varied parameters like symmetry and the size of legs and/or arms, so as to better simulate natural body variations. In almost all previous re-identification scenarios, it is\nassumed that subjects do not change their clothes between camera acquisitions. Re-identification datasets adhere to this\nassumption, associating identity to appearance (a particular apparel represents a single subject). With SOMAset, we relax\nthis constraint, rendering each of the 50 subjects with 8 different sets of clothing: 5 of these were shared across the\nsexes while 3 each were exclusive for males / females (thus in total there are 11 types of outfit). In this way, we stimulate\nthe network to focus on morphological cues, other than mere appearance.\n\nIn more detail, the 3 clothing variations dedicated to females are: T-shirt with shorts; blouse with skirt; sport top with leggings. The 3 male clothing variations are: suit; striped shirt with jeans; shirt with black trousers. The shared clothing category includes the following 5 variations: white t-shirt with jeans; long sleeve shirt with jeans; blue T-shirt with jeans; jacket over shirt with jeans; overalls.\n\nOut of the 50 subjects, 16 received Caucasian skin, 16 have darker skin tones, while the remaining 18 have beige skin tones to\nmodel Asian types. We did not include further variations (e.g. structural) of the faces and we did omit hair styles, to bound the number of possible variations. Notably, adopting more types of garments does not seem to affect the performance drastically, after some preliminary experiments, not reported here for the lack of space.\n\nEach of the 400 subject-clothing combinations assumed 250 different poses. These poses are extracted from professionally-captured human motion recordings, provided by the CMU Graphics Lab Motion Capture Database. We opted for extracting poses from a recording titled \u2019navigate\u2019, where the subject walks forwards, backwards and sideways.\n\nEach of the resulting N = 100K subject-clothing-pose combinations (N = 50 subjects \u00d7 8 clothing sets \u00d7 250 poses) is placed in a realistic scene (see below) and captured by a virtual camera with a randomly chosen viewpoint, following a uniform  distribution. Specifically, we place the subject in a random location over the floor of the scene, and we take 250 different viewpoints uniformly spanning a hemisphere centered 8 meters away from the subject\u2019s initial position. This induces a distance varying from 6 to 10 meters between the camera and the rendered subject-clothing-pose. The different camera viewpoints generate people with diverse image occupancy, different lighting patterns and relative pose w.r.t. the observer. A structured outdoor scene was created for rendering, which covers an area of approximately 900 m2 , where each of the 100K instances was\nlocated. The scene includes trees, buildings, pavement, grass and a vehicle, giving a certain variability as the viewpoint\nchanges. \n\n",
  "datasetId": 15886,
  "datasetSlug": "somaset",
  "hasDatasetSlug": true,
  "ownerUser": "",
  "hasOwnerUser": false,
  "usabilityRating": 0.875,
  "hasUsabilityRating": true,
  "totalViews": 19717,
  "totalVotes": 34,
  "totalDownloads": 1030,
  "title": "SOMASet",
  "hasTitle": true,
  "subtitle": "Synthetic training data for deep CNNs in re-identification",
  "hasSubtitle": true,
  "description": "### Context\n\nSOMASet is an synthetic dataset consisting of 100K images of 50 human prototypes (25 female and 25 male), created by mixing three somatotypical \u201cseeds\u201d:\n\n - ectomorph (long and lean),\n -  mesomorph (athletic, small waist)\n -  endomorph (soft and round body, large frame),\n\nSOMASet is also accounting for different ethnicities.\n Each of these prototypes wears 11 sets of clothes and assumes 250 different poses, over an outdoor background scene, ray-traced for lifelike illumination.\n\n### Content\n\nThe 100.000 Images are organized inside the `somaset` folder in the following form:\n**/somaset/NN/JJ/XXXX.jpg**\n\n -  NN Is a number between `01` and `50` representing an unique human prototype.\n -  JJ is a number between `01` and `08` use to encode a set of garments (clothes).\n - XXXX is a number between `0001` and `0250` encoding different human posing and camera viewpoint.\n\nYou can read more about details about SOMASet in the Paper, and in the Protocol sention below.\n### Acknowledgements\n\n\u26a0 IN THE CASE YOU PUBLISH MATERIAL BASED ON THIS DATASET THEN IN YOUR ACKNOWLEDGEMENTS, PLEASE MENTION THE BASE WORK AS A REFERENCE\n\n@article{barbosa2017looking,\n  title={Looking beyond appearances: Synthetic training data for deep cnns in re-identification},\n  author={Barbosa, Igor Barros and Cristani, Marco and Caputo, Barbara and Rognhaugen, Aleksander and Theoharis, Theoharis},\n  journal={Computer Vision and Image Understanding},\n  year={2017},\n  publisher={Academic Press}\n}\n## Protocol\n\nIn this section we present SOMAset, describe the protocol followed for creating it and discuss the features that make it\nunique compared to other existing re-id collections. The human figure is normally defined as a mixture of three main somatotypes [12]: ectomorph (long and lean), mesomorph (athletic, small waist) and endomorph (soft and round body, large frame). We account for these facets using an open-source program for 3D photo-realistic human design, Makehuman, and a rendering engine, Blender. Starting from a generic 3D human model we created 25 male and 25 female subjects, by manually varying the height, weight and body proportions for each subject so as to represent mixtures of the three aforementioned somatotypes. \n\nIn order to further improve the similarity to real acquisitions, we also slightly varied parameters like symmetry and the size of legs and/or arms, so as to better simulate natural body variations. In almost all previous re-identification scenarios, it is\nassumed that subjects do not change their clothes between camera acquisitions. Re-identification datasets adhere to this\nassumption, associating identity to appearance (a particular apparel represents a single subject). With SOMAset, we relax\nthis constraint, rendering each of the 50 subjects with 8 different sets of clothing: 5 of these were shared across the\nsexes while 3 each were exclusive for males / females (thus in total there are 11 types of outfit). In this way, we stimulate\nthe network to focus on morphological cues, other than mere appearance.\n\nIn more detail, the 3 clothing variations dedicated to females are: T-shirt with shorts; blouse with skirt; sport top with leggings. The 3 male clothing variations are: suit; striped shirt with jeans; shirt with black trousers. The shared clothing category includes the following 5 variations: white t-shirt with jeans; long sleeve shirt with jeans; blue T-shirt with jeans; jacket over shirt with jeans; overalls.\n\nOut of the 50 subjects, 16 received Caucasian skin, 16 have darker skin tones, while the remaining 18 have beige skin tones to\nmodel Asian types. We did not include further variations (e.g. structural) of the faces and we did omit hair styles, to bound the number of possible variations. Notably, adopting more types of garments does not seem to affect the performance drastically, after some preliminary experiments, not reported here for the lack of space.\n\nEach of the 400 subject-clothing combinations assumed 250 different poses. These poses are extracted from professionally-captured human motion recordings, provided by the CMU Graphics Lab Motion Capture Database. We opted for extracting poses from a recording titled \u2019navigate\u2019, where the subject walks forwards, backwards and sideways.\n\nEach of the resulting N = 100K subject-clothing-pose combinations (N = 50 subjects \u00d7 8 clothing sets \u00d7 250 poses) is placed in a realistic scene (see below) and captured by a virtual camera with a randomly chosen viewpoint, following a uniform  distribution. Specifically, we place the subject in a random location over the floor of the scene, and we take 250 different viewpoints uniformly spanning a hemisphere centered 8 meters away from the subject\u2019s initial position. This induces a distance varying from 6 to 10 meters between the camera and the rendered subject-clothing-pose. The different camera viewpoints generate people with diverse image occupancy, different lighting patterns and relative pose w.r.t. the observer. A structured outdoor scene was created for rendering, which covers an area of approximately 900 m2 , where each of the 100K instances was\nlocated. The scene includes trees, buildings, pavement, grass and a vehicle, giving a certain variability as the viewpoint\nchanges. \n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "arts and entertainment",
    "classification",
    "image",
    "multiclass classification"
  ],
  "licenses": [
    {
      "nameNullable": "CC-BY-NC-SA-4.0",
      "name": "CC-BY-NC-SA-4.0",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}