{
  "id": "whenamancodes/the-lambada-dataset-for-word-prediction",
  "id_no": 5290670,
  "datasetSlugNullable": "the-lambada-dataset-for-word-prediction",
  "ownerUserNullable": "whenamancodes",
  "usabilityRatingNullable": 0.9375,
  "titleNullable": "The LAMBADA Dataset for Word Prediction",
  "subtitleNullable": "Evaluating text understanding through word prediction",
  "descriptionNullable": "## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https://aclanthology.org/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development/test/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http://www.aclweb.org/anthology/P16-1144}\n}\n\nFirst released on 2016, September 26.\n",
  "datasetId": 5290670,
  "datasetSlug": "the-lambada-dataset-for-word-prediction",
  "hasDatasetSlug": true,
  "ownerUser": "whenamancodes",
  "hasOwnerUser": true,
  "usabilityRating": 0.9375,
  "hasUsabilityRating": true,
  "totalViews": 11,
  "totalVotes": 1,
  "totalDownloads": 0,
  "title": "The LAMBADA Dataset for Word Prediction",
  "hasTitle": true,
  "subtitle": "Evaluating text understanding through word prediction",
  "hasSubtitle": true,
  "description": "## Context :\nWe introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.\n\nThe LAMBADA paper can be found [here](https://aclanthology.org/P16-1144.pdf).\n\n## Research Ideas\nEvaluating the performance of language models: The LAMBADA dataset can be used to assess the capabilities and limitations of different computational models in understanding and predicting text. By using the dataset, researchers can compare and benchmark their models' word prediction accuracy and contextual understanding.\nDeveloping better natural language processing (NLP) algorithms: The dataset can offer valuable insights for improving NLP algorithms and techniques for tasks such as text comprehension, information extraction, summarization, and question answering. Researchers can analyze patterns within the dataset to identify areas where existing algorithms fall short or need enhancement.\nTraining language generation models: With the LAMBADA dataset, developers can train language generation models (e.g., chatbots or virtual assistants) to provide more accurate and contextually appropriate responses in natural language conversations. By exposing these models to a wide range of text samples from different domains, they can learn to generate coherent and relevant predictions in various conversational contexts\n\n## How to use the dataset\nA Guide to Evaluating Text Understanding and Word Prediction Models\n\nWhat is the LAMBADA dataset? The LAMBADA dataset is designed specifically for assessing contextual understanding of language models through word prediction. It consists of sentences or passages of text with corresponding domains that provide context for the word prediction tasks. The dataset comprises three main files: validation.csv, train.csv, and test.csv.\n\nFamiliarize yourself with the columns: a) 'text' column: This column contains sentences or passages from various domains that are used for word prediction tasks. b) 'domain' column: This categorical column indicates the specific domain or topic associated with each text sample.\n\nUnderstanding file purposes: a) validation.csv: The primary purpose of this file is to evaluate computational models by testing their word prediction abilities on unseen data samples in different domains. b) train.csv: Utilize this file as training data while evaluating computational models' abilities in both text comprehension and accurate word prediction. c) test.csv: This file enables you to assess your model's performance based on its ability to accurately predict words within provided contexts.\n\nEffective utilization tips: a) Preprocessing: Before using any machine learning model on this dataset, it is essential to preprocess the data by removing noise such as punctuation marks and special characters while preserving critical textual information. b) Feature Engineering: Explore additional ways like extracting n-grams or employing advanced embedding techniques (e.g., Word2Vec, BERT) to enhance model performance. c) Model Selection: Experiment with various machine learning algorithms, such as LSTM or Transformer-based models, to identify the best approach for word prediction tasks within text understanding.\n\n## LAMBADA DATASET :\n\nThis archive contains the LAMBADA dataset (Language Modeling Broadened to Account for Discourse Aspects) described in D. Paperno, G. Kruszewski, A. Lazaridou, Q. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda and R.\nFernandez. 2016. The LAMBADA dataset: Word prediction requiring a broad discourse context. Proceedings of ACL 2016 (54th Annual Meeting of the Association for Computational Linguistics), East Stroudsburg PA: ACL, pages\n1525-1534. The source data come from the Book Corpus, made in turn of unpublished novels (see Y. Zhu, R. Kiros, R.f Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. ICCV 2015, pages 19-27).\n\nYou will find 5 files besides this readme in the archive:\n\n1. lambada_development_plain_text.txt\nThe development data include 4,869 test passages (extracted from 1,331 novels, disjoint from the rest).\n\n2. lambada_test_plain_text.txt\nThe test data include 5,153 test passages (extracted from 1,332 novels, disjoint from the rest).\n\n3. lambada_control_test_data_plain_text.txt\nThe control data is a set of sentences randomly sampled from the same novels, and of the same shape and size as the ones used to build the test dataset, but not filtered in any way. This is the set referred to as the \"control\" set in the paper.\n\n**NOTE:** In these 3 files each line corresponds to a passage, including context, target sentence, and target word. For each passage, the word to be guessed is the last one.\n\n4. train-novels.tar\nThe training data include the full text of 2,662 novels (disjoint from those in dev+test), comprising more than 200M words. It consists of text from the same domain as the dev+test passages, but not filtered in any way.\n\n**NOTE:** Development/test/control (1-3) and train (4) sentences have been tokenized in the same way.\n\n5. lambada-vocab-2.txt\nThis is the alphabetically sorted list of words from which the one to be guessed must be picked. It includes 112,745 types.\n\nIf you use the dataset in published work, PLEASE CITE THE LAMBADA PAPER:\n\n@InProceedings{paperno-EtAl:2016:P16-1,\n  author    = {Paperno, Denis  and  Kruszewski, Germ\\'{a}n  and  Lazaridou,\nAngeliki  and  Pham, Ngoc Quan  and  Bernardi, Raffaella  and  Pezzelle,\nSandro  and  Baroni, Marco  and  Boleda, Gemma  and  Fernandez, Raquel},\n  title     = {The {LAMBADA} dataset: Word prediction requiring a broad\ndiscourse context},\n  booktitle = {Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers)},\n  month     = {August},\n  year      = {2016},\n  address   = {Berlin, Germany},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1525--1534},\n  url       = {http://www.aclweb.org/anthology/P16-1144}\n}\n\nFirst released on 2016, September 26.\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "languages",
    "literature",
    "computer science",
    "nlp",
    "text mining",
    "deep learning",
    "neural networks"
  ],
  "licenses": [
    {
      "nameNullable": "Attribution 4.0 International (CC BY 4.0)",
      "name": "Attribution 4.0 International (CC BY 4.0)",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}