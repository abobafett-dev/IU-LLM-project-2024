{
  "id": "wordroid/dataset-reuters-newswire-topics-in-keras",
  "id_no": 1511639,
  "datasetSlugNullable": "dataset-reuters-newswire-topics-in-keras",
  "ownerUserNullable": "wordroid",
  "usabilityRatingNullable": 0.6875,
  "titleNullable": "Dataset Reuters newswire topics in keras",
  "subtitleNullable": "Reuters newswire classification dataset",
  "descriptionNullable": "http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt\n\n\n          Reuters-21578 text categorization test collection\n                           Distribution 1.0\n                          README file (v 1.3)\n                              14 May 2004\n\n                            David D. Lewis\n               David D. Lewis Consulting and Ornarose, Inc. \n                          www.daviddlewis.com\n\n\nI. Introduction\n\n[Note: There's much that could be improved in this document, but given\nthat Reuters-21578 is being superceded by RCV1, I'm not likely to make\nthose improvements myself.  Anyone who would like to create a revised\nversion of this document is invited to contact me.]\n\n   This README describes Distribution 1.0 of the Reuters-21578 text\ncategorization test collection, a resource for research in information\nretrieval, machine learning, and other corpus-based research.\n\n\nII. Copyright & Notification \n\n   The copyright for the text of newswire articles and Reuters\nannotations in the Reuters-21578 collection resides with Reuters Ltd.\nReuters Ltd. and Carnegie Group, Inc. have agreed to allow the free\ndistribution of this data *for research purposes only*.  \n   If you publish results based on this data set, please acknowledge\nits use, refer to the data set by the name \"Reuters-21578,\nDistribution 1.0\", and inform your readers of the current location of\nthe data set (see \"Availability & Questions\").\n\n\nIII. Availability & Questions\n\n   The Reuters-21578, Distribution 1.0 test collection is available\nfrom \n http://www.daviddlewis.com/resources/testcollections/reuters21578\n\nBesides this README file, the collection consists of 22 data files, an\nSGML DTD file describing the data file format, and six files\ndescribing the categories used to index the data.  (See Sections VI\nand VII for more details.)  Some additional files, which are not part\nof the collection but have been contributed by other researchers as\nuseful resources are also included.  All files are available\nuncompressed, and in addition a single gzipped Unix tar archive of the\nentire distribution is available as reuters21578.tar.gz.\n\n   The text categorization mailing list, DDLBETA, is a good place to\nsend questions about this collection and other text categorization\nissues. You may join the list by writing David Lewis at\nddlbeta-request@daviddlewis.com. \n\n\nIV. History & Acknowledgements\n\n   The documents in the Reuters-21578 collection appeared on the\nReuters newswire in 1987.  The documents were assembled and indexed\nwith categories by personnel from Reuters Ltd. (Sam Dobbins, Mike\nTopliss, Steve Weinstein) and Carnegie Group, Inc. (Peggy Andersen,\nMonica Cellio, Phil Hayes, Laura Knecht, Irene Nirenburg) in 1987.  \n\nIn 1990, the documents were made available by Reuters and CGI for\nresearch purposes to the Information Retrieval Laboratory (W.  Bruce\nCroft, Director) of the Computer and Information Science Department at\nthe University of Massachusetts at Amherst.  Formatting of the\ndocuments and production of associated data files was done in 1990 by\nDavid D.  Lewis and Stephen Harding at the Information Retrieval\nLaboratory.\n\nFurther formatting and data file production was done in 1991 and 1992\nby David D. Lewis and Peter Shoemaker at the Center for Information\nand Language Studies, University of Chicago.  This version of the data\nwas made available for anonymous FTP as \"Reuters-22173, Distribution\n1.0\" in January 1993. From 1993 through 1996, Distribution 1.0 was\nhosted at a succession of FTP sites maintained by the Center for\nIntelligent Information Retrieval (W. Bruce Croft, Director) of the\nComputer Science Department at the University of Massachusetts at\nAmherst.\n\nAt the ACM SIGIR '96 conference in August, 1996 a group of text\ncategorization researchers discussed how published results on\nReuters-22173 could be made more comparable across studies.  It was\ndecided that a new version of collection should be produced with less\nambiguous formatting, and including documentation carefully spelling\nout standard methods of using the collection.  The opportunity would\nalso be used to correct a variety of typographical and other errors in\nthe categorization and formatting of the collection.\n\nSteve Finch and David D. Lewis did this cleanup of the collection\nSeptember through November of 1996, relying heavily on Finch's\nSGML-tagged version of the collection from an earlier study.  One\nresult of the re-examination of the collection was the removal of 595\ndocuments which were exact duplicates (based on identity of timestamps\ndown to the second) of other documents in the collection. The new\ncollection therefore has only 21,578 documents, and thus is called the\nReuters-21578 collection.  This README describes version 1.0 of this\nnew collection, which we refer to as \"Reuters-21578, Distribution\n1.0\".\n\nIn preparing the collection and documentation we have benefited from\ndiscussions with Eric Brown, William Cohen, Fred Damerau, Yoram\nSinger, Amit Singhal, and Yiming Yang, among many others.\n\nWe thank all the people and organizations listed above for their\nefforts and support, without which this collection would not exist.\n\nA variety of other changes were also made in going from Reuters-22173\nto Reuters-21578:\n\n   1. Documents were marked up with SGML tags, and a corresponding\nSGML DTD was produced, so that the boundaries of important sections of\ndocuments (e.g. category fields) are unambiguous.\n   2. The set of categories that are legal for each of the five\ncontrolled vocabulary fields was specified. All category names not\nlegal for a field were corrected to a legal category, moved to their\nappropriate field, or removed, as appropriate.\n   3. Documents were given new ID numbers, in chronological order, and\nare collected 1000 to a file in order by ID (and therefore in order\nchronologically). \n\n\nV. What is a Text Categorization Test Collection and Who Cares? \n\n   *Text categorization* is the task of deciding whether a piece of\ntext belongs to any of a set of prespecified categories.  It is a\ngeneric text processing task useful in indexing documents for later\nretrieval, as a stage in natural language processing systems, for\ncontent analysis, and in many other roles [LEWIS94d].\n\n   The use of standard, widely distributed test collections has been a\nconsiderable aid in the development of algorithms for the related task\nof *text retrieval* (finding documents that satisfy a particular\nuser's information need, usually expressed in an textual request).\nText retrieval test collections have allowed the comparison of\nalgorithms developed by a variety of researchers around the world.\n(For more on text retrieval test collections see SPARCKJONES76.)\n\n   Standard test collections have been lacking, however, for text\ncategorization. Few data sets have been used by more than one\nresearcher, making results hard to compare.  The Reuters-22173 test\ncollection has been used in a number of published studies since it was\nmade available, and we believe that the Reuters-21578 collection will\nbe even more valuable.\n\n   The collection may also be of interest to researchers in machine\nlearning, as it provides a classification task with challenging\nproperties. There are multiple categories, the categories are\noverlapping and nonexhaustive, and there are relationships among the\ncategories.  There are interesting possibilities for the use of domain\nknowledge.  There are many possible feature sets that can be extracted\nfrom the text, and most plausible feature/example matrices are large\nand sparse.  There is even some temporal structure to the data\n[LEWIS94b], though problems with the indexing and the uneven\ndistribution of stories within the timespan covered may make this\ncollection a poor one to explore temporal issues.\n\n\nVI. Formatting \n\n     The Reuters-21578 collection is distributed in 22 files. Each of\nthe first 21 files (reut2-000.sgm through reut2-020.sgm) contain 1000\ndocuments, while the last (reut2-021.sgm) contains 578 documents.  \n\n     The files are in SGML format.  Rather than going into the details\nof the SGML language, we describe here in an informal way how the SGML\ntags are used to divide each file, and each document, into sections.\nReaders interested in more detail on SGML are encouraged to pursue\none of the many books and web pages on the subject.\n\n     Each of the 22 files begins with a document type declaration line:\n               \n\nThe DTD file lewis.dtd is included in the distribution.  Following the\ndocument type declaration line are individual Reuters articles marked\nup with SGML tags, as described below.\n\n\n   VI.A. The REUTERS tag:\n\n    Each article starts with an \"open tag\" of the form\n\n    \n\nIn all cases the  tags are the only items\non their line.  \n\n     Each REUTERS tag contains explicit specifications of the values\nof five attributes, TOPICS, LEWISSPLIT, CGISPLIT, OLDID, and NEWID.\nThese attributes are meant to identify documents and groups of \ndocuments, and have the following meanings: \n\n     1. TOPICS : The possible values are YES, NO, and BYPASS:\n        a. YES indicates that *in the original data* there was at\nleast one entry in the TOPICS fields.\n        b. NO indicates that *in the original data* the story had no\nentries in the TOPICS field.\n        c. BYPASS indicates that *in the original data* the story was\nmarked with the string \"bypass\" (or a typographical variant on that\nstring).\n     This poorly-named attribute unfortunately is the subject of much\nconfusion. It is meant to indicate whether or not the document had\nTOPICS categories *in the raw Reuters-22173 dataset*.  The sole use of\nthis attribute is to defining training set splits similar to those\nused in previous research. (See the section on training set splits.)\nThe TOPICS attribute does **NOT** indicate anything about whether or\nnot the Reuters-21578 document has any TOPICS categories.  (Version\n1.0 of this document was errorful on this point.)  That can be\ndetermined by actually looking at the TOPICS field. A story with\nTOPICS=\"YES\" can have no TOPICS categories, and a story with\nTOPICS=\"NO\" can have TOPICS categories.\n     Now, a reasonable (though not certain) assumption is that for all\nTOPICS=\"YES\" stories the indexer at least thought about whether the\nstory belonged to a valid TOPICS category.  Thus, the TOPICS=\"YES\"\nstories with no topics can reasonably be considered negative examples\nfor all 135 valid TOPICS categories.\n     TOPICS=\"NO\" stories are more problematic in their interpretation.\nSome of them presumedly result because the indexer made an explicit\ndecision that they did not belong to any of the 135 valid TOPICS\ncategories.  However, there are many cases where it is clear that a\nstory should belong to one or more TOPICS categories, but for some\nreason the category was not assigned.  There appear to be certain time\nintervals where large numbers of such stories are concentrated,\nsuggesting that some parts of the data set were simply not indexed, or\nnot indexed for some categories or category sets.  Also, in a few\ncases, the indexer clearly meant to assign TOPICS categories, but put\nthem in the wrong field.  These cases have been corrected in the\nReuters-21578 data, yielding stories that have TOPICS categories, but\nwhere TOPICS=\"NO\", because the the category was not assigned in the\nraw version of the data.\n     \"BYPASS\" stories clearly were not indexed, and so are useful only\nfor general distributional information on the language used in the\ndocuments.\n\n     2. LEWISSPLIT : The possible values are TRAINING, TEST, and\nNOT-USED.  TRAINING indicates it was used in the training set in the\nexperiments reported in LEWIS91d (Chapters 9 and 10), LEWIS92b,\nLEWIS92e, and LEWIS94b.  TEST indicates it was used in the test set\nfor those experiments, and NOT-USED means it was not used in those\nexperiments.\n\n     3. CGISPLIT : The possible values are TRAINING-SET and\nPUBLISHED-TESTSET indicating whether the document was in the training\nset or the test set for the experiments reported in HAYES89 and\nHAYES90b.\n\n     4. OLDID : The identification number (ID) the story had in the\nReuters-22173 collection.\n\n     5. NEWID : The identification number (ID) the story has in the\nReuters-21578, Distribution 1.0 collection.  These IDs are assigned to\nthe stories in chronological order.\n\nIn addition, some REUTERS tags have a sixth attribute, CSECS, which\ncan be ignored.  \n\nThe use of these attributes is critical to allowing comparability\nbetween different studies with the collection, and is discussed\nfurther in Section VIII.\n\n\n  VI.B. Document-Internal Tags \n\n     Just as the  tags serve to delimit\ndocuments within a file, other tags are used to delimit elements\nwithin a document.  We discuss these in the order in which they\ntypically appear, though the exact order should not be relied upon in\nprocessing. In some cases, additional tags occur within an element\ndelimited by these top level document-internal tags.  These are\ndiscussed in this section as well.\n\n     We specify below whether each open/close tag pair is used exactly\nonce (ONCE) per a story, or a variable (VARIABLE) number of times\n(possibly zero).  In many cases the start tag of a pair appears only\nat the beginning of a line, with the corresponding end tag always\nappearing at the end of the same line.  When this is the case, we\nindicate it with the notation \"SAMELINE\" below, as an aid to those\nprocessing the files without SGML tools.  \n\n     1.  [ONCE, SAMELINE]: Encloses the date and time\nof the document, possibly followed by some non-date noise material.\n\n     2.  [VARIABLE] : Notes on certain hand\ncorrections that were done to the original Reuters corpus by Steve\nFinch.\n\n     3.  [ONCE, SAMELINE]: Encloses the list of\nTOPICS categories, if any, for the document. If TOPICS categories are\npresent, each will be delimited by the tags .\n     \n     4.  [ONCE, SAMELINE]: Same as ",
  "datasetId": 1511639,
  "datasetSlug": "dataset-reuters-newswire-topics-in-keras",
  "hasDatasetSlug": true,
  "ownerUser": "wordroid",
  "hasOwnerUser": true,
  "usabilityRating": 0.6875,
  "hasUsabilityRating": true,
  "totalViews": 3262,
  "totalVotes": 1,
  "totalDownloads": 194,
  "title": "Dataset Reuters newswire topics in keras",
  "hasTitle": true,
  "subtitle": "Reuters newswire classification dataset",
  "hasSubtitle": true,
  "description": "http://www.daviddlewis.com/resources/testcollections/reuters21578/readme.txt\n\n\n          Reuters-21578 text categorization test collection\n                           Distribution 1.0\n                          README file (v 1.3)\n                              14 May 2004\n\n                            David D. Lewis\n               David D. Lewis Consulting and Ornarose, Inc. \n                          www.daviddlewis.com\n\n\nI. Introduction\n\n[Note: There's much that could be improved in this document, but given\nthat Reuters-21578 is being superceded by RCV1, I'm not likely to make\nthose improvements myself.  Anyone who would like to create a revised\nversion of this document is invited to contact me.]\n\n   This README describes Distribution 1.0 of the Reuters-21578 text\ncategorization test collection, a resource for research in information\nretrieval, machine learning, and other corpus-based research.\n\n\nII. Copyright & Notification \n\n   The copyright for the text of newswire articles and Reuters\nannotations in the Reuters-21578 collection resides with Reuters Ltd.\nReuters Ltd. and Carnegie Group, Inc. have agreed to allow the free\ndistribution of this data *for research purposes only*.  \n   If you publish results based on this data set, please acknowledge\nits use, refer to the data set by the name \"Reuters-21578,\nDistribution 1.0\", and inform your readers of the current location of\nthe data set (see \"Availability & Questions\").\n\n\nIII. Availability & Questions\n\n   The Reuters-21578, Distribution 1.0 test collection is available\nfrom \n http://www.daviddlewis.com/resources/testcollections/reuters21578\n\nBesides this README file, the collection consists of 22 data files, an\nSGML DTD file describing the data file format, and six files\ndescribing the categories used to index the data.  (See Sections VI\nand VII for more details.)  Some additional files, which are not part\nof the collection but have been contributed by other researchers as\nuseful resources are also included.  All files are available\nuncompressed, and in addition a single gzipped Unix tar archive of the\nentire distribution is available as reuters21578.tar.gz.\n\n   The text categorization mailing list, DDLBETA, is a good place to\nsend questions about this collection and other text categorization\nissues. You may join the list by writing David Lewis at\nddlbeta-request@daviddlewis.com. \n\n\nIV. History & Acknowledgements\n\n   The documents in the Reuters-21578 collection appeared on the\nReuters newswire in 1987.  The documents were assembled and indexed\nwith categories by personnel from Reuters Ltd. (Sam Dobbins, Mike\nTopliss, Steve Weinstein) and Carnegie Group, Inc. (Peggy Andersen,\nMonica Cellio, Phil Hayes, Laura Knecht, Irene Nirenburg) in 1987.  \n\nIn 1990, the documents were made available by Reuters and CGI for\nresearch purposes to the Information Retrieval Laboratory (W.  Bruce\nCroft, Director) of the Computer and Information Science Department at\nthe University of Massachusetts at Amherst.  Formatting of the\ndocuments and production of associated data files was done in 1990 by\nDavid D.  Lewis and Stephen Harding at the Information Retrieval\nLaboratory.\n\nFurther formatting and data file production was done in 1991 and 1992\nby David D. Lewis and Peter Shoemaker at the Center for Information\nand Language Studies, University of Chicago.  This version of the data\nwas made available for anonymous FTP as \"Reuters-22173, Distribution\n1.0\" in January 1993. From 1993 through 1996, Distribution 1.0 was\nhosted at a succession of FTP sites maintained by the Center for\nIntelligent Information Retrieval (W. Bruce Croft, Director) of the\nComputer Science Department at the University of Massachusetts at\nAmherst.\n\nAt the ACM SIGIR '96 conference in August, 1996 a group of text\ncategorization researchers discussed how published results on\nReuters-22173 could be made more comparable across studies.  It was\ndecided that a new version of collection should be produced with less\nambiguous formatting, and including documentation carefully spelling\nout standard methods of using the collection.  The opportunity would\nalso be used to correct a variety of typographical and other errors in\nthe categorization and formatting of the collection.\n\nSteve Finch and David D. Lewis did this cleanup of the collection\nSeptember through November of 1996, relying heavily on Finch's\nSGML-tagged version of the collection from an earlier study.  One\nresult of the re-examination of the collection was the removal of 595\ndocuments which were exact duplicates (based on identity of timestamps\ndown to the second) of other documents in the collection. The new\ncollection therefore has only 21,578 documents, and thus is called the\nReuters-21578 collection.  This README describes version 1.0 of this\nnew collection, which we refer to as \"Reuters-21578, Distribution\n1.0\".\n\nIn preparing the collection and documentation we have benefited from\ndiscussions with Eric Brown, William Cohen, Fred Damerau, Yoram\nSinger, Amit Singhal, and Yiming Yang, among many others.\n\nWe thank all the people and organizations listed above for their\nefforts and support, without which this collection would not exist.\n\nA variety of other changes were also made in going from Reuters-22173\nto Reuters-21578:\n\n   1. Documents were marked up with SGML tags, and a corresponding\nSGML DTD was produced, so that the boundaries of important sections of\ndocuments (e.g. category fields) are unambiguous.\n   2. The set of categories that are legal for each of the five\ncontrolled vocabulary fields was specified. All category names not\nlegal for a field were corrected to a legal category, moved to their\nappropriate field, or removed, as appropriate.\n   3. Documents were given new ID numbers, in chronological order, and\nare collected 1000 to a file in order by ID (and therefore in order\nchronologically). \n\n\nV. What is a Text Categorization Test Collection and Who Cares? \n\n   *Text categorization* is the task of deciding whether a piece of\ntext belongs to any of a set of prespecified categories.  It is a\ngeneric text processing task useful in indexing documents for later\nretrieval, as a stage in natural language processing systems, for\ncontent analysis, and in many other roles [LEWIS94d].\n\n   The use of standard, widely distributed test collections has been a\nconsiderable aid in the development of algorithms for the related task\nof *text retrieval* (finding documents that satisfy a particular\nuser's information need, usually expressed in an textual request).\nText retrieval test collections have allowed the comparison of\nalgorithms developed by a variety of researchers around the world.\n(For more on text retrieval test collections see SPARCKJONES76.)\n\n   Standard test collections have been lacking, however, for text\ncategorization. Few data sets have been used by more than one\nresearcher, making results hard to compare.  The Reuters-22173 test\ncollection has been used in a number of published studies since it was\nmade available, and we believe that the Reuters-21578 collection will\nbe even more valuable.\n\n   The collection may also be of interest to researchers in machine\nlearning, as it provides a classification task with challenging\nproperties. There are multiple categories, the categories are\noverlapping and nonexhaustive, and there are relationships among the\ncategories.  There are interesting possibilities for the use of domain\nknowledge.  There are many possible feature sets that can be extracted\nfrom the text, and most plausible feature/example matrices are large\nand sparse.  There is even some temporal structure to the data\n[LEWIS94b], though problems with the indexing and the uneven\ndistribution of stories within the timespan covered may make this\ncollection a poor one to explore temporal issues.\n\n\nVI. Formatting \n\n     The Reuters-21578 collection is distributed in 22 files. Each of\nthe first 21 files (reut2-000.sgm through reut2-020.sgm) contain 1000\ndocuments, while the last (reut2-021.sgm) contains 578 documents.  \n\n     The files are in SGML format.  Rather than going into the details\nof the SGML language, we describe here in an informal way how the SGML\ntags are used to divide each file, and each document, into sections.\nReaders interested in more detail on SGML are encouraged to pursue\none of the many books and web pages on the subject.\n\n     Each of the 22 files begins with a document type declaration line:\n               \n\nThe DTD file lewis.dtd is included in the distribution.  Following the\ndocument type declaration line are individual Reuters articles marked\nup with SGML tags, as described below.\n\n\n   VI.A. The REUTERS tag:\n\n    Each article starts with an \"open tag\" of the form\n\n    \n\nIn all cases the  tags are the only items\non their line.  \n\n     Each REUTERS tag contains explicit specifications of the values\nof five attributes, TOPICS, LEWISSPLIT, CGISPLIT, OLDID, and NEWID.\nThese attributes are meant to identify documents and groups of \ndocuments, and have the following meanings: \n\n     1. TOPICS : The possible values are YES, NO, and BYPASS:\n        a. YES indicates that *in the original data* there was at\nleast one entry in the TOPICS fields.\n        b. NO indicates that *in the original data* the story had no\nentries in the TOPICS field.\n        c. BYPASS indicates that *in the original data* the story was\nmarked with the string \"bypass\" (or a typographical variant on that\nstring).\n     This poorly-named attribute unfortunately is the subject of much\nconfusion. It is meant to indicate whether or not the document had\nTOPICS categories *in the raw Reuters-22173 dataset*.  The sole use of\nthis attribute is to defining training set splits similar to those\nused in previous research. (See the section on training set splits.)\nThe TOPICS attribute does **NOT** indicate anything about whether or\nnot the Reuters-21578 document has any TOPICS categories.  (Version\n1.0 of this document was errorful on this point.)  That can be\ndetermined by actually looking at the TOPICS field. A story with\nTOPICS=\"YES\" can have no TOPICS categories, and a story with\nTOPICS=\"NO\" can have TOPICS categories.\n     Now, a reasonable (though not certain) assumption is that for all\nTOPICS=\"YES\" stories the indexer at least thought about whether the\nstory belonged to a valid TOPICS category.  Thus, the TOPICS=\"YES\"\nstories with no topics can reasonably be considered negative examples\nfor all 135 valid TOPICS categories.\n     TOPICS=\"NO\" stories are more problematic in their interpretation.\nSome of them presumedly result because the indexer made an explicit\ndecision that they did not belong to any of the 135 valid TOPICS\ncategories.  However, there are many cases where it is clear that a\nstory should belong to one or more TOPICS categories, but for some\nreason the category was not assigned.  There appear to be certain time\nintervals where large numbers of such stories are concentrated,\nsuggesting that some parts of the data set were simply not indexed, or\nnot indexed for some categories or category sets.  Also, in a few\ncases, the indexer clearly meant to assign TOPICS categories, but put\nthem in the wrong field.  These cases have been corrected in the\nReuters-21578 data, yielding stories that have TOPICS categories, but\nwhere TOPICS=\"NO\", because the the category was not assigned in the\nraw version of the data.\n     \"BYPASS\" stories clearly were not indexed, and so are useful only\nfor general distributional information on the language used in the\ndocuments.\n\n     2. LEWISSPLIT : The possible values are TRAINING, TEST, and\nNOT-USED.  TRAINING indicates it was used in the training set in the\nexperiments reported in LEWIS91d (Chapters 9 and 10), LEWIS92b,\nLEWIS92e, and LEWIS94b.  TEST indicates it was used in the test set\nfor those experiments, and NOT-USED means it was not used in those\nexperiments.\n\n     3. CGISPLIT : The possible values are TRAINING-SET and\nPUBLISHED-TESTSET indicating whether the document was in the training\nset or the test set for the experiments reported in HAYES89 and\nHAYES90b.\n\n     4. OLDID : The identification number (ID) the story had in the\nReuters-22173 collection.\n\n     5. NEWID : The identification number (ID) the story has in the\nReuters-21578, Distribution 1.0 collection.  These IDs are assigned to\nthe stories in chronological order.\n\nIn addition, some REUTERS tags have a sixth attribute, CSECS, which\ncan be ignored.  \n\nThe use of these attributes is critical to allowing comparability\nbetween different studies with the collection, and is discussed\nfurther in Section VIII.\n\n\n  VI.B. Document-Internal Tags \n\n     Just as the  tags serve to delimit\ndocuments within a file, other tags are used to delimit elements\nwithin a document.  We discuss these in the order in which they\ntypically appear, though the exact order should not be relied upon in\nprocessing. In some cases, additional tags occur within an element\ndelimited by these top level document-internal tags.  These are\ndiscussed in this section as well.\n\n     We specify below whether each open/close tag pair is used exactly\nonce (ONCE) per a story, or a variable (VARIABLE) number of times\n(possibly zero).  In many cases the start tag of a pair appears only\nat the beginning of a line, with the corresponding end tag always\nappearing at the end of the same line.  When this is the case, we\nindicate it with the notation \"SAMELINE\" below, as an aid to those\nprocessing the files without SGML tools.  \n\n     1.  [ONCE, SAMELINE]: Encloses the date and time\nof the document, possibly followed by some non-date noise material.\n\n     2.  [VARIABLE] : Notes on certain hand\ncorrections that were done to the original Reuters corpus by Steve\nFinch.\n\n     3.  [ONCE, SAMELINE]: Encloses the list of\nTOPICS categories, if any, for the document. If TOPICS categories are\npresent, each will be delimited by the tags .\n     \n     4.  [ONCE, SAMELINE]: Same as ",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "computer science"
  ],
  "licenses": [
    {
      "nameNullable": "other",
      "name": "other",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}