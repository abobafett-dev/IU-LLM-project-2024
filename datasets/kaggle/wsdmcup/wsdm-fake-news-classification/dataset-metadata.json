{
  "id": "wsdmcup/wsdm-fake-news-classification",
  "id_no": 154756,
  "datasetSlugNullable": "wsdm-fake-news-classification",
  "ownerUserNullable": "wsdmcup",
  "usabilityRatingNullable": 0.7058823529411765,
  "titleNullable": "WSDM - Fake News Classification",
  "subtitleNullable": "Identify the fake news.",
  "descriptionNullable": "# Background\n\nWSDM (pronounced \"wisdom\") is one of the premier conferences on web-inspired research involving search and data mining. [The 12th ACM International WSDM Conference][1] will take place in Melbourne, Australia during Feb. 11-15, 2019. \n\nThis task is organized by ByteDance, the Platinum Level Sponsor of the conference. ByteDance is a global Internet technology company started from China. Our goal is to build a global content platform that enable people to enjoy various content in various forms. We inform, entertain, and inspire people across language, culture and geography.\n\nOne of the challenges which we are facing is to combat different types of fake news. Fake news here refers to all forms of false, inaccurate or misleading information, which now poses a big threat to human civilization.\n\nAt Bytedance, we have created a large-scale database to store existing fake news articles. Any new article must go through a test on the truthfulness of content before being published. We conduct matching between the new article and the articles in the database. Articles identified as containing fake news will be withdrawn after human verification. The accuracy and efficiency of the process, therefore, becomes crucial for us to make the platform safe, reliable, and healthy.\n\n# About This Dataset\n\nThis dataset is released as the competition dataset of [Task: Fake News Classification][1] with the following task:\n\nGiven the title of a fake news article A and the title of a coming news article B,  participants are asked to classify B into one of the three categories.\n\n- agreed: B talks about the same fake news as A\n- disagreed: B refutes the fake news in A\n- unrelated: B is unrelated to A\n\n## File \n- **train.csv** - training data contains 320,767 news pairs in both Chinese and English. This file provides the only data you can use to finish the task. Using external data is not allowed.\n- **test.csv** - testing data contains 80,126  news pairs in both Chinese and English. The approximately 25% of the testing data is set to be public and is used to calculate your accuracy shown on the leading board. The remaining 75% private data is used to calculate your final result of the competition.\n- **sample_submission.csv** - sample answer to the testing data.\n\n\n## Data fields\n- **id** - the id of each news pair.\n- **tid1** - the id of fake news title 1.\n- **tid2** - the id of news title 2.\n- **title1_zh** - the fake news title 1 in Chinese.\n- **title2_zh** - the news title 2 in Chinese.\n- **title1_en** - the fake news title 1 in English.\n- **title2_en** - the news title 2 in English.\n- **label** - indicates the relation between the news pair: agreed/disagreed/unrelated.\n\nThe English titles are machine translated from the related Chinese titles. This may help participants from all background to get better understanding of the datasets. Participants are highly recommended to use the Chinese version titles to finish the task.  \n\n# Evaluation Metrics\n\nWe use **Weighted Categorization Accuracy** to evaluate your performance. Weighted categorization accuracy can be generally defined as:\n\n$$ WeightedAccuracy(y, \\hat{y}, \\omega) = \\frac{1}{n} \\displaystyle{\\sum_{i=1}^{n}} \\frac{\\omega_i(y_i=\\hat{y}_i)}{\\sum \\omega_i} $$\n\nwhere \\\\(y\\\\) are ground truths, \\\\(\\hat{y}\\\\) are the predicted results, and \\\\(\\omega_i\\\\) is the weight associated with the \\\\(i\\\\)th item in the dataset.\n\nIn our test set, we assign each testing item a weight according to its category. The weights of the three categories, agreed, disagreed and unrelated are \\\\(\\frac{1}{15}\\\\), \\\\(\\frac{1}{5}\\\\), \\\\(\\frac{1}{16}\\\\), respectively. We set the weights in consideration of the imbalance of the data distribution to minimize the bias to your performance caused by the majority class (unrelated pairs accounts for approximately 70% of the dataset). \n\n  [1]: https://www.kaggle.com/c/fake-news-pair-classification-challenge",
  "datasetId": 154756,
  "datasetSlug": "wsdm-fake-news-classification",
  "hasDatasetSlug": true,
  "ownerUser": "wsdmcup",
  "hasOwnerUser": true,
  "usabilityRating": 0.7058823529411765,
  "hasUsabilityRating": true,
  "totalViews": 10877,
  "totalVotes": 22,
  "totalDownloads": 836,
  "title": "WSDM - Fake News Classification",
  "hasTitle": true,
  "subtitle": "Identify the fake news.",
  "hasSubtitle": true,
  "description": "# Background\n\nWSDM (pronounced \"wisdom\") is one of the premier conferences on web-inspired research involving search and data mining. [The 12th ACM International WSDM Conference][1] will take place in Melbourne, Australia during Feb. 11-15, 2019. \n\nThis task is organized by ByteDance, the Platinum Level Sponsor of the conference. ByteDance is a global Internet technology company started from China. Our goal is to build a global content platform that enable people to enjoy various content in various forms. We inform, entertain, and inspire people across language, culture and geography.\n\nOne of the challenges which we are facing is to combat different types of fake news. Fake news here refers to all forms of false, inaccurate or misleading information, which now poses a big threat to human civilization.\n\nAt Bytedance, we have created a large-scale database to store existing fake news articles. Any new article must go through a test on the truthfulness of content before being published. We conduct matching between the new article and the articles in the database. Articles identified as containing fake news will be withdrawn after human verification. The accuracy and efficiency of the process, therefore, becomes crucial for us to make the platform safe, reliable, and healthy.\n\n# About This Dataset\n\nThis dataset is released as the competition dataset of [Task: Fake News Classification][1] with the following task:\n\nGiven the title of a fake news article A and the title of a coming news article B,  participants are asked to classify B into one of the three categories.\n\n- agreed: B talks about the same fake news as A\n- disagreed: B refutes the fake news in A\n- unrelated: B is unrelated to A\n\n## File \n- **train.csv** - training data contains 320,767 news pairs in both Chinese and English. This file provides the only data you can use to finish the task. Using external data is not allowed.\n- **test.csv** - testing data contains 80,126  news pairs in both Chinese and English. The approximately 25% of the testing data is set to be public and is used to calculate your accuracy shown on the leading board. The remaining 75% private data is used to calculate your final result of the competition.\n- **sample_submission.csv** - sample answer to the testing data.\n\n\n## Data fields\n- **id** - the id of each news pair.\n- **tid1** - the id of fake news title 1.\n- **tid2** - the id of news title 2.\n- **title1_zh** - the fake news title 1 in Chinese.\n- **title2_zh** - the news title 2 in Chinese.\n- **title1_en** - the fake news title 1 in English.\n- **title2_en** - the news title 2 in English.\n- **label** - indicates the relation between the news pair: agreed/disagreed/unrelated.\n\nThe English titles are machine translated from the related Chinese titles. This may help participants from all background to get better understanding of the datasets. Participants are highly recommended to use the Chinese version titles to finish the task.  \n\n# Evaluation Metrics\n\nWe use **Weighted Categorization Accuracy** to evaluate your performance. Weighted categorization accuracy can be generally defined as:\n\n$$ WeightedAccuracy(y, \\hat{y}, \\omega) = \\frac{1}{n} \\displaystyle{\\sum_{i=1}^{n}} \\frac{\\omega_i(y_i=\\hat{y}_i)}{\\sum \\omega_i} $$\n\nwhere \\\\(y\\\\) are ground truths, \\\\(\\hat{y}\\\\) are the predicted results, and \\\\(\\omega_i\\\\) is the weight associated with the \\\\(i\\\\)th item in the dataset.\n\nIn our test set, we assign each testing item a weight according to its category. The weights of the three categories, agreed, disagreed and unrelated are \\\\(\\frac{1}{15}\\\\), \\\\(\\frac{1}{5}\\\\), \\\\(\\frac{1}{16}\\\\), respectively. We set the weights in consideration of the imbalance of the data distribution to minimize the bias to your performance caused by the majority class (unrelated pairs accounts for approximately 70% of the dataset). \n\n  [1]: https://www.kaggle.com/c/fake-news-pair-classification-challenge",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "nlp",
    "news"
  ],
  "licenses": [
    {
      "nameNullable": "unknown",
      "name": "unknown",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}