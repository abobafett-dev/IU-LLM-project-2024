{
  "id": "youssef2020/fase-expretion",
  "id_no": 412419,
  "datasetSlugNullable": "fase-expretion",
  "ownerUserNullable": "youssef2020",
  "usabilityRatingNullable": 0.47058823529411764,
  "titleNullable": "fase expretion",
  "subtitleNullable": "",
  "descriptionNullable": "all information in this  the site and Property rights  &gt;&gt;&gt;https://arxiv.org/abs/1811.11283\npdf &gt;&gt;&gt;https://arxiv.org/pdf/1811.11283v2.pdf\ninformation on data  &gt;&gt;&gt;\n**                                                                         Facial expression comparison dataset **\n \n This dataset consists of face image triplets along with annotations that specify which two faces in the triplet form the most similar pair in terms of facial expression. To the best of our knowledge, there is no existing large scale face dataset with such expression comparison annotations. \n \nDataset content: The dataset is released as CSV files. Each line in a CSV file corresponds to one data sample, which consists of three face images and annotations (by multiple human annotators) that indicate which two faces in the triplet form the most similar pair in terms of expression.  \n \nEach face image is specified using an image url and a face bounding box (top-left and bottom-right coordinates). The users need to download the images themselves using the provided urls. The face regions can be cropped from the downloaded images using the provided bounding boxes.  \n \nNote:  Some of the images in this dataset may not be publicly available (if an owner removes their images from public domain) when a user is trying to download them. In such cases, depending on the hosting website and the tool used to download, a dummy image may get downloaded instead of the original image. We recommend users to run a face detection algorithm on the downloaded images and verify that a face is indeed present at the location specified by the provided bounding box. \n \nEach annotation is an integer from the set {1, 2, 3}. A value of 1 means the expressions on second and third faces in the triplet are visually more similar to each other when compared to the expression on the first face. A value of 2 means the expressions on the first and third faces in the triplet are visually more similar to each other when compared to the expression on the second face. A value of 3 means the expressions on the first and second faces in the triplet are visually more similar to each other when compared to the expression on the third face. Each human annotator who participated in the annotation process has a unique id. Most of the samples in this dataset were annotated by six human raters. However, there are few samples which have more than six annotations. \n \nEach triplet in the dataset is also categorized into one of the following three types: one-class triplet, two-class-triplet and three-class triplet. Please see the next section for the definition of these types.\nEach line in the CSV files has the following entries: \n\u25cf URL of image1 (string) \n\u25cf Top-left column of the face bounding box in image1 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image1 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image1 normalized by height (float) \n\u25cf Bottom-right row of the face bounding box in image1 normalized by height (float) \n\u25cf URL of image2 (string)\n \u25cf Top-left column of the face bounding box in image2 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image2 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image2 normalized by height (float) \n\u25cf Bottom-right row of the face bounding box in image2 normalized by height (float) \n\u25cf URL of image3 (string) \n\u25cf Top-left column of the face bounding box in image3 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image3 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image3 normalized by height (float)\n \u25cf Bottom-right row of the face bounding box in image3 normalized by height (float)\n \u25cf Triplet_type (string) - A string indicating the variation of expressions in the triplet. \n\u25cf Annotator1_id (string) - This is just a string of random numbers that can be used to search for all the samples in the dataset annotated by a particular annotator. \n\u25cf Annotation1 (integer) \n\u25cf Annotator2_id (string)\n \u25cf Annotation2 (integer)\n \u25cf \u2026. \n \nHow was this dataset collected? \nThe triplets in this dataset were generated by sampling images from an internal face dataset in which each image has one or more of the following emotion labels:  Amusement, Anger, Awe, Boredom, Concentration, Confusion, Contemplation, Contempt, Contentment, Desire, Disappointment, Disgust, Distress, Doubt, Ecstasy, Elation, Embarrassment, Fear, Interest, Love, Neutral, Pain, Pride, Realization, Relief, Sadness, Shame, Surprise, Sympathy, Triumph . To reduce the effect of category-bias in the triplet dataset, we sampled the images such that all the above categories are well represented. \n \nEach triplet was shown to human annotators and each annotator was asked to pick two images from the triplet which are visually the most similar. Annotators were instructed to focus only on the expressions and ignore other factors such as identity, gender, ethnicity, pose, age, etc.  \n \nThis dataset specifically focuses on three types of triplets: \n\u25cf One-class triplet: All the three images in the triplet share a common label. \n\u25cf Two-class triplet: Only two images in the triplet share a label and the third one does not share any label with the other two. \n\u25cf Three-class triplet: None of the images in the triplet share a label. \n\nNote:  The images in the face dataset (from which we sampled the triplets) were not labeled exhaustively, i.e., an image does not have all the labels that are applicable to it. The current categorization into one-class, two-class and three-class triplets is done based on the existing labels. Hence, a triplet currently classified as a two/three-class triplet may not actually be a two/three-class triplet if the images had been labeled exhaustively. \n \nDataset statistics: \n    This dataset is divided into two partitions:\n \u25cf TRAIN:  faceexp-comparison-data-train-public.csv  \n              \u25cb For training expression models \n \n\u25cf TEST:  faceexp-comparison-data-test-public.csv\n                 \u25cb For evaluation of the trained models \n \n \nThe number of triplets and the number of faces in each partition are shown in the table below: \n \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2703737%2F06482beb56941c2a8bd1315755692002%2Fyouss.PNG?generation=1573500453805941&alt=media)\n\n \n \nCitation: \nPlease cite the below paper if you use this dataset in your work. R Vemulapalli, A Agarwala, \u201c A Compact Embedding for Facial Expression Similarity \u201d, CoRR, abs/1811.11283, 2018. \n \n \nContact: Please email Raviteja Vemulapalli ( ravitejavemu@google.com ) or Aseem Agarwala ( aseemaa@google.com ) for any questions regarding the dataset. \n \n \nAcknowledgements: We thank Gautam Prasad, Ting Liu, Brendan Jou, Alan Cowen,  Florian Schroff and Hartwig Adam  from Google for their support and suggestions during the data collection process. \n\n",
  "datasetId": 412419,
  "datasetSlug": "fase-expretion",
  "hasDatasetSlug": true,
  "ownerUser": "youssef2020",
  "hasOwnerUser": true,
  "usabilityRating": 0.47058823529411764,
  "hasUsabilityRating": true,
  "totalViews": 5155,
  "totalVotes": 11,
  "totalDownloads": 246,
  "title": "fase expretion",
  "hasTitle": true,
  "subtitle": "",
  "hasSubtitle": true,
  "description": "all information in this  the site and Property rights  &gt;&gt;&gt;https://arxiv.org/abs/1811.11283\npdf &gt;&gt;&gt;https://arxiv.org/pdf/1811.11283v2.pdf\ninformation on data  &gt;&gt;&gt;\n**                                                                         Facial expression comparison dataset **\n \n This dataset consists of face image triplets along with annotations that specify which two faces in the triplet form the most similar pair in terms of facial expression. To the best of our knowledge, there is no existing large scale face dataset with such expression comparison annotations. \n \nDataset content: The dataset is released as CSV files. Each line in a CSV file corresponds to one data sample, which consists of three face images and annotations (by multiple human annotators) that indicate which two faces in the triplet form the most similar pair in terms of expression.  \n \nEach face image is specified using an image url and a face bounding box (top-left and bottom-right coordinates). The users need to download the images themselves using the provided urls. The face regions can be cropped from the downloaded images using the provided bounding boxes.  \n \nNote:  Some of the images in this dataset may not be publicly available (if an owner removes their images from public domain) when a user is trying to download them. In such cases, depending on the hosting website and the tool used to download, a dummy image may get downloaded instead of the original image. We recommend users to run a face detection algorithm on the downloaded images and verify that a face is indeed present at the location specified by the provided bounding box. \n \nEach annotation is an integer from the set {1, 2, 3}. A value of 1 means the expressions on second and third faces in the triplet are visually more similar to each other when compared to the expression on the first face. A value of 2 means the expressions on the first and third faces in the triplet are visually more similar to each other when compared to the expression on the second face. A value of 3 means the expressions on the first and second faces in the triplet are visually more similar to each other when compared to the expression on the third face. Each human annotator who participated in the annotation process has a unique id. Most of the samples in this dataset were annotated by six human raters. However, there are few samples which have more than six annotations. \n \nEach triplet in the dataset is also categorized into one of the following three types: one-class triplet, two-class-triplet and three-class triplet. Please see the next section for the definition of these types.\nEach line in the CSV files has the following entries: \n\u25cf URL of image1 (string) \n\u25cf Top-left column of the face bounding box in image1 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image1 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image1 normalized by height (float) \n\u25cf Bottom-right row of the face bounding box in image1 normalized by height (float) \n\u25cf URL of image2 (string)\n \u25cf Top-left column of the face bounding box in image2 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image2 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image2 normalized by height (float) \n\u25cf Bottom-right row of the face bounding box in image2 normalized by height (float) \n\u25cf URL of image3 (string) \n\u25cf Top-left column of the face bounding box in image3 normalized by width (float) \n\u25cf Bottom-right column of the face bounding box in image3 normalized by width (float) \n\u25cf Top-left row of the face bounding box in image3 normalized by height (float)\n \u25cf Bottom-right row of the face bounding box in image3 normalized by height (float)\n \u25cf Triplet_type (string) - A string indicating the variation of expressions in the triplet. \n\u25cf Annotator1_id (string) - This is just a string of random numbers that can be used to search for all the samples in the dataset annotated by a particular annotator. \n\u25cf Annotation1 (integer) \n\u25cf Annotator2_id (string)\n \u25cf Annotation2 (integer)\n \u25cf \u2026. \n \nHow was this dataset collected? \nThe triplets in this dataset were generated by sampling images from an internal face dataset in which each image has one or more of the following emotion labels:  Amusement, Anger, Awe, Boredom, Concentration, Confusion, Contemplation, Contempt, Contentment, Desire, Disappointment, Disgust, Distress, Doubt, Ecstasy, Elation, Embarrassment, Fear, Interest, Love, Neutral, Pain, Pride, Realization, Relief, Sadness, Shame, Surprise, Sympathy, Triumph . To reduce the effect of category-bias in the triplet dataset, we sampled the images such that all the above categories are well represented. \n \nEach triplet was shown to human annotators and each annotator was asked to pick two images from the triplet which are visually the most similar. Annotators were instructed to focus only on the expressions and ignore other factors such as identity, gender, ethnicity, pose, age, etc.  \n \nThis dataset specifically focuses on three types of triplets: \n\u25cf One-class triplet: All the three images in the triplet share a common label. \n\u25cf Two-class triplet: Only two images in the triplet share a label and the third one does not share any label with the other two. \n\u25cf Three-class triplet: None of the images in the triplet share a label. \n\nNote:  The images in the face dataset (from which we sampled the triplets) were not labeled exhaustively, i.e., an image does not have all the labels that are applicable to it. The current categorization into one-class, two-class and three-class triplets is done based on the existing labels. Hence, a triplet currently classified as a two/three-class triplet may not actually be a two/three-class triplet if the images had been labeled exhaustively. \n \nDataset statistics: \n    This dataset is divided into two partitions:\n \u25cf TRAIN:  faceexp-comparison-data-train-public.csv  \n              \u25cb For training expression models \n \n\u25cf TEST:  faceexp-comparison-data-test-public.csv\n                 \u25cb For evaluation of the trained models \n \n \nThe number of triplets and the number of faces in each partition are shown in the table below: \n \n![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2703737%2F06482beb56941c2a8bd1315755692002%2Fyouss.PNG?generation=1573500453805941&alt=media)\n\n \n \nCitation: \nPlease cite the below paper if you use this dataset in your work. R Vemulapalli, A Agarwala, \u201c A Compact Embedding for Facial Expression Similarity \u201d, CoRR, abs/1811.11283, 2018. \n \n \nContact: Please email Raviteja Vemulapalli ( ravitejavemu@google.com ) or Aseem Agarwala ( aseemaa@google.com ) for any questions regarding the dataset. \n \n \nAcknowledgements: We thank Gautam Prasad, Ting Liu, Brendan Jou, Alan Cowen,  Florian Schroff and Hartwig Adam  from Google for their support and suggestions during the data collection process. \n\n",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "earth and nature",
    "image"
  ],
  "licenses": [
    {
      "nameNullable": "unknown",
      "name": "unknown",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}