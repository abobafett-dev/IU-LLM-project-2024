{
  "id": "zandysupotco/sd-vae-ft-ema-f8-256-faces6-enc",
  "id_no": 5121985,
  "datasetSlugNullable": "sd-vae-ft-ema-f8-256-faces6-enc",
  "ownerUserNullable": "zandysupotco",
  "usabilityRatingNullable": 0.625,
  "titleNullable": "VAE-Encoded-Faces-HQ",
  "subtitleNullable": "250k 256*256 pix high resolution human and anime faces encoded by SD-VAE-f8",
  "descriptionNullable": "This dataset contains 250k 256\\*256 pix high resolution human, anime and animal faces encoded by \"sd-vae-ema-f8\" from huggingface / diffusers (https://github.com/huggingface/diffusers/) and saved as \".pt\" files (e.g. the \"afhq_x.pt\" contains a torch.Tensor shaped [15.8k, 32, 32, 4] with dtype float32; the \"afhq_cls.pt\" is the dataset label, a LongTensor shaped [15.8k,]). Each original image is 256\\*256\\*3, and is encoded to 32\\*32\\*4. The original images are from 6 different datasets on Kaggle.  Motivated by personal interests, I created this dataset for class-conditional image generation with Latent Diffusion Models (LDMs).\n# Contents\nI chose the following HQ datasets on Kaggle based on personal appetite.\n|Datset Label| Dataset Name |  Dataset Size |Description| URL | \n| --- | --- | \n|0|AFHQ |15.8k| Cat, dog and wild animal faces| https://www.kaggle.com/datasets/dimensi0n/afhq-512/data\n|1|FFHQ |70.0k|Human faces|https://www.kaggle.com/datasets/xhlulu/flickrfaceshq-dataset-nvidia-resized-256px\n|2|CelebA-HQ |30.0k|Celebrity faces| https://www.kaggle.com/datasets/denislukovnikov/celebahq256-images-only\n|3|FaceAttributes |24.0k |Human faces| https://www.kaggle.com/datasets/mantasu/face-attributes-grouped\n|4|AnimeGAN |25.7k|Anime faces generated by styleGAN-2| https://www.kaggle.com/datasets/prasoonkottarathil/gananime-lite\n|5|AnimeFaces |92.2k| Anime faces|https://www.kaggle.com/datasets/scribbless/another-anime-face-dataset\n\nI find my LDM hard to learn the samples in AFHQ and FaceAttributes, but behaves reasonably well on the other datasets. \n\n# Encoding and Decoding Pipeline\nThe image is first downsampled to 256 pix (the above datasets provide original images of either 256 pix or 512 pix). They're normalized (img = img / 127.5 - 1) before encoded by the sd-vae-ema-f8 encoder. The output latent code is shaped as [batch_size, 32, 32, 4]. The std of the latent code is ~4.5 and the mean is &lt;0.5. \n```\nimport torch\nfrom diffusers.models import AutoencoderKL\n\n\ndef encode(normalized_images: torch.Tensor, mode=True):\n    dist = vae_model.encode(normalized_images).latent_dist\n    if mode:\n        return dist.mode()\n    else:\n        return dist.sample()\n\ndef decode(latent_code: torch.Tensor):\n    return vae_model.decode(latent_code).sample\n\n# this is a model with 34M encoder params and 49M decoder params\nmodel_name = \"stabilityai/sd-vae-ft-ema\"\nvae_model = AutoencoderKL.from_pretrained(model_name)\nvae_model.eval().requires_grad_(False)\n```\nIt took about 45 min on a P100 GPU on Kaggle to encode these 250k images (with a batch size of 32, which didn't fully take advantage of the GPU's 16GB VRAM!).",
  "datasetId": 5121985,
  "datasetSlug": "sd-vae-ft-ema-f8-256-faces6-enc",
  "hasDatasetSlug": true,
  "ownerUser": "zandysupotco",
  "hasOwnerUser": true,
  "usabilityRating": 0.625,
  "hasUsabilityRating": true,
  "totalViews": 26,
  "totalVotes": 0,
  "totalDownloads": 2,
  "title": "VAE-Encoded-Faces-HQ",
  "hasTitle": true,
  "subtitle": "250k 256*256 pix high resolution human and anime faces encoded by SD-VAE-f8",
  "hasSubtitle": true,
  "description": "This dataset contains 250k 256\\*256 pix high resolution human, anime and animal faces encoded by \"sd-vae-ema-f8\" from huggingface / diffusers (https://github.com/huggingface/diffusers/) and saved as \".pt\" files (e.g. the \"afhq_x.pt\" contains a torch.Tensor shaped [15.8k, 32, 32, 4] with dtype float32; the \"afhq_cls.pt\" is the dataset label, a LongTensor shaped [15.8k,]). Each original image is 256\\*256\\*3, and is encoded to 32\\*32\\*4. The original images are from 6 different datasets on Kaggle.  Motivated by personal interests, I created this dataset for class-conditional image generation with Latent Diffusion Models (LDMs).\n# Contents\nI chose the following HQ datasets on Kaggle based on personal appetite.\n|Datset Label| Dataset Name |  Dataset Size |Description| URL | \n| --- | --- | \n|0|AFHQ |15.8k| Cat, dog and wild animal faces| https://www.kaggle.com/datasets/dimensi0n/afhq-512/data\n|1|FFHQ |70.0k|Human faces|https://www.kaggle.com/datasets/xhlulu/flickrfaceshq-dataset-nvidia-resized-256px\n|2|CelebA-HQ |30.0k|Celebrity faces| https://www.kaggle.com/datasets/denislukovnikov/celebahq256-images-only\n|3|FaceAttributes |24.0k |Human faces| https://www.kaggle.com/datasets/mantasu/face-attributes-grouped\n|4|AnimeGAN |25.7k|Anime faces generated by styleGAN-2| https://www.kaggle.com/datasets/prasoonkottarathil/gananime-lite\n|5|AnimeFaces |92.2k| Anime faces|https://www.kaggle.com/datasets/scribbless/another-anime-face-dataset\n\nI find my LDM hard to learn the samples in AFHQ and FaceAttributes, but behaves reasonably well on the other datasets. \n\n# Encoding and Decoding Pipeline\nThe image is first downsampled to 256 pix (the above datasets provide original images of either 256 pix or 512 pix). They're normalized (img = img / 127.5 - 1) before encoded by the sd-vae-ema-f8 encoder. The output latent code is shaped as [batch_size, 32, 32, 4]. The std of the latent code is ~4.5 and the mean is &lt;0.5. \n```\nimport torch\nfrom diffusers.models import AutoencoderKL\n\n\ndef encode(normalized_images: torch.Tensor, mode=True):\n    dist = vae_model.encode(normalized_images).latent_dist\n    if mode:\n        return dist.mode()\n    else:\n        return dist.sample()\n\ndef decode(latent_code: torch.Tensor):\n    return vae_model.decode(latent_code).sample\n\n# this is a model with 34M encoder params and 49M decoder params\nmodel_name = \"stabilityai/sd-vae-ft-ema\"\nvae_model = AutoencoderKL.from_pretrained(model_name)\nvae_model.eval().requires_grad_(False)\n```\nIt took about 45 min on a P100 GPU on Kaggle to encode these 250k images (with a batch size of 32, which didn't fully take advantage of the GPU's 16GB VRAM!).",
  "hasDescription": true,
  "isPrivate": false,
  "keywords": [
    "animals",
    "image",
    "anime and manga",
    "image generator"
  ],
  "licenses": [
    {
      "nameNullable": "unknown",
      "name": "unknown",
      "hasName": true
    }
  ],
  "collaborators": [],
  "data": []
}